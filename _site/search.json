[
  {
    "objectID": "In-Class_Ex/In-Class_Ex_5/MC1.html",
    "href": "In-Class_Ex/In-Class_Ex_5/MC1.html",
    "title": "In-Class Exercise 05",
    "section": "",
    "text": "MC 01\npacman::p_load(tidyverse, jsonlite, SmartEDA, tidygraph, ggraph)\nkg &lt;- fromJSON(\"data/MC1_graph.json\")"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex_5/MC1.html#initial-eda",
    "href": "In-Class_Ex/In-Class_Ex_5/MC1.html#initial-eda",
    "title": "In-Class Exercise 05",
    "section": "Initial EDA",
    "text": "Initial EDA\n\nggplot(data = edges_tbl, aes(y = `Edge Type`)) + \n  geom_bar()"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex_5/MC1.html#creating-knowledge-graph",
    "href": "In-Class_Ex/In-Class_Ex_5/MC1.html#creating-knowledge-graph",
    "title": "In-Class Exercise 05",
    "section": "Creating knowledge graph",
    "text": "Creating knowledge graph\n\nStep 1: Mapping from node id to row index\n\nid_map &lt;- tibble(id = nodes_tbl$id, index = seq_len(nrow(nodes_tbl)))\n\n\n\nStep 2: Map source and target IDs to row indices\n\nedges_tbl &lt;- edges_tbl %&gt;%\n  left_join(id_map, by = c(\"source\" = \"id\"), suffix = c(\"\", \"_source\")) %&gt;%\n  rename(from = index) %&gt;%\n  left_join(id_map, by = c(\"target\" = \"id\"), suffix = c(\"\", \"_target\")) %&gt;%\n  rename(to = index)\n\n\n\nStep 3\n\nedges_tbl &lt;- edges_tbl %&gt;%\n  filter(!is.na(from), !is.na(to))\n\n\n\nStep 4: Creating the graph\n\ngraph &lt;- tbl_graph(nodes = nodes_tbl, edges = edges_tbl, \n                   directed = kg$directed)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex_5/MC1.html#visualizing-the-knowledge-graph",
    "href": "In-Class_Ex/In-Class_Ex_5/MC1.html#visualizing-the-knowledge-graph",
    "title": "In-Class Exercise 05",
    "section": "Visualizing the knowledge graph",
    "text": "Visualizing the knowledge graph\n\nset.seed(1234)\n\n\nVisualizing the whole Graph\n\nggraph(graph, layout = \"fr\") + \n  geom_edge_link(alpha = 0.3, colour = \"gray\") +\n  geom_node_point(aes(color = `Node Type`), size = 4) +\n  geom_node_text(aes(label = name), repel = TRUE, size = 2.5) +\n  theme_void()\n\n\nStep 1: Filter edges to only “MemberOf”\n\ngraph_memberof &lt;- graph %&gt;%\n  activate(edges) %&gt;%\n  filter(`Edge Type` == \"MemberOf\")\n\n\n\nStep 2: Extract only connected nodes (ie. used in these edges)\n\nused_node_indices &lt;- graph_memberof %&gt;%\n  activate(edges) %&gt;%\n  as_tibble() %&gt;%\n  select(from, to) %&gt;%\n  unlist() %&gt;%\n  unique()\n\n\n\nStep 3: Keep only those nodes\n\ngraph_memberof &lt;- graph_memberof %&gt;%\n  activate(nodes) %&gt;%\n  mutate(row_id = row_number()) %&gt;%\n  filter(row_id %in% used_node_indices) %&gt;%\n  select(-row_id)  #optional cleanup\n\n\n\nPlot the sub-graph\n\nggraph(graph_memberof, layout = \"fr\") + \n  geom_edge_link(alpha = 0.5, colour = \"gray\") +\n  geom_node_point(aes(color = `Node Type`), size = 1) +\n  geom_node_text(aes(label = name), repel = TRUE, size = 2.5) +\n  theme_void()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608-VAA",
    "section": "",
    "text": "Welcome to ISSS608 Visual Analytics and Applications.\nIn this website, you will find my coursework prepared for this course.\nThe Reference materials used can be found in Prof Kam Tin Seong’s course Webpage: https://isss608-ay2024-25apr.netlify.app/"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_1B/Take-home_Ex01B.html",
    "href": "Take-home_Ex/Take-home_Ex_1B/Take-home_Ex01B.html",
    "title": "Take-home Exercise 01B",
    "section": "",
    "text": "Phase 2: to select one submission provided by a classmate, critic three good design principles and three areas for further improvement. With reference to the comment, prepare the makeover version of the data visualisation. I am selecting this submission from other classmate, as shown here.\n\n\nThe data should be processed by using appropriate tidyverse family of packages and the data visualisation must be prepared using ggplot2 and its extensions.\n\npacman::p_load(ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               tidyverse, ggplot2) \n\n\n\n\nTo accomplish the task, Singapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2024 dataset shared by Department of Statistics, Singapore (DOS) will be used and we wil load it as follows:\n\npop_data &lt;- read_csv(\"data/respopagesex2024.csv\", col_names = TRUE)\n\n\n\n\n\n\nOriginal Data Visualization:\n\npyramid_data &lt;- pop_data %&gt;%\n  group_by(Age, Sex) %&gt;%\n  summarise(Pop = sum(Pop), .groups = \"drop\") %&gt;%\n  mutate(Pop = ifelse(Sex == \"Males\", -Pop, Pop))\n\nggplot(pyramid_data, aes(x = Age, y = Pop, fill = Sex)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  scale_y_continuous(labels = abs) +\n  scale_fill_manual(values = c(\"Males\" = \"#102E50\", \"Females\" = \"#F7CFD8\")) + \n  scale_x_discrete(breaks = seq(0, 100, by = 10)) +\n  labs(title = \"Singapore Age Pyramid (2024)\",\n       x = \"Age\", y = \"Population\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nComments:\nThree good design principles:\n\nDifferent contrast colours are used to differentiate between Males and Females Resident count\nAppropriate use of chart (pyramid chart) instead of points to represent discrete values\nClear labeling for chart title, legend, x axis title and y axis title\n\nThree areas for further improvement:\n\nAxis Labeling and Scale Consistency\nConfusing x axis (starts from 0) and y axis (0 starts from the center) labeling. And, the distance between 0 to 10 to 20 on the y-axis is not consistent which signalling there is an error in the data preparation. The y-axis values ranges from 0 mark to 80 mark, whereas the dataset for Age ranges from 0 to 90+. There should be a 90 mark in the y-axis to represent the clearer and more accurate representation of the chart.\nGraphical Integrity: The top chart shows 2 wide bars after tapering off of the top pyramid which is not a true representation of the dataset. There should be a data cleaning performed before the visualization to change the data type for Age column from strings to integer and there is 1 value in the Age column: 90_and_Over that needs to be recoded to a numeric number. The lack of data preparation has led to the wrong representation of the data in this chart.\nGroup Ages into 5-Year Bins: The current age pyramid displays age in single-year intervals, resulting in a visually dense and harder-to-read chart. Binning the ages into 5-year groups (e.g., 0–4, 5–9, …, 85–89, 90+) would simplify the structure and highlight broader population trends more effectively. Additionally, including a vertical line to indicate the median age would provide a valuable reference point, making it easier to interpret the overall age distribution and identify demographic imbalance\n\nMakeover version of the Chart\nSome data preparation is needed: - to make the age group of interval 5 years from 0 to 90+ - to change the data type of Age from character to numeric\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\n\n# Data preparation\npyramid_data &lt;- pop_data %&gt;%\n  mutate(Age = ifelse(Age == \"90_and_Over\", \"91\", Age),\n         Age = as.numeric(Age)) %&gt;%\n  filter(!is.na(Age)) %&gt;%\n  mutate(AgeGroup = cut(Age,\n                        breaks = c(0, 5, 10, 15, 20, 25, 30, 35, 40, 45,\n                                   50, 55, 60, 65, 70, 75, 80, 85, 90, Inf),\n                        right = TRUE,\n                        include.lowest = TRUE,\n                        labels = c(\"0–5\", \"6–10\", \"11–15\", \"16–20\", \"21–25\",\n                                   \"26–30\", \"31–35\", \"36–40\", \"41–45\", \"46–50\",\n                                   \"51–55\", \"56–60\", \"61–65\", \"66–70\", \"71–75\",\n                                   \"76–80\", \"81–85\", \"86–90\", \"90+\"))) %&gt;%\n  group_by(AgeGroup, Sex) %&gt;%\n  summarise(Pop = sum(Pop, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  mutate(Pop = ifelse(Sex == \"Males\", -Pop, Pop),\n         Label = comma(abs(Pop), accuracy = 1),\n         Rank = dense_rank(desc(AgeGroup)))\n\nmedian_group &lt;- pyramid_data %&gt;%\n  group_by(AgeGroup) %&gt;%\n  summarise(TotalPop = sum(abs(Pop))) %&gt;%\n  mutate(CumSum = cumsum(TotalPop),\n         MedianFlag = CumSum &gt;= sum(TotalPop) / 2) %&gt;%\n  filter(MedianFlag) %&gt;%\n  slice(1) %&gt;%\n  pull(AgeGroup)\n\n# Plot\nggplot(pyramid_data, aes(x = AgeGroup, y = Pop, fill = Sex)) +\n  geom_bar(stat = \"identity\", width = 0.9) +\n  geom_text(aes(label = Label,\n              hjust = case_when(\n                AgeGroup %in% c(\"90+\", \"86–90\") & Sex == \"Males\" ~ 1.1,\n                AgeGroup %in% c(\"90+\", \"86–90\") & Sex == \"Females\" ~ -0.1,\n                Sex == \"Males\" ~ 0,  # centered inside left bar\n                Sex == \"Females\" ~ 1.5  # centered inside right bar\n              )),\n          size = 3, color = \"black\") +  # &lt;- Added the missing plus sign here\n  geom_vline(xintercept = 0, color = \"black\") +\n  coord_flip() +\n  scale_y_continuous(labels = NULL, breaks = NULL) +  # Remove x-axis tick values\n  scale_fill_manual(values = c(\"Males\" = \"#7EC8E3\", \"Females\" = \"#F7CFD8\")) +\n  labs(title = \"Singapore Population Age Pyramid (2024)\",\n       x = \"Age Group (Years)\", y = NULL, fill = \"Sex\") +\n  theme_minimal(base_size = 11) +\n  theme(plot.title = element_text(size = 14, face = \"bold\"),\n        axis.text.y = element_text(size = 9),\n        panel.grid.major.y = element_blank(),\n        legend.position = \"right\") +\n  annotate(\"text\", x = median_group, y = 0,\n           label = paste(\"Median:\", median_group),\n           vjust = -0.8, fontface = \"italic\", color = \"gray40\", size = 4)\n\n\n\n\n\n\n\nOriginal Visualization:\n\nlibrary(tidyverse)\n\n# Summarise total and elderly population by PA\ntop_areas &lt;- pop_data %&gt;%\n  group_by(PA) %&gt;%\n  summarise(\n    Total = sum(Pop),\n    Elderly = sum(Pop[Age &gt;= 65]),\n    .groups = \"drop\"\n  ) %&gt;%\n  \n  top_n(15, Total)  # or 20 if you prefer\n\n# Reshape into long format for grouped bars\nplot_data &lt;- top_areas %&gt;%\n  pivot_longer(cols = c(Total, Elderly),\n               names_to = \"Type\",\n               values_to = \"Population\") %&gt;%\n  mutate(\n    Type = recode(Type,\n                  \"Total\" = \"Total Population\",\n                  \"Elderly\" = \"Elderly (65+)\")\n  )\n# Reorder PA by Total Population (not elderly)\nplot_data &lt;- plot_data %&gt;%\n  left_join(top_areas %&gt;% select(PA, Total), by = \"PA\") %&gt;%\n  mutate(PA = fct_reorder(PA, Total, .desc = FALSE))  # use forcats::fct_reorder\n\nggplot(plot_data, aes(x = PA, y = Population, fill = Type)) +\n  geom_col(position = \"dodge\") +\n  geom_text(aes(label = scales::comma(Population)),\n            position = position_dodge(width = 0.9), hjust = -0.1, size = 3) +\n  scale_y_continuous(labels = scales::comma,\n                     expand = expansion(mult = c(0, 0.15))) +\n  scale_fill_manual(values = c(\"Total Population\" = \"#8E7DBE\", \"Elderly (65+)\" = \"#EFC000\")) +\n  coord_flip() +\n  labs(\n    title = \"Total vs Elderly Population in Top Planning Areas (2024)\",\n    x = \"Planning Area\",\n    y = \"Population\",\n    fill = \"\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\n\n\nComments:\nThree good design principles:\n\nEffective use of contrast colours are used to differentiate between the total population and elderly population count by planning areas\nClear notation of the data values inside the chart\nClear labeling for legend, x axis and y axis title and mark.\n\nThree areas for further improvement\n\nThe chart currently includes both detailed bar annotations and x-axis tick marks, which introduces visual redundancy. When values are already clearly displayed inside or beside the bars, retaining dense tick marks on the x-axis adds clutter without enhancing interpretability. It is advisable to choose either to annotate values inside the chart or to rely on well-spaced x-axis tick marks—not both. Removing one will streamline the visual presentation and improve focus on the data.\nMisleading data due to insufficient data cleaning and preparation. The chart reflects inaccurate figures—e.g., Bedok’s elderly population is overstated as 70,130 instead of the correct 62,990—due to lack of data preparation. Proper data cleaning, including converting the Age column to numeric and recoding “90_and_Over” as 90, is essential to ensure data integrity and reliable insights.\nSorting and chart type The chart should be sorted by elderly population, not total population, to match its stated purpose. Using a stacked bar chart (rather than side-by-side horizontal barbars) would better show the elderly count in relation to the total population within each planning area.\n\nMakeover version of the Chart\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Data Preparation\npop_data &lt;- pop_data %&gt;%\n  mutate(\n    Age = str_trim(Age),\n    Age = case_when(\n      Age == \"90_and_Over\" ~ \"90\",\n      str_detect(Age, \"^[0-9]+$\") ~ Age,\n      TRUE ~ NA_character_\n    ),\n    Age = as.numeric(Age)\n  )\n\n\ntop_areas &lt;- pop_data %&gt;%\n  group_by(PA) %&gt;%\n  summarise(\n    Total = sum(Pop, na.rm = TRUE),\n    Elderly = sum(Pop[Age &gt;= 65], na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  slice_max(Elderly, n = 20)\n\n# Prepare data\nplot_data &lt;- top_areas %&gt;%\n  mutate(NonElderly = Total - Elderly) %&gt;%\n  select(PA, Elderly, NonElderly) %&gt;%\n  pivot_longer(cols = c(Elderly, NonElderly),\n               names_to = \"Group\",\n               values_to = \"Population\") %&gt;%\n  mutate(Group = recode(Group,\n                        \"Elderly\" = \"Elderly (65+)\",\n                        \"NonElderly\" = \"Non-Elderly\"),\n         Group = factor(Group, levels = c(\"Non-Elderly\", \"Elderly (65+)\"))) %&gt;%\n  left_join(top_areas %&gt;% select(PA, Elderly), by = \"PA\") %&gt;%\n  mutate(PA = fct_reorder(PA, Elderly, .desc = FALSE))  \n\n# Plot\nggplot(plot_data, aes(x = PA, y = Population, fill = Group)) +\n  geom_col(width = 0.9) +\n  scale_y_continuous(\n    breaks = seq(25000, 300000, by = 25000),\n    labels = function(x) x / 1000,  # Show in '000\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  scale_fill_manual(values = c(\n    \"Non-Elderly\" = \"#A6D1E6\",\n    \"Elderly (65+)\" = \"#FFB347\"\n  )) +\n  coord_flip() +\n  labs(\n    title = \"Top 20 PA by Elderly Population Count (Age 65+)\",\n    x = \"Planning Area\",\n    y = \"Population (in '000)\",\n    fill = \"\"\n  ) +\n  theme_classic(base_size = 12) +\n  theme(legend.position = \"right\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_1B/Take-home_Ex01B.html#the-designing-tool",
    "href": "Take-home_Ex/Take-home_Ex_1B/Take-home_Ex01B.html#the-designing-tool",
    "title": "Take-home Exercise 01B",
    "section": "",
    "text": "The data should be processed by using appropriate tidyverse family of packages and the data visualisation must be prepared using ggplot2 and its extensions.\n\npacman::p_load(ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               tidyverse, ggplot2)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_1B/Take-home_Ex01B.html#import-data",
    "href": "Take-home_Ex/Take-home_Ex_1B/Take-home_Ex01B.html#import-data",
    "title": "Take-home Exercise 01B",
    "section": "",
    "text": "To accomplish the task, Singapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2024 dataset shared by Department of Statistics, Singapore (DOS) will be used and we wil load it as follows:\n\npop_data &lt;- read_csv(\"data/respopagesex2024.csv\", col_names = TRUE)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_1B/Take-home_Ex01B.html#data-visualization",
    "href": "Take-home_Ex/Take-home_Ex_1B/Take-home_Ex01B.html#data-visualization",
    "title": "Take-home Exercise 01B",
    "section": "",
    "text": "Original Data Visualization:\n\npyramid_data &lt;- pop_data %&gt;%\n  group_by(Age, Sex) %&gt;%\n  summarise(Pop = sum(Pop), .groups = \"drop\") %&gt;%\n  mutate(Pop = ifelse(Sex == \"Males\", -Pop, Pop))\n\nggplot(pyramid_data, aes(x = Age, y = Pop, fill = Sex)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  scale_y_continuous(labels = abs) +\n  scale_fill_manual(values = c(\"Males\" = \"#102E50\", \"Females\" = \"#F7CFD8\")) + \n  scale_x_discrete(breaks = seq(0, 100, by = 10)) +\n  labs(title = \"Singapore Age Pyramid (2024)\",\n       x = \"Age\", y = \"Population\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nComments:\nThree good design principles:\n\nDifferent contrast colours are used to differentiate between Males and Females Resident count\nAppropriate use of chart (pyramid chart) instead of points to represent discrete values\nClear labeling for chart title, legend, x axis title and y axis title\n\nThree areas for further improvement:\n\nAxis Labeling and Scale Consistency\nConfusing x axis (starts from 0) and y axis (0 starts from the center) labeling. And, the distance between 0 to 10 to 20 on the y-axis is not consistent which signalling there is an error in the data preparation. The y-axis values ranges from 0 mark to 80 mark, whereas the dataset for Age ranges from 0 to 90+. There should be a 90 mark in the y-axis to represent the clearer and more accurate representation of the chart.\nGraphical Integrity: The top chart shows 2 wide bars after tapering off of the top pyramid which is not a true representation of the dataset. There should be a data cleaning performed before the visualization to change the data type for Age column from strings to integer and there is 1 value in the Age column: 90_and_Over that needs to be recoded to a numeric number. The lack of data preparation has led to the wrong representation of the data in this chart.\nGroup Ages into 5-Year Bins: The current age pyramid displays age in single-year intervals, resulting in a visually dense and harder-to-read chart. Binning the ages into 5-year groups (e.g., 0–4, 5–9, …, 85–89, 90+) would simplify the structure and highlight broader population trends more effectively. Additionally, including a vertical line to indicate the median age would provide a valuable reference point, making it easier to interpret the overall age distribution and identify demographic imbalance\n\nMakeover version of the Chart\nSome data preparation is needed: - to make the age group of interval 5 years from 0 to 90+ - to change the data type of Age from character to numeric\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\n\n# Data preparation\npyramid_data &lt;- pop_data %&gt;%\n  mutate(Age = ifelse(Age == \"90_and_Over\", \"91\", Age),\n         Age = as.numeric(Age)) %&gt;%\n  filter(!is.na(Age)) %&gt;%\n  mutate(AgeGroup = cut(Age,\n                        breaks = c(0, 5, 10, 15, 20, 25, 30, 35, 40, 45,\n                                   50, 55, 60, 65, 70, 75, 80, 85, 90, Inf),\n                        right = TRUE,\n                        include.lowest = TRUE,\n                        labels = c(\"0–5\", \"6–10\", \"11–15\", \"16–20\", \"21–25\",\n                                   \"26–30\", \"31–35\", \"36–40\", \"41–45\", \"46–50\",\n                                   \"51–55\", \"56–60\", \"61–65\", \"66–70\", \"71–75\",\n                                   \"76–80\", \"81–85\", \"86–90\", \"90+\"))) %&gt;%\n  group_by(AgeGroup, Sex) %&gt;%\n  summarise(Pop = sum(Pop, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  mutate(Pop = ifelse(Sex == \"Males\", -Pop, Pop),\n         Label = comma(abs(Pop), accuracy = 1),\n         Rank = dense_rank(desc(AgeGroup)))\n\nmedian_group &lt;- pyramid_data %&gt;%\n  group_by(AgeGroup) %&gt;%\n  summarise(TotalPop = sum(abs(Pop))) %&gt;%\n  mutate(CumSum = cumsum(TotalPop),\n         MedianFlag = CumSum &gt;= sum(TotalPop) / 2) %&gt;%\n  filter(MedianFlag) %&gt;%\n  slice(1) %&gt;%\n  pull(AgeGroup)\n\n# Plot\nggplot(pyramid_data, aes(x = AgeGroup, y = Pop, fill = Sex)) +\n  geom_bar(stat = \"identity\", width = 0.9) +\n  geom_text(aes(label = Label,\n              hjust = case_when(\n                AgeGroup %in% c(\"90+\", \"86–90\") & Sex == \"Males\" ~ 1.1,\n                AgeGroup %in% c(\"90+\", \"86–90\") & Sex == \"Females\" ~ -0.1,\n                Sex == \"Males\" ~ 0,  # centered inside left bar\n                Sex == \"Females\" ~ 1.5  # centered inside right bar\n              )),\n          size = 3, color = \"black\") +  # &lt;- Added the missing plus sign here\n  geom_vline(xintercept = 0, color = \"black\") +\n  coord_flip() +\n  scale_y_continuous(labels = NULL, breaks = NULL) +  # Remove x-axis tick values\n  scale_fill_manual(values = c(\"Males\" = \"#7EC8E3\", \"Females\" = \"#F7CFD8\")) +\n  labs(title = \"Singapore Population Age Pyramid (2024)\",\n       x = \"Age Group (Years)\", y = NULL, fill = \"Sex\") +\n  theme_minimal(base_size = 11) +\n  theme(plot.title = element_text(size = 14, face = \"bold\"),\n        axis.text.y = element_text(size = 9),\n        panel.grid.major.y = element_blank(),\n        legend.position = \"right\") +\n  annotate(\"text\", x = median_group, y = 0,\n           label = paste(\"Median:\", median_group),\n           vjust = -0.8, fontface = \"italic\", color = \"gray40\", size = 4)\n\n\n\n\n\n\n\nOriginal Visualization:\n\nlibrary(tidyverse)\n\n# Summarise total and elderly population by PA\ntop_areas &lt;- pop_data %&gt;%\n  group_by(PA) %&gt;%\n  summarise(\n    Total = sum(Pop),\n    Elderly = sum(Pop[Age &gt;= 65]),\n    .groups = \"drop\"\n  ) %&gt;%\n  \n  top_n(15, Total)  # or 20 if you prefer\n\n# Reshape into long format for grouped bars\nplot_data &lt;- top_areas %&gt;%\n  pivot_longer(cols = c(Total, Elderly),\n               names_to = \"Type\",\n               values_to = \"Population\") %&gt;%\n  mutate(\n    Type = recode(Type,\n                  \"Total\" = \"Total Population\",\n                  \"Elderly\" = \"Elderly (65+)\")\n  )\n# Reorder PA by Total Population (not elderly)\nplot_data &lt;- plot_data %&gt;%\n  left_join(top_areas %&gt;% select(PA, Total), by = \"PA\") %&gt;%\n  mutate(PA = fct_reorder(PA, Total, .desc = FALSE))  # use forcats::fct_reorder\n\nggplot(plot_data, aes(x = PA, y = Population, fill = Type)) +\n  geom_col(position = \"dodge\") +\n  geom_text(aes(label = scales::comma(Population)),\n            position = position_dodge(width = 0.9), hjust = -0.1, size = 3) +\n  scale_y_continuous(labels = scales::comma,\n                     expand = expansion(mult = c(0, 0.15))) +\n  scale_fill_manual(values = c(\"Total Population\" = \"#8E7DBE\", \"Elderly (65+)\" = \"#EFC000\")) +\n  coord_flip() +\n  labs(\n    title = \"Total vs Elderly Population in Top Planning Areas (2024)\",\n    x = \"Planning Area\",\n    y = \"Population\",\n    fill = \"\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\n\n\nComments:\nThree good design principles:\n\nEffective use of contrast colours are used to differentiate between the total population and elderly population count by planning areas\nClear notation of the data values inside the chart\nClear labeling for legend, x axis and y axis title and mark.\n\nThree areas for further improvement\n\nThe chart currently includes both detailed bar annotations and x-axis tick marks, which introduces visual redundancy. When values are already clearly displayed inside or beside the bars, retaining dense tick marks on the x-axis adds clutter without enhancing interpretability. It is advisable to choose either to annotate values inside the chart or to rely on well-spaced x-axis tick marks—not both. Removing one will streamline the visual presentation and improve focus on the data.\nMisleading data due to insufficient data cleaning and preparation. The chart reflects inaccurate figures—e.g., Bedok’s elderly population is overstated as 70,130 instead of the correct 62,990—due to lack of data preparation. Proper data cleaning, including converting the Age column to numeric and recoding “90_and_Over” as 90, is essential to ensure data integrity and reliable insights.\nSorting and chart type The chart should be sorted by elderly population, not total population, to match its stated purpose. Using a stacked bar chart (rather than side-by-side horizontal barbars) would better show the elderly count in relation to the total population within each planning area.\n\nMakeover version of the Chart\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Data Preparation\npop_data &lt;- pop_data %&gt;%\n  mutate(\n    Age = str_trim(Age),\n    Age = case_when(\n      Age == \"90_and_Over\" ~ \"90\",\n      str_detect(Age, \"^[0-9]+$\") ~ Age,\n      TRUE ~ NA_character_\n    ),\n    Age = as.numeric(Age)\n  )\n\n\ntop_areas &lt;- pop_data %&gt;%\n  group_by(PA) %&gt;%\n  summarise(\n    Total = sum(Pop, na.rm = TRUE),\n    Elderly = sum(Pop[Age &gt;= 65], na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  slice_max(Elderly, n = 20)\n\n# Prepare data\nplot_data &lt;- top_areas %&gt;%\n  mutate(NonElderly = Total - Elderly) %&gt;%\n  select(PA, Elderly, NonElderly) %&gt;%\n  pivot_longer(cols = c(Elderly, NonElderly),\n               names_to = \"Group\",\n               values_to = \"Population\") %&gt;%\n  mutate(Group = recode(Group,\n                        \"Elderly\" = \"Elderly (65+)\",\n                        \"NonElderly\" = \"Non-Elderly\"),\n         Group = factor(Group, levels = c(\"Non-Elderly\", \"Elderly (65+)\"))) %&gt;%\n  left_join(top_areas %&gt;% select(PA, Elderly), by = \"PA\") %&gt;%\n  mutate(PA = fct_reorder(PA, Elderly, .desc = FALSE))  \n\n# Plot\nggplot(plot_data, aes(x = PA, y = Population, fill = Group)) +\n  geom_col(width = 0.9) +\n  scale_y_continuous(\n    breaks = seq(25000, 300000, by = 25000),\n    labels = function(x) x / 1000,  # Show in '000\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  scale_fill_manual(values = c(\n    \"Non-Elderly\" = \"#A6D1E6\",\n    \"Elderly (65+)\" = \"#FFB347\"\n  )) +\n  coord_flip() +\n  labs(\n    title = \"Top 20 PA by Elderly Population Count (Age 65+)\",\n    x = \"Planning Area\",\n    y = \"Population (in '000)\",\n    fill = \"\"\n  ) +\n  theme_classic(base_size = 12) +\n  theme(legend.position = \"right\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_1/Take-home_Ex01.html",
    "href": "Take-home_Ex/Take-home_Ex_1/Take-home_Ex01.html",
    "title": "Take-Home Exercise 1",
    "section": "",
    "text": "Creating enlightening and truthful data visualizations involves focusing on accuracy, transparency, and the ability to effectively communicate insights. It’s about presenting data in a way that is both informative and aesthetically pleasing, ensuring the audience can grasp the information quickly and accurately.\n\n\n\nA local online media company that publishes daily content on digital platforms is planning to release an article on demographic structures and distribution of Singapore in 2024.\n\n\n\nAssuming the role of the graphical editor of the media company, we are tasked to prepare at most three data visualization for the article.\n\n\n\n\n\nWe load the following R packages using the pacman::p_load() function:\n\ntidyverse: R packages designed for data science\nggrepel: to provides geoms for ggplot2 to repel overlapping text labels\nggthemes: to use additional themes for ggplot2\npatchwork: to prepare composite figure created using ggplot2\nscales: to provide the internal scaling infrastructure used by ggplot2\nggpubr to create publication ready ggplot2 plots.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed in the computer.\n\npacman::p_load(tidyverse, ggrepel, patchwork, ggthemes, scales,\n               ggpubr) \n\n\n\n\nTo accomplish the task, Singapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2024 dataset shared by Department of Statistics, Singapore (DOS) will be used and we wil load it as follows:\n\ndata &lt;- read_csv(\"data/respopagesex2024.csv\", col_names = TRUE)\n\n\n\n\n\n\nWe first take a look at the data. Using the code below, we can get the details of the dataset which contains 60,424 rows and 6 columns.\n\nglimpse(data)\n\nRows: 60,424\nColumns: 6\n$ PA   &lt;chr&gt; \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo K…\n$ SZ   &lt;chr&gt; \"Ang Mo Kio Town Centre\", \"Ang Mo Kio Town Centre\", \"Ang Mo Kio T…\n$ Age  &lt;chr&gt; \"0\", \"0\", \"1\", \"1\", \"2\", \"2\", \"3\", \"3\", \"4\", \"4\", \"5\", \"5\", \"6\", …\n$ Sex  &lt;chr&gt; \"Males\", \"Females\", \"Males\", \"Females\", \"Males\", \"Females\", \"Male…\n$ Pop  &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 30, 10, 20, 10, 20, 30, 30, 10, 3…\n$ Time &lt;dbl&gt; 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024,…\n\n\n\n\n\n\nWe notice that there is only one value in Time column (2024) which will not be used for further analysis, we will delete this column as per the code chunk below:\n\n\ndata &lt;- data %&gt;% select(-Time)\n\n\nWe will rename the column names in the dataset for clarity, as detailed provided by the Department of Statistics (DOS), as follows:\n\nPA → Planning Area\nSZ → Subzone\nPop → Resident Count\n\n\n\n\n\n\nSide Note:\n\n\n\nPlease note: according to the DOS accompanying documentation of this dataset, the population figures in the csv file have been rounded to the nearest 10, and as such, total counts may not sum exactly due to rounding adjustments.\n\n\n\ncolnames(data) &lt;- c(\"PlanningArea\", \"SubZone\", \"Age\", \"Sex\", \"ResidentCount\")\n\n\nNext, we observe that for the Age column, there is a value of : “90_and_over”. We will replace this value with “90” and change the data type from string/character to numeric and then create a new column to classify the age according to the age bracket in the interval of 5 years as per the standard age group published in DOS, with the following code:\n\n\ndata &lt;- data %&gt;%\n  mutate(\n    Age = str_to_lower(Age),\n    Age = ifelse(Age == \"90_and_over\", \"90\", Age),\n    Age = as.numeric(Age),\n    AgeGroup = cut(Age,\n                   breaks = c(0,4,9,14,19,24,29,34,39,44,49,54,59,64,69,74,79,84,89, Inf),\n                   labels = c(\"0–4\", \"5–9\", \"10–14\", \"15–19\", \"20–24\", \"25–29\",\n                              \"30–34\", \"35–39\", \"40–44\", \"45–49\", \"50–54\", \n                              \"55–59\", \"60–64\", \"65–69\", \"70–74\", \"75–79\", \n                              \"80–84\", \"85–89\", \"90+\"),\n                   right = TRUE, include.lowest = TRUE)\n  )\n\n\nFurther observation of the dataset, we discover there are multiple rows with “0” values in the “Pop”/“ResidentCount” column. We will remove these rows as per the code chunk below, and calculate the number of rows and total population before and after the deletion to ensure completeness with the following code:\n\n\ntotal_population_before &lt;- sum(data$ResidentCount, na.rm = TRUE)\ntotal_rows_before &lt;- nrow(data)\n\nzero_count &lt;- data %&gt;%\n  filter(ResidentCount == 0) %&gt;%\n  nrow()\n\ncat(\"Total population before cleaning:\", format(total_population_before, big.mark = \",\"), \"\\n\")\n\nTotal population before cleaning: 4,193,530 \n\ncat(\"Total rows before cleaning:\", total_rows_before, \"\\n\")\n\nTotal rows before cleaning: 60424 \n\ncat(\"Rows with 0 ResidentCount removed:\", zero_count, \"\\n\")\n\nRows with 0 ResidentCount removed: 23181 \n\ndata &lt;- data %&gt;%\n  filter(ResidentCount &gt; 0)\n\ntotal_population_after &lt;- sum(data$ResidentCount, na.rm = TRUE)\ntotal_rows_after &lt;- nrow(data)\n\ncat(\"Total population after cleaning:\", format(total_population_after, big.mark = \",\"), \"\\n\")\n\nTotal population after cleaning: 4,193,530 \n\ncat(\"Remaining rows:\", total_rows_after, \"\\n\")\n\nRemaining rows: 37243 \n\n\n\n\n\nNext, Using the duplicated function, we see that there are no duplicate entries in the data.\n\ndata[duplicated(data),]\n\n# A tibble: 0 × 6\n# ℹ 6 variables: PlanningArea &lt;chr&gt;, SubZone &lt;chr&gt;, Age &lt;dbl&gt;, Sex &lt;chr&gt;,\n#   ResidentCount &lt;dbl&gt;, AgeGroup &lt;fct&gt;\n\n\n\n\n\nWe run the code below to check for any missing values, and there is none.\n\ncolSums(is.na(data))\n\n PlanningArea       SubZone           Age           Sex ResidentCount \n            0             0             0             0             0 \n     AgeGroup \n            0 \n\n\n\n\n\nWe run an overview of the final dataset again before proceeding to the visualization. Final dataset contains 37,243 rows and 7 columns:\n\nglimpse(data)\n\nRows: 37,243\nColumns: 6\n$ PlanningArea  &lt;chr&gt; \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", …\n$ SubZone       &lt;chr&gt; \"Ang Mo Kio Town Centre\", \"Ang Mo Kio Town Centre\", \"Ang…\n$ Age           &lt;dbl&gt; 0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9,…\n$ Sex           &lt;chr&gt; \"Males\", \"Females\", \"Males\", \"Females\", \"Males\", \"Female…\n$ ResidentCount &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 30, 10, 20, 10, 20, 30, …\n$ AgeGroup      &lt;fct&gt; 0–4, 0–4, 0–4, 0–4, 0–4, 0–4, 0–4, 0–4, 0–4, 0–4, 5–9, 5…\n\n\n\n\n\n\n\n\n\nInsights:\n\nThe population pyramid reveals a dominant working-age group between ages 30–44, forming the broadest segment of the chart, with largest age group for Female: from 35-39 age group: 166,150 and Males: from 30-34 age group with 155,630\nThe base is narrower, especially for those aged 0–14, which highlights the ongoing trend of declining birth rates.\nFemales significantly outnumber males from age 65 onwards, highlighting gender differences in life expectancy\nThe median age of 42 reinforces Singapore’s aging trend, with implications for healthcare and eldercare planning.\nThe median age of 42 underscores Singapore’s aging population, signaling increasing needs in healthcare, retirement, and eldercare.\n\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npyramid_data &lt;- data %&gt;%\n  group_by(AgeGroup, Sex) %&gt;%\n  summarise(ResidentCount = sum(ResidentCount, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    ResidentCountSigned = ifelse(Sex == \"Males\", -ResidentCount, ResidentCount),\n    fill_color = case_when(\n      Sex == \"Males\" ~ \"#4292c6\",\n      Sex == \"Females\" ~ \"#e377c2\",\n      TRUE ~ \"gray\"\n    ),\n    AgeGroup = factor(AgeGroup, levels = c(\"0–4\", \"5–9\", \"10–14\", \"15–19\", \"20–24\",\n                                           \"25–29\", \"30–34\", \"35–39\", \"40–44\", \"45–49\",\n                                           \"50–54\", \"55–59\", \"60–64\", \"65–69\", \"70–74\",\n                                           \"75–79\", \"80–84\", \"85–89\", \"90+\"))\n  )\n\nmedian_age &lt;- data %&gt;%\n  group_by(Age) %&gt;%\n  summarise(Total = sum(ResidentCount, na.rm = TRUE)) %&gt;%\n  arrange(Age) %&gt;%\n  mutate(cum_pop = cumsum(Total), prop = cum_pop / sum(Total)) %&gt;%\n  filter(prop &gt;= 0.5) %&gt;%\n  slice(1) %&gt;%\n  pull(Age)\n\nage_group_labels &lt;- levels(pyramid_data$AgeGroup)\nmedian_group_index &lt;- findInterval(median_age, seq(0, 100, by = 5))\nmedian_group &lt;- age_group_labels[median_group_index]\n\ntotal_males &lt;- pyramid_data %&gt;% filter(Sex == \"Males\") %&gt;% summarise(sum = sum(abs(ResidentCountSigned))) %&gt;% pull(sum)\ntotal_females &lt;- pyramid_data %&gt;% filter(Sex == \"Females\") %&gt;% summarise(sum = sum(ResidentCountSigned)) %&gt;% pull(sum)\n\nggplot(pyramid_data, aes(y = AgeGroup, x = ResidentCountSigned, fill = fill_color)) +\n  geom_col(width = 0.9) +\n  geom_text(aes(label = abs(ResidentCountSigned),\n                x = ifelse(ResidentCountSigned &lt; 0, ResidentCountSigned - 5000, ResidentCountSigned + 5000)),\n            hjust = ifelse(pyramid_data$ResidentCountSigned &lt; 0, 1, 0),\n            size = 3, color = \"black\") +\n  annotate(\"segment\",\n           x = -max(abs(pyramid_data$ResidentCountSigned)) * 1.5,\n           xend = max(abs(pyramid_data$ResidentCountSigned)) * 1.5,\n           y = median_group, yend = median_group,\n           linetype = \"dotted\", color = \"#A9A9A9\", linewidth = 0.9) +\n  annotate(\"text\",\n           x = max(abs(pyramid_data$ResidentCountSigned)) * 1.5,\n           y = median_group,\n           label = paste0(\"Median: \", median_age),\n           hjust = 0, size = 2.8, color = \"black\", fontface = \"bold\") +\n  annotate(\"text\", y = \"0–4\",\n           x = -max(abs(pyramid_data$ResidentCountSigned)) * 0.95,\n           label = paste0(\"Males\\nTotal: \", format(total_males, big.mark = \",\")),\n           size = 2.6, color = \"#1E90FF\", fontface = \"bold\", hjust = 1) +\n  annotate(\"text\", y = \"0–4\",\n           x = max(abs(pyramid_data$ResidentCountSigned)) * 0.95,\n           label = paste0(\"Females\\nTotal: \", format(total_females, big.mark = \",\")),\n           size = 2.6, color = \"#c51b8a\", fontface = \"bold\", hjust = 0) +\n  scale_fill_identity() +\n  scale_x_continuous(labels = abs, expand = expansion(mult = c(0.12, 0.12))) +\n  labs(\n    title = \"Singapore’s Shifting Age Structure (June 2024)\",\n    subtitle = \"Middle-age Population Dominates; Youth Base Shrinking, Elderly Segment Rising\",\n    x = NULL,\n    y = \"Age Group (Years)\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    panel.background = element_rect(fill = \"#FFFCE8\", color = NA),\n    plot.background = element_rect(fill = \"#FFFCE8\", color = NA),\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\", margin = margin(b = 6)),\n    plot.subtitle = element_text(hjust = 0.5, size = 12, margin = margin(b = 12)),\n    axis.text.y = element_text(size = 10),\n    axis.title.y = element_text(size = 11, face = \"bold\"),\n    axis.text.x = element_blank(),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights:\n\nThe top chart shows the proportion of elderly residents in the top 10 planning areas. Outram has the highest share, with 26.9% of its population are seniors, followed by Ang Mo Kio (24.3%) and Bukit Merah (23.4%). These established towns may benefit from enhanced elderly-supportive environments, such as barrier-free access, senior-oriented amenities, and close-proximity services.\nThe bottom chart presents the elderly resident count, with Bedok having the largest at 60,770, followed by Tampines (49,700) and Hougang (44,640). This is largely due to their larger area size and population base. These towns would benefit from service scaling, such as Active Ageing Centres (AACs), public transport connectivity, and healthcare access.\nWith Singapore’s elderly population projected to reach one in four residents (DOS, 2024), it is important to consider both distribution by proportion and resident count for effective planning.\nThis dual perspective supports the Ministry of Health’s 2023 Action Plan, which aims to double eldercare centres by 2025 and enhance community-based support (MOH, 2023).\n\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nelderly_data &lt;- data %&gt;% filter(Age &gt;= 65)\n\ntotal_pop &lt;- data %&gt;%\n  group_by(PlanningArea) %&gt;%\n  summarise(Total_Pop = sum(ResidentCount, na.rm = TRUE))\n\nelderly_count &lt;- elderly_data %&gt;%\n  group_by(PlanningArea) %&gt;%\n  summarise(Elderly_Pop = sum(ResidentCount, na.rm = TRUE)) %&gt;%\n  arrange(desc(Elderly_Pop)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  mutate(PlanningArea = fct_reorder(PlanningArea, Elderly_Pop))\n\nelderly_prop &lt;- elderly_data %&gt;%\n  group_by(PlanningArea) %&gt;%\n  summarise(Elderly_Pop = sum(ResidentCount, na.rm = TRUE)) %&gt;%\n  left_join(total_pop, by = \"PlanningArea\") %&gt;%\n  mutate(Elderly_Proportion = Elderly_Pop / Total_Pop) %&gt;%\n  arrange(desc(Elderly_Proportion)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  mutate(PlanningArea = fct_reorder(PlanningArea, Elderly_Proportion))\n\n# Plot\np1 &lt;- ggplot(elderly_prop, aes(x = Elderly_Proportion, y = PlanningArea)) +\n  geom_col(fill = \"#4DAF4A\", width = 0.85) +\n  scale_x_continuous(\n    breaks = seq(0, 0.35, by = 0.05),\n    labels = percent_format(accuracy = 1),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  labs(\n    title = \"Top 10 Planning Areas by Elderly Proportion (Age 65+)\",\n    x = \"Proportion of Elderly Residents\",\n    y = NULL\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    panel.background = element_rect(fill = \"#FFFCE8\", color = NA),\n    plot.background = element_rect(fill = \"#FFFCE8\", color = NA),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n    axis.title.x = element_text(size = 11),\n    panel.grid.major.x = element_line(color = \"grey90\"),\n    panel.grid.minor = element_blank()\n  )\n\n\np2 &lt;- ggplot(elderly_count, aes(x = Elderly_Pop, y = PlanningArea)) +\n  geom_col(fill = \"#4DAF4A\", width = 0.85) +\n  scale_x_continuous(\n    breaks = seq(0, 70000, by = 10000),\n    labels = comma_format(),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  labs(\n    title = \"Top 10 Planning Areas by Elderly Resident Count (Age 65+)\",\n    x = \"Number of Elderly Residents\",\n    y = NULL\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    panel.background = element_rect(fill = \"#FFFCE8\", color = NA),\n    plot.background = element_rect(fill = \"#FFFCE8\", color = NA),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n    axis.title.x = element_text(size = 11),\n    panel.grid.major.x = element_line(color = \"grey90\"),\n    panel.grid.minor = element_blank()\n  )\n\np1 / p2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights:\n\nTampines, Bedok, and Sengkang top the list of most populated PA with over 250,000 residents each.\nMillennials (age 28–43) are the largest group in most PA, while Gen X dominates in Bedok, a mature estate.\nYounger generations (Gen Alpha and Gen Z) are more concentrated in newer towns like Sengkang, Punggol, and Jurong West, aligned with recent BTO developments that attract young families.\nThe distribution reflects a balanced generational mix, highlighting Singapore’s multigenerational living pattern—with both aging residents and young households sharing town spaces.\nThese trends align with Singapore’s Smart Nation and HDB’s ‘Designing for Life’ vision: fostering harmonious, inclusive communities where families of all ages can live, age, and thrive together through well-integrated facilities, technology, and people-first urban design.\n\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata &lt;- data %&gt;%\n  mutate(Age = as.numeric(Age),\n         Generation = case_when(\n           Age &lt;= 9 ~ \"Gen Alpha (≤9)\",\n           Age &lt;= 27 ~ \"Gen Z (10–27)\",\n           Age &lt;= 43 ~ \"Millennials (28–43)\",\n           Age &lt;= 59 ~ \"Gen X (44–59)\",\n           Age &lt;= 77 ~ \"Baby Boomers (60–77)\",\n           TRUE ~ \"Silent Gen (78+)\"\n         ))\n\ngen_by_area &lt;- data %&gt;%\n  group_by(PlanningArea, Generation) %&gt;%\n  summarise(ResidentCount = sum(ResidentCount, na.rm = TRUE), .groups = \"drop\")\n\ntop10_areas &lt;- gen_by_area %&gt;%\n  group_by(PlanningArea) %&gt;%\n  summarise(TotalPop = sum(ResidentCount)) %&gt;%\n  arrange(desc(TotalPop)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  pull(PlanningArea)\n\ngen_top10 &lt;- gen_by_area %&gt;%\n  filter(PlanningArea %in% top10_areas) %&gt;%\n  mutate(\n    PlanningArea = fct_reorder(PlanningArea, ResidentCount, .fun = sum, .desc = TRUE),\n    Generation = factor(Generation, levels = c(\"Silent Gen (78+)\", \"Baby Boomers (60–77)\",\n                                               \"Gen X (44–59)\", \"Millennials (28–43)\",\n                                               \"Gen Z (10–27)\", \"Gen Alpha (≤9)\"))\n  )\n\ngen_colors &lt;- c(\n  \"Silent Gen (78+)\" = \"#c6dbef\",\n  \"Baby Boomers (60–77)\" = \"#6baed6\",\n  \"Gen X (44–59)\" = \"#b2df8a\",\n  \"Millennials (28–43)\" = \"#33a02c\",\n  \"Gen Z (10–27)\" = \"#fb9a99\",\n  \"Gen Alpha (≤9)\" = \"#e31a1c\"\n)\n\n#Plot\nggplot(gen_top10, aes(x = PlanningArea, y = ResidentCount, fill = Generation)) +\n  geom_col(width = 0.8, color = \"white\") +\n  scale_fill_manual(values = gen_colors) +\n  scale_y_continuous(\n    labels = comma,\n    breaks = seq(0, 300000, 50000),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  labs(\n    title = \"Generational Composition of Top 10 Most Populated Planning Areas\",\n    subtitle = \"Younger generations dominate newer towns, while older cohorts concentrate in mature estates\",\n    x = \"Planning Area\",\n    y = \"Resident Count\",\n    fill = \"Generation\"\n  ) +\n  theme_clean(base_size = 12) +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(size = 11, hjust = 0.5),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid.major.x = element_blank(),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSingapore’s demographic structure, based on June 2024 data, highlights a maturing society with a dominant working-age group and a median age of 42. The growing share of seniors and the narrowing base of younger age groups reflect the effects of population aging and low birth rates. Mature estates such as Outram have the highest proportion of elderly residents, while Bedok and Tampines house the largest absolute numbers. In contrast, newer towns like Sengkang and Punggol show higher concentrations of younger generations—particularly Gen Alpha and Gen Z—driven by recent BTO developments attracting young families. Millennials remain the largest generational group across most areas, reinforcing their role in shaping urban life. This evolving yet balanced generational landscape underscores the need for inclusive community planning that supports both young families and seniors—fostering intergenerational harmony and enabling families to live, age, and thrive together.\n\n\n\n\nDepartment of Statistics Singapore. (2024). Population Trends 2024.\nRetrieved from: https://www.singstat.gov.sg/publications/population/population-trends\nMinistry of Health Singapore. (2023). Action Plan for Successful Ageing.\nRetrieved from: https://www.moh.gov.sg/newsroom/launch-of-the-2023-action-plan-for-successful-ageing\nHousing & Development Board (HDB). (2021). Designing for Life: Community Planning and Design Guide.\nRetrieved from: https://www.hdb.gov.sg/cs/infoweb/designing-for-life\nSmart Nation and Digital Government Office. (2023). Smart Nation: Empowering Everyone Through Technology.\nRetrieved from: https://www.smartnation.gov.sg\nSingapore Department of Statistics. (n.d.). National Statistical Standards.\nRetrieved from: https://www.singstat.gov.sg/standards"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_1/Take-home_Ex01.html#overview",
    "href": "Take-home_Ex/Take-home_Ex_1/Take-home_Ex01.html#overview",
    "title": "Take-Home Exercise 1",
    "section": "",
    "text": "Creating enlightening and truthful data visualizations involves focusing on accuracy, transparency, and the ability to effectively communicate insights. It’s about presenting data in a way that is both informative and aesthetically pleasing, ensuring the audience can grasp the information quickly and accurately."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_1/Take-home_Ex01.html#setting-the-scene",
    "href": "Take-home_Ex/Take-home_Ex_1/Take-home_Ex01.html#setting-the-scene",
    "title": "Take-Home Exercise 1",
    "section": "",
    "text": "A local online media company that publishes daily content on digital platforms is planning to release an article on demographic structures and distribution of Singapore in 2024."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_1/Take-home_Ex01.html#the-task",
    "href": "Take-home_Ex/Take-home_Ex_1/Take-home_Ex01.html#the-task",
    "title": "Take-Home Exercise 1",
    "section": "",
    "text": "Assuming the role of the graphical editor of the media company, we are tasked to prepare at most three data visualization for the article."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_1/Take-home_Ex01.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex_1/Take-home_Ex01.html#getting-started",
    "title": "Take-Home Exercise 1",
    "section": "",
    "text": "We load the following R packages using the pacman::p_load() function:\n\ntidyverse: R packages designed for data science\nggrepel: to provides geoms for ggplot2 to repel overlapping text labels\nggthemes: to use additional themes for ggplot2\npatchwork: to prepare composite figure created using ggplot2\nscales: to provide the internal scaling infrastructure used by ggplot2\nggpubr to create publication ready ggplot2 plots.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed in the computer.\n\npacman::p_load(tidyverse, ggrepel, patchwork, ggthemes, scales,\n               ggpubr) \n\n\n\n\nTo accomplish the task, Singapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2024 dataset shared by Department of Statistics, Singapore (DOS) will be used and we wil load it as follows:\n\ndata &lt;- read_csv(\"data/respopagesex2024.csv\", col_names = TRUE)\n\n\n\n\n\n\nWe first take a look at the data. Using the code below, we can get the details of the dataset which contains 60,424 rows and 6 columns.\n\nglimpse(data)\n\nRows: 60,424\nColumns: 6\n$ PA   &lt;chr&gt; \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo K…\n$ SZ   &lt;chr&gt; \"Ang Mo Kio Town Centre\", \"Ang Mo Kio Town Centre\", \"Ang Mo Kio T…\n$ Age  &lt;chr&gt; \"0\", \"0\", \"1\", \"1\", \"2\", \"2\", \"3\", \"3\", \"4\", \"4\", \"5\", \"5\", \"6\", …\n$ Sex  &lt;chr&gt; \"Males\", \"Females\", \"Males\", \"Females\", \"Males\", \"Females\", \"Male…\n$ Pop  &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 30, 10, 20, 10, 20, 30, 30, 10, 3…\n$ Time &lt;dbl&gt; 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024,…\n\n\n\n\n\n\nWe notice that there is only one value in Time column (2024) which will not be used for further analysis, we will delete this column as per the code chunk below:\n\n\ndata &lt;- data %&gt;% select(-Time)\n\n\nWe will rename the column names in the dataset for clarity, as detailed provided by the Department of Statistics (DOS), as follows:\n\nPA → Planning Area\nSZ → Subzone\nPop → Resident Count\n\n\n\n\n\n\nSide Note:\n\n\n\nPlease note: according to the DOS accompanying documentation of this dataset, the population figures in the csv file have been rounded to the nearest 10, and as such, total counts may not sum exactly due to rounding adjustments.\n\n\n\ncolnames(data) &lt;- c(\"PlanningArea\", \"SubZone\", \"Age\", \"Sex\", \"ResidentCount\")\n\n\nNext, we observe that for the Age column, there is a value of : “90_and_over”. We will replace this value with “90” and change the data type from string/character to numeric and then create a new column to classify the age according to the age bracket in the interval of 5 years as per the standard age group published in DOS, with the following code:\n\n\ndata &lt;- data %&gt;%\n  mutate(\n    Age = str_to_lower(Age),\n    Age = ifelse(Age == \"90_and_over\", \"90\", Age),\n    Age = as.numeric(Age),\n    AgeGroup = cut(Age,\n                   breaks = c(0,4,9,14,19,24,29,34,39,44,49,54,59,64,69,74,79,84,89, Inf),\n                   labels = c(\"0–4\", \"5–9\", \"10–14\", \"15–19\", \"20–24\", \"25–29\",\n                              \"30–34\", \"35–39\", \"40–44\", \"45–49\", \"50–54\", \n                              \"55–59\", \"60–64\", \"65–69\", \"70–74\", \"75–79\", \n                              \"80–84\", \"85–89\", \"90+\"),\n                   right = TRUE, include.lowest = TRUE)\n  )\n\n\nFurther observation of the dataset, we discover there are multiple rows with “0” values in the “Pop”/“ResidentCount” column. We will remove these rows as per the code chunk below, and calculate the number of rows and total population before and after the deletion to ensure completeness with the following code:\n\n\ntotal_population_before &lt;- sum(data$ResidentCount, na.rm = TRUE)\ntotal_rows_before &lt;- nrow(data)\n\nzero_count &lt;- data %&gt;%\n  filter(ResidentCount == 0) %&gt;%\n  nrow()\n\ncat(\"Total population before cleaning:\", format(total_population_before, big.mark = \",\"), \"\\n\")\n\nTotal population before cleaning: 4,193,530 \n\ncat(\"Total rows before cleaning:\", total_rows_before, \"\\n\")\n\nTotal rows before cleaning: 60424 \n\ncat(\"Rows with 0 ResidentCount removed:\", zero_count, \"\\n\")\n\nRows with 0 ResidentCount removed: 23181 \n\ndata &lt;- data %&gt;%\n  filter(ResidentCount &gt; 0)\n\ntotal_population_after &lt;- sum(data$ResidentCount, na.rm = TRUE)\ntotal_rows_after &lt;- nrow(data)\n\ncat(\"Total population after cleaning:\", format(total_population_after, big.mark = \",\"), \"\\n\")\n\nTotal population after cleaning: 4,193,530 \n\ncat(\"Remaining rows:\", total_rows_after, \"\\n\")\n\nRemaining rows: 37243 \n\n\n\n\n\nNext, Using the duplicated function, we see that there are no duplicate entries in the data.\n\ndata[duplicated(data),]\n\n# A tibble: 0 × 6\n# ℹ 6 variables: PlanningArea &lt;chr&gt;, SubZone &lt;chr&gt;, Age &lt;dbl&gt;, Sex &lt;chr&gt;,\n#   ResidentCount &lt;dbl&gt;, AgeGroup &lt;fct&gt;\n\n\n\n\n\nWe run the code below to check for any missing values, and there is none.\n\ncolSums(is.na(data))\n\n PlanningArea       SubZone           Age           Sex ResidentCount \n            0             0             0             0             0 \n     AgeGroup \n            0 \n\n\n\n\n\nWe run an overview of the final dataset again before proceeding to the visualization. Final dataset contains 37,243 rows and 7 columns:\n\nglimpse(data)\n\nRows: 37,243\nColumns: 6\n$ PlanningArea  &lt;chr&gt; \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", …\n$ SubZone       &lt;chr&gt; \"Ang Mo Kio Town Centre\", \"Ang Mo Kio Town Centre\", \"Ang…\n$ Age           &lt;dbl&gt; 0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9,…\n$ Sex           &lt;chr&gt; \"Males\", \"Females\", \"Males\", \"Females\", \"Males\", \"Female…\n$ ResidentCount &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 30, 10, 20, 10, 20, 30, …\n$ AgeGroup      &lt;fct&gt; 0–4, 0–4, 0–4, 0–4, 0–4, 0–4, 0–4, 0–4, 0–4, 0–4, 5–9, 5…"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_1/Take-home_Ex01.html#data-visualization",
    "href": "Take-home_Ex/Take-home_Ex_1/Take-home_Ex01.html#data-visualization",
    "title": "Take-Home Exercise 1",
    "section": "",
    "text": "Insights:\n\nThe population pyramid reveals a dominant working-age group between ages 30–44, forming the broadest segment of the chart, with largest age group for Female: from 35-39 age group: 166,150 and Males: from 30-34 age group with 155,630\nThe base is narrower, especially for those aged 0–14, which highlights the ongoing trend of declining birth rates.\nFemales significantly outnumber males from age 65 onwards, highlighting gender differences in life expectancy\nThe median age of 42 reinforces Singapore’s aging trend, with implications for healthcare and eldercare planning.\nThe median age of 42 underscores Singapore’s aging population, signaling increasing needs in healthcare, retirement, and eldercare.\n\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npyramid_data &lt;- data %&gt;%\n  group_by(AgeGroup, Sex) %&gt;%\n  summarise(ResidentCount = sum(ResidentCount, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    ResidentCountSigned = ifelse(Sex == \"Males\", -ResidentCount, ResidentCount),\n    fill_color = case_when(\n      Sex == \"Males\" ~ \"#4292c6\",\n      Sex == \"Females\" ~ \"#e377c2\",\n      TRUE ~ \"gray\"\n    ),\n    AgeGroup = factor(AgeGroup, levels = c(\"0–4\", \"5–9\", \"10–14\", \"15–19\", \"20–24\",\n                                           \"25–29\", \"30–34\", \"35–39\", \"40–44\", \"45–49\",\n                                           \"50–54\", \"55–59\", \"60–64\", \"65–69\", \"70–74\",\n                                           \"75–79\", \"80–84\", \"85–89\", \"90+\"))\n  )\n\nmedian_age &lt;- data %&gt;%\n  group_by(Age) %&gt;%\n  summarise(Total = sum(ResidentCount, na.rm = TRUE)) %&gt;%\n  arrange(Age) %&gt;%\n  mutate(cum_pop = cumsum(Total), prop = cum_pop / sum(Total)) %&gt;%\n  filter(prop &gt;= 0.5) %&gt;%\n  slice(1) %&gt;%\n  pull(Age)\n\nage_group_labels &lt;- levels(pyramid_data$AgeGroup)\nmedian_group_index &lt;- findInterval(median_age, seq(0, 100, by = 5))\nmedian_group &lt;- age_group_labels[median_group_index]\n\ntotal_males &lt;- pyramid_data %&gt;% filter(Sex == \"Males\") %&gt;% summarise(sum = sum(abs(ResidentCountSigned))) %&gt;% pull(sum)\ntotal_females &lt;- pyramid_data %&gt;% filter(Sex == \"Females\") %&gt;% summarise(sum = sum(ResidentCountSigned)) %&gt;% pull(sum)\n\nggplot(pyramid_data, aes(y = AgeGroup, x = ResidentCountSigned, fill = fill_color)) +\n  geom_col(width = 0.9) +\n  geom_text(aes(label = abs(ResidentCountSigned),\n                x = ifelse(ResidentCountSigned &lt; 0, ResidentCountSigned - 5000, ResidentCountSigned + 5000)),\n            hjust = ifelse(pyramid_data$ResidentCountSigned &lt; 0, 1, 0),\n            size = 3, color = \"black\") +\n  annotate(\"segment\",\n           x = -max(abs(pyramid_data$ResidentCountSigned)) * 1.5,\n           xend = max(abs(pyramid_data$ResidentCountSigned)) * 1.5,\n           y = median_group, yend = median_group,\n           linetype = \"dotted\", color = \"#A9A9A9\", linewidth = 0.9) +\n  annotate(\"text\",\n           x = max(abs(pyramid_data$ResidentCountSigned)) * 1.5,\n           y = median_group,\n           label = paste0(\"Median: \", median_age),\n           hjust = 0, size = 2.8, color = \"black\", fontface = \"bold\") +\n  annotate(\"text\", y = \"0–4\",\n           x = -max(abs(pyramid_data$ResidentCountSigned)) * 0.95,\n           label = paste0(\"Males\\nTotal: \", format(total_males, big.mark = \",\")),\n           size = 2.6, color = \"#1E90FF\", fontface = \"bold\", hjust = 1) +\n  annotate(\"text\", y = \"0–4\",\n           x = max(abs(pyramid_data$ResidentCountSigned)) * 0.95,\n           label = paste0(\"Females\\nTotal: \", format(total_females, big.mark = \",\")),\n           size = 2.6, color = \"#c51b8a\", fontface = \"bold\", hjust = 0) +\n  scale_fill_identity() +\n  scale_x_continuous(labels = abs, expand = expansion(mult = c(0.12, 0.12))) +\n  labs(\n    title = \"Singapore’s Shifting Age Structure (June 2024)\",\n    subtitle = \"Middle-age Population Dominates; Youth Base Shrinking, Elderly Segment Rising\",\n    x = NULL,\n    y = \"Age Group (Years)\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    panel.background = element_rect(fill = \"#FFFCE8\", color = NA),\n    plot.background = element_rect(fill = \"#FFFCE8\", color = NA),\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\", margin = margin(b = 6)),\n    plot.subtitle = element_text(hjust = 0.5, size = 12, margin = margin(b = 12)),\n    axis.text.y = element_text(size = 10),\n    axis.title.y = element_text(size = 11, face = \"bold\"),\n    axis.text.x = element_blank(),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights:\n\nThe top chart shows the proportion of elderly residents in the top 10 planning areas. Outram has the highest share, with 26.9% of its population are seniors, followed by Ang Mo Kio (24.3%) and Bukit Merah (23.4%). These established towns may benefit from enhanced elderly-supportive environments, such as barrier-free access, senior-oriented amenities, and close-proximity services.\nThe bottom chart presents the elderly resident count, with Bedok having the largest at 60,770, followed by Tampines (49,700) and Hougang (44,640). This is largely due to their larger area size and population base. These towns would benefit from service scaling, such as Active Ageing Centres (AACs), public transport connectivity, and healthcare access.\nWith Singapore’s elderly population projected to reach one in four residents (DOS, 2024), it is important to consider both distribution by proportion and resident count for effective planning.\nThis dual perspective supports the Ministry of Health’s 2023 Action Plan, which aims to double eldercare centres by 2025 and enhance community-based support (MOH, 2023).\n\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nelderly_data &lt;- data %&gt;% filter(Age &gt;= 65)\n\ntotal_pop &lt;- data %&gt;%\n  group_by(PlanningArea) %&gt;%\n  summarise(Total_Pop = sum(ResidentCount, na.rm = TRUE))\n\nelderly_count &lt;- elderly_data %&gt;%\n  group_by(PlanningArea) %&gt;%\n  summarise(Elderly_Pop = sum(ResidentCount, na.rm = TRUE)) %&gt;%\n  arrange(desc(Elderly_Pop)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  mutate(PlanningArea = fct_reorder(PlanningArea, Elderly_Pop))\n\nelderly_prop &lt;- elderly_data %&gt;%\n  group_by(PlanningArea) %&gt;%\n  summarise(Elderly_Pop = sum(ResidentCount, na.rm = TRUE)) %&gt;%\n  left_join(total_pop, by = \"PlanningArea\") %&gt;%\n  mutate(Elderly_Proportion = Elderly_Pop / Total_Pop) %&gt;%\n  arrange(desc(Elderly_Proportion)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  mutate(PlanningArea = fct_reorder(PlanningArea, Elderly_Proportion))\n\n# Plot\np1 &lt;- ggplot(elderly_prop, aes(x = Elderly_Proportion, y = PlanningArea)) +\n  geom_col(fill = \"#4DAF4A\", width = 0.85) +\n  scale_x_continuous(\n    breaks = seq(0, 0.35, by = 0.05),\n    labels = percent_format(accuracy = 1),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  labs(\n    title = \"Top 10 Planning Areas by Elderly Proportion (Age 65+)\",\n    x = \"Proportion of Elderly Residents\",\n    y = NULL\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    panel.background = element_rect(fill = \"#FFFCE8\", color = NA),\n    plot.background = element_rect(fill = \"#FFFCE8\", color = NA),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n    axis.title.x = element_text(size = 11),\n    panel.grid.major.x = element_line(color = \"grey90\"),\n    panel.grid.minor = element_blank()\n  )\n\n\np2 &lt;- ggplot(elderly_count, aes(x = Elderly_Pop, y = PlanningArea)) +\n  geom_col(fill = \"#4DAF4A\", width = 0.85) +\n  scale_x_continuous(\n    breaks = seq(0, 70000, by = 10000),\n    labels = comma_format(),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  labs(\n    title = \"Top 10 Planning Areas by Elderly Resident Count (Age 65+)\",\n    x = \"Number of Elderly Residents\",\n    y = NULL\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    panel.background = element_rect(fill = \"#FFFCE8\", color = NA),\n    plot.background = element_rect(fill = \"#FFFCE8\", color = NA),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n    axis.title.x = element_text(size = 11),\n    panel.grid.major.x = element_line(color = \"grey90\"),\n    panel.grid.minor = element_blank()\n  )\n\np1 / p2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights:\n\nTampines, Bedok, and Sengkang top the list of most populated PA with over 250,000 residents each.\nMillennials (age 28–43) are the largest group in most PA, while Gen X dominates in Bedok, a mature estate.\nYounger generations (Gen Alpha and Gen Z) are more concentrated in newer towns like Sengkang, Punggol, and Jurong West, aligned with recent BTO developments that attract young families.\nThe distribution reflects a balanced generational mix, highlighting Singapore’s multigenerational living pattern—with both aging residents and young households sharing town spaces.\nThese trends align with Singapore’s Smart Nation and HDB’s ‘Designing for Life’ vision: fostering harmonious, inclusive communities where families of all ages can live, age, and thrive together through well-integrated facilities, technology, and people-first urban design.\n\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata &lt;- data %&gt;%\n  mutate(Age = as.numeric(Age),\n         Generation = case_when(\n           Age &lt;= 9 ~ \"Gen Alpha (≤9)\",\n           Age &lt;= 27 ~ \"Gen Z (10–27)\",\n           Age &lt;= 43 ~ \"Millennials (28–43)\",\n           Age &lt;= 59 ~ \"Gen X (44–59)\",\n           Age &lt;= 77 ~ \"Baby Boomers (60–77)\",\n           TRUE ~ \"Silent Gen (78+)\"\n         ))\n\ngen_by_area &lt;- data %&gt;%\n  group_by(PlanningArea, Generation) %&gt;%\n  summarise(ResidentCount = sum(ResidentCount, na.rm = TRUE), .groups = \"drop\")\n\ntop10_areas &lt;- gen_by_area %&gt;%\n  group_by(PlanningArea) %&gt;%\n  summarise(TotalPop = sum(ResidentCount)) %&gt;%\n  arrange(desc(TotalPop)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  pull(PlanningArea)\n\ngen_top10 &lt;- gen_by_area %&gt;%\n  filter(PlanningArea %in% top10_areas) %&gt;%\n  mutate(\n    PlanningArea = fct_reorder(PlanningArea, ResidentCount, .fun = sum, .desc = TRUE),\n    Generation = factor(Generation, levels = c(\"Silent Gen (78+)\", \"Baby Boomers (60–77)\",\n                                               \"Gen X (44–59)\", \"Millennials (28–43)\",\n                                               \"Gen Z (10–27)\", \"Gen Alpha (≤9)\"))\n  )\n\ngen_colors &lt;- c(\n  \"Silent Gen (78+)\" = \"#c6dbef\",\n  \"Baby Boomers (60–77)\" = \"#6baed6\",\n  \"Gen X (44–59)\" = \"#b2df8a\",\n  \"Millennials (28–43)\" = \"#33a02c\",\n  \"Gen Z (10–27)\" = \"#fb9a99\",\n  \"Gen Alpha (≤9)\" = \"#e31a1c\"\n)\n\n#Plot\nggplot(gen_top10, aes(x = PlanningArea, y = ResidentCount, fill = Generation)) +\n  geom_col(width = 0.8, color = \"white\") +\n  scale_fill_manual(values = gen_colors) +\n  scale_y_continuous(\n    labels = comma,\n    breaks = seq(0, 300000, 50000),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  labs(\n    title = \"Generational Composition of Top 10 Most Populated Planning Areas\",\n    subtitle = \"Younger generations dominate newer towns, while older cohorts concentrate in mature estates\",\n    x = \"Planning Area\",\n    y = \"Resident Count\",\n    fill = \"Generation\"\n  ) +\n  theme_clean(base_size = 12) +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(size = 11, hjust = 0.5),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid.major.x = element_blank(),\n    legend.position = \"right\"\n  )"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_1/Take-home_Ex01.html#summary",
    "href": "Take-home_Ex/Take-home_Ex_1/Take-home_Ex01.html#summary",
    "title": "Take-Home Exercise 1",
    "section": "",
    "text": "Singapore’s demographic structure, based on June 2024 data, highlights a maturing society with a dominant working-age group and a median age of 42. The growing share of seniors and the narrowing base of younger age groups reflect the effects of population aging and low birth rates. Mature estates such as Outram have the highest proportion of elderly residents, while Bedok and Tampines house the largest absolute numbers. In contrast, newer towns like Sengkang and Punggol show higher concentrations of younger generations—particularly Gen Alpha and Gen Z—driven by recent BTO developments attracting young families. Millennials remain the largest generational group across most areas, reinforcing their role in shaping urban life. This evolving yet balanced generational landscape underscores the need for inclusive community planning that supports both young families and seniors—fostering intergenerational harmony and enabling families to live, age, and thrive together."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_1/Take-home_Ex01.html#references",
    "href": "Take-home_Ex/Take-home_Ex_1/Take-home_Ex01.html#references",
    "title": "Take-Home Exercise 1",
    "section": "",
    "text": "Department of Statistics Singapore. (2024). Population Trends 2024.\nRetrieved from: https://www.singstat.gov.sg/publications/population/population-trends\nMinistry of Health Singapore. (2023). Action Plan for Successful Ageing.\nRetrieved from: https://www.moh.gov.sg/newsroom/launch-of-the-2023-action-plan-for-successful-ageing\nHousing & Development Board (HDB). (2021). Designing for Life: Community Planning and Design Guide.\nRetrieved from: https://www.hdb.gov.sg/cs/infoweb/designing-for-life\nSmart Nation and Digital Government Office. (2023). Smart Nation: Empowering Everyone Through Technology.\nRetrieved from: https://www.smartnation.gov.sg\nSingapore Department of Statistics. (n.d.). National Statistical Standards.\nRetrieved from: https://www.singstat.gov.sg/standards"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "title": "Hands-on Exercise 05",
    "section": "",
    "text": "In this hands-on exercise, we will learn how to model, analyse and visualise network data using R,\nto be able to:\n\ncreate graph object data frames, manipulate them using appropriate functions of dplyr, lubridate, and tidygraph,\nbuild network graph visualisation using appropriate functions of ggraph,\ncompute network geometrics using tidygraph,\nbuild advanced graph visualisation by incorporating the network geometrics, and\nbuild interactive network visualisation using visNetwork package.\n\n\n\n\n\n\nIn this hands-on exercise, four network data modelling and visualisation packages will be installed and launched. They are igraph, tidygraph, ggraph and visNetwork. Beside these four packages, tidyverse and lubridate, an R package specially designed to handle and wrangling time data will be installed and launched too.\nThe code chunk:\n\n\nShow the code\n\n\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts, \n               concaveman, ggforce)\n\n\n\n\n\n\nThe data sets used in this hands-on exercise is from an oil exploration and extraction company. There are two data sets. One contains the nodes data and the other contains the edges (also know as link) data.\n\n\n\nGAStech-email_edges.csv which consists of two weeks of 9063 emails correspondances between 55 employees.\n\n\n\n\n\n\nGAStech_email_nodes.csv which consist of the names, department and title of the 55 employees.\n\n\n\n\n\nIn this step, we will import GAStech_email_node.csv and GAStech_email_edges-v2.csv into RStudio environment by using read_csv() of readr package.\n\n\nShow the code\n\n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\n\n\n\n\nNext, we will examine the structure of the data frame using glimpse() of dplyr.\n\n\nShow the code\n\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe output report of GAStech_edges above reveals that the SentDate is treated as “Character” data type instead of date data type. This is an error! Before we continue, it is important for us to change the data type of SentDate field back to “Date”” data type.\n\n\n\n\n\nThe code chunk below will be used to perform the changes.\n\n\nShow the code\n\n\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nboth dmy() and wday() are functions of lubridate package. lubridate is an R package that makes it easier to work with dates and times.\ndmy() transforms the SentDate to Date data type.\nwday() returns the day of the week as a decimal number or an ordered factor if label is TRUE. The argument abbr is FALSE keep the daya spells in full, i.e. Monday. The function will create a new column in the data.frame i.e. Weekday and the output of wday() will save in this newly created field.\nthe values in the Weekday field are in ordinal scale.\n\n\n\n\n\n\nTable below shows the data structure of the reformatted GAStech_edges data frame\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 10\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n$ SendDate    &lt;date&gt; 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-0…\n$ Weekday     &lt;ord&gt; Friday, Friday, Friday, Friday, Friday, Friday, Friday, Fr…\n\n\n\n\n\nA close examination of GAStech_edges data.frame reveals that it consists of individual e-mail flow records. This is not very useful for visualisation.\nIn view of this, we will aggregate the individual by date, senders, receivers, main subject and day of the week.\nThe code chunk:\n\n\nShow the code\n\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nfour functions from dplyr package are used. They are: filter(), group(), summarise(), and ungroup().\nThe output data.frame is called GAStech_edges_aggregated.\nA new field called Weight has been added in GAStech_edges_aggregated.\n\n\n\n\n\n\nTable below shows the data structure of the reformatted GAStech_edges_aggregated data frame\n\nglimpse(GAStech_edges_aggregated)\n\nRows: 1,372\nColumns: 4\n$ source  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ target  &lt;dbl&gt; 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6,…\n$ Weekday &lt;ord&gt; Sunday, Monday, Tuesday, Wednesday, Friday, Sunday, Monday, Tu…\n$ Weight  &lt;int&gt; 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5,…\n\n\n\n\n\n\nIn this section, you will learn how to create a graph data model by using tidygraph package. It provides a tidy API for graph/network manipulation. While network data itself is not tidy, it can be envisioned as two tidy tables, one for node data and one for edge data. tidygraph provides a way to switch between the two tables and provides dplyr verbs for manipulating them. Furthermore it provides access to a lot of graph algorithms with return values that facilitate their use in a tidy workflow.\nBefore getting started, you are advised to read these two articles:\n\nIntroducing tidygraph\ntidygraph 1.1 - A tidy hope\n\n\n\nTwo functions of tidygraph package can be used to create network objects, they are:\n\ntbl_graph() creates a tbl_graph network object from nodes and edges data.\nas_tbl_graph() converts network data and objects to a tbl_graph network. Below are network data and objects supported by as_tbl_graph()\n\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor).\n\n\n\n\n\n\nactivate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\n\n\n\nIn the above the .N() function is used to gain access to the node data while manipulating the edge data. Similarly .E() will give you the edge data and .G() will give you the tbl_graph object itself.\n\n\n\n\nIn this section, we will use tbl_graph() of tinygraph package to build an tidygraph’s network graph data.frame.\nBefore typing the codes, it is recommended to review to reference guide of tbl_graph()\n\n\nShow the code\n\n\nGAStech_graph &lt;- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\n\n\n\n\n\nShow the code\n\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 × 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Mana…\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# ℹ 44 more rows\n#\n# Edge Data: 1,372 × 4\n   from    to Weekday Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\n\n\n\n\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 4541 edges.\nThe command also prints the first six rows of “Node Data” and the first three of “Edge Data”.\nIt states that the Node Data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.\n\n\n\n\nThe nodes tibble data frame is activated by default, but you can change which tibble data frame is active with the activate() function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest “weight” first, we could use activate() and then arrange().\nFor example,\n\n\nShow the code\n\n\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(Weight))\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 1,372 × 4 (active)\n    from    to Weekday   Weight\n   &lt;int&gt; &lt;int&gt; &lt;ord&gt;      &lt;int&gt;\n 1    40    41 Saturday      13\n 2    41    43 Monday        11\n 3    35    31 Tuesday       10\n 4    40    41 Monday        10\n 5    40    43 Monday        10\n 6    36    32 Sunday         9\n 7    40    43 Saturday       9\n 8    41    40 Monday         9\n 9    19    15 Wednesday      8\n10    35    38 Tuesday        8\n# ℹ 1,362 more rows\n#\n# Node Data: 54 × 4\n     id label           Department     Title           \n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows\n\n\n\nVisit the reference guide of activate() to find out more about the function.\n\n\n\n\nggraph is an extension of ggplot2, making it easier to carry over basic ggplot skills to the design of network graphs.\nAs in all network graph, there are three main aspects to a ggraph’s network graph, they are:\n\nnodes,\nedges and\nlayouts.\n\nFor a comprehensive discussion of each of this aspect of graph, please refer to their respective vignettes provided.\n\n\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using GAStech_graph. Before your get started, it is advisable to read their respective reference guide at least once.\n\n\nShow the code\n\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nThe basic plotting function is ggraph(), which takes the data to be used for the graph and the type of layout desired. Both of the arguments for ggraph() are built around igraph. Therefore, ggraph() can use either an igraph object or a tbl_graph object.\n\n\n\n\n\n\nIn this section, you will use theme_graph() to remove the x and y axes. Before your get started, it is advisable to read it’s reference guide at least once.\n\n\nShow the code\n\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nggraph introduces a special ggplot theme that provides better defaults for network graphs than the normal ggplot defaults. theme_graph(), besides removing axes, grids, and border, changes the font to Arial Narrow (this can be overridden).\nThe ggraph theme can be set for a series of plots with the set_graph_style() command run before the graphs are plotted or by using theme_graph() in the individual plots.\n\n\n\n\n\n\nFurthermore, theme_graph() makes it easy to change the coloring of the plot.\n\n\nShow the code\n\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n\n\n\n\n\n\nggraph support many layout for standard used, they are: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl. Figures below and on the right show layouts supported by ggraph().\n\n\n\n\nThe code chunks below will be used to plot the network graph using Fruchterman and Reingold layout.\n\n\nShow the code\n\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above:\n\nlayout argument is used to define the layout to be used.\n\n\n\n\nIn this section, we will colour each node by referring to their respective departments.\n\n\nShow the code\n\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunks above:\n\ngeom_node_point is equivalent in functionality to geo_point of ggplot2. It allows for simple plotting of nodes in different shapes, colours and sizes. In the codes chnuks above colour and size are used.\n\n\n\n\nIn the code chunk below, the thickness of the edges will be mapped with the Weight variable.\n\n\nShow the code\n\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunks above:\n\ngeom_edge_link draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. In the example above, argument width is used to map the width of the line in proportional to the Weight attribute and argument alpha is used to introduce opacity on the line.\n\n\n\n\n\nAnother very useful feature of ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaning way by spreading nodes and edges out based on their attributes. In this section, we will learn how to use faceting technique to visualise network data.\nThere are three functions in ggraph to implement faceting, they are:\n\nfacet_nodes() whereby edges are only draw in a panel if both terminal nodes are present here,\nfacet_edges() whereby nodes are always drawn in al panels even if the node data contains an attribute named the same as the one used for the edge facetting, and\nfacet_graph() faceting on two variables simultaneously.\n\n\n\nIn the code chunk below, facet_edges() is used. Before getting started, it is advisable for us to read it’s reference guide at least once.\n\n\nShow the code\n\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n\n\n\n\n\nThe code chunk below uses theme() to change the position of the legend.\n\n\nShow the code\n\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n\n\n\n\n\nThe code chunk below adds frame to each graph.\n\n\nShow the code\n\n\nset_graph_style() \n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\n\n\n\n\nIn the code chunkc below, facet_nodes() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\n\nShow the code\n\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures, namely: degree, betweenness, closeness and eigenvector. It is beyond the scope of this hands-on exercise to cover the principles and mathematics of these measure here. Students are encouraged to refer to Chapter 7: Actor Prominence of A User’s Guide to Network Analysis in R to gain better understanding of theses network measures.\n\n\nShow the code\n\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\nmutate() of dplyr is used to perform the computation.\nthe algorithm used, on the other hand, is the centrality_betweenness() of tidygraph.\n\n\n\n\nIt is important to note that from ggraph v2.0 onward tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\n\n\nShow the code\n\n\ng &lt;- GAStech_graph %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\n\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. Use this link to find out more about community detection functions provided by tidygraph,\nIn the code chunk below group_edge_betweenness() is used.\n\n\nShow the code\n\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(community = as.factor(\n    group_edge_betweenness(\n      weights = Weight, \n      directed = TRUE))) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(\n    aes(\n      width=Weight), \n    alpha=0.2) +\n  scale_edge_width(\n    range = c(0.1, 5)) +\n  geom_node_point(\n    aes(colour = community))  \n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\nIn order to support effective visual investigation, the community network above has been revised by using geom_mark_hull() of ggforce package.\n\n\n\n\n\n\nImportant\n\n\n\n\nPlease be reminded that you must to install and include ggforce and concaveman packages before running the code chunk below.\n\n\n\n\n\nShow the code\n\n\ng &lt;- GAStech_graph %&gt;%\n  activate(nodes) %&gt;%\n  mutate(community = as.factor(\n    group_optimal(weights = Weight)),\n         betweenness_measure = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") +\n  geom_mark_hull(\n    aes(x, y, \n        group = community, \n        fill = community),  \n    alpha = 0.2,  \n    expand = unit(0.3, \"cm\"),  # Expand\n    radius = unit(0.3, \"cm\")  # Smoothness\n  ) + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(fill = Department,\n                      size = betweenness_measure),\n                      color = \"black\",\n                      shape = 21)\n  \ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvisNetwork() is a R package for network visualization, using vis.js javascript library.\nvisNetwork() function uses a nodes list and edges list to create an interactive graph.\n\nThe nodes list must include an “id” column, and the edge list must have “from” and “to” columns.\nThe function also plots the labels for the nodes, using the names of the actors from the “label” column in the node list.\n\nThe resulting graph is fun to play around with.\n\nwe can move the nodes and the graph will use an algorithm to keep the nodes properly spaced.\nwe can also zoom in and out on the plot and move it around to re-center it.\n\n\n\n\nBefore we can plot the interactive network graph, we need to prepare the data model by using the code chunk below.\n\n\nShow the code\n\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %&gt;%\n  rename(from = id) %&gt;%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %&gt;%\n  rename(to = id) %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(from, to) %&gt;%\n    summarise(weight = n()) %&gt;%\n  filter(from!=to) %&gt;%\n  filter(weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n\n\nThe code chunk below will be used to plot an interactive network graph by using the data prepared.\n\n\nShow the code\n\n\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n\n\n\n\n\n\nIn the code chunk below, Fruchterman and Reingold layout is used.\n\n\nShow the code\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") \n\n\n\n\n\n\nVisit Igraph to find out more about visIgraphLayout’s argument.\n\n\n\nvisNetwork() looks for a field called “group” in the nodes object and colour the nodes according to the values of the group field.\nThe code chunk below rename Department field to group.\n\n\nShow the code\n\n\nGAStech_nodes &lt;- GAStech_nodes %&gt;%\n  rename(group = Department) \n\n\nWhen we rerun the code chunk below, visNetwork shades the nodes by assigning unique colour to each category in the group field.\n\n\nShow the code\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n\nIn the code run below visEdges() is used to symbolise the edges.\n- The argument arrows is used to define where to place the arrow.\n- The smooth argument is used to plot the edges using a smooth curve.\n\n\nShow the code\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\nVisit Option to find out more about visEdges’s argument.\n\n\n\nIn the code chunk below, visOptions() is used to incorporate interactivity features in the data visualisation.\n\nThe argument highlightNearest highlights nearest when clicking a node.\nThe argument nodesIdSelection adds an id node selection creating an HTML select element.\n\n\n\nShow the code\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\nVisit Option to find out more about visOption’s argument.\n\n\n\n\n\n\n\n\nVisual Analysis of Complex Networks for Business Intelligence with Gephi\nGraph Drawing\nGraph Analytics - Lesson Learned and Challenges Ahead\nLearning to Read and Interpret Network Graph Data Visualizations\nThe Visualization of Networks\nViZster: Visualizing Online Social Networks\nAdam Perer. “Finding Beautiful Insights in the Chaos of Social Network Visualizations”. In ’‘’Beautiful Visualization’’’. O’Reilly Press.\nVisual Complexity"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#overview",
    "title": "Hands-on Exercise 05",
    "section": "",
    "text": "In this hands-on exercise, we will learn how to model, analyse and visualise network data using R,\nto be able to:\n\ncreate graph object data frames, manipulate them using appropriate functions of dplyr, lubridate, and tidygraph,\nbuild network graph visualisation using appropriate functions of ggraph,\ncompute network geometrics using tidygraph,\nbuild advanced graph visualisation by incorporating the network geometrics, and\nbuild interactive network visualisation using visNetwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#getting-started",
    "title": "Hands-on Exercise 05",
    "section": "",
    "text": "In this hands-on exercise, four network data modelling and visualisation packages will be installed and launched. They are igraph, tidygraph, ggraph and visNetwork. Beside these four packages, tidyverse and lubridate, an R package specially designed to handle and wrangling time data will be installed and launched too.\nThe code chunk:\n\n\nShow the code\n\n\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts, \n               concaveman, ggforce)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#the-data",
    "title": "Hands-on Exercise 05",
    "section": "",
    "text": "The data sets used in this hands-on exercise is from an oil exploration and extraction company. There are two data sets. One contains the nodes data and the other contains the edges (also know as link) data.\n\n\n\nGAStech-email_edges.csv which consists of two weeks of 9063 emails correspondances between 55 employees.\n\n\n\n\n\n\nGAStech_email_nodes.csv which consist of the names, department and title of the 55 employees.\n\n\n\n\n\nIn this step, we will import GAStech_email_node.csv and GAStech_email_edges-v2.csv into RStudio environment by using read_csv() of readr package.\n\n\nShow the code\n\n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\n\n\n\n\nNext, we will examine the structure of the data frame using glimpse() of dplyr.\n\n\nShow the code\n\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe output report of GAStech_edges above reveals that the SentDate is treated as “Character” data type instead of date data type. This is an error! Before we continue, it is important for us to change the data type of SentDate field back to “Date”” data type.\n\n\n\n\n\nThe code chunk below will be used to perform the changes.\n\n\nShow the code\n\n\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nboth dmy() and wday() are functions of lubridate package. lubridate is an R package that makes it easier to work with dates and times.\ndmy() transforms the SentDate to Date data type.\nwday() returns the day of the week as a decimal number or an ordered factor if label is TRUE. The argument abbr is FALSE keep the daya spells in full, i.e. Monday. The function will create a new column in the data.frame i.e. Weekday and the output of wday() will save in this newly created field.\nthe values in the Weekday field are in ordinal scale.\n\n\n\n\n\n\nTable below shows the data structure of the reformatted GAStech_edges data frame\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 10\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n$ SendDate    &lt;date&gt; 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-0…\n$ Weekday     &lt;ord&gt; Friday, Friday, Friday, Friday, Friday, Friday, Friday, Fr…\n\n\n\n\n\nA close examination of GAStech_edges data.frame reveals that it consists of individual e-mail flow records. This is not very useful for visualisation.\nIn view of this, we will aggregate the individual by date, senders, receivers, main subject and day of the week.\nThe code chunk:\n\n\nShow the code\n\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nfour functions from dplyr package are used. They are: filter(), group(), summarise(), and ungroup().\nThe output data.frame is called GAStech_edges_aggregated.\nA new field called Weight has been added in GAStech_edges_aggregated.\n\n\n\n\n\n\nTable below shows the data structure of the reformatted GAStech_edges_aggregated data frame\n\nglimpse(GAStech_edges_aggregated)\n\nRows: 1,372\nColumns: 4\n$ source  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ target  &lt;dbl&gt; 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6,…\n$ Weekday &lt;ord&gt; Sunday, Monday, Tuesday, Wednesday, Friday, Sunday, Monday, Tu…\n$ Weight  &lt;int&gt; 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5,…"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#creating-network-objects-using-tidygraph",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#creating-network-objects-using-tidygraph",
    "title": "Hands-on Exercise 05",
    "section": "",
    "text": "In this section, you will learn how to create a graph data model by using tidygraph package. It provides a tidy API for graph/network manipulation. While network data itself is not tidy, it can be envisioned as two tidy tables, one for node data and one for edge data. tidygraph provides a way to switch between the two tables and provides dplyr verbs for manipulating them. Furthermore it provides access to a lot of graph algorithms with return values that facilitate their use in a tidy workflow.\nBefore getting started, you are advised to read these two articles:\n\nIntroducing tidygraph\ntidygraph 1.1 - A tidy hope\n\n\n\nTwo functions of tidygraph package can be used to create network objects, they are:\n\ntbl_graph() creates a tbl_graph network object from nodes and edges data.\nas_tbl_graph() converts network data and objects to a tbl_graph network. Below are network data and objects supported by as_tbl_graph()\n\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor).\n\n\n\n\n\n\nactivate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\n\n\n\nIn the above the .N() function is used to gain access to the node data while manipulating the edge data. Similarly .E() will give you the edge data and .G() will give you the tbl_graph object itself.\n\n\n\n\nIn this section, we will use tbl_graph() of tinygraph package to build an tidygraph’s network graph data.frame.\nBefore typing the codes, it is recommended to review to reference guide of tbl_graph()\n\n\nShow the code\n\n\nGAStech_graph &lt;- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\n\n\n\n\n\nShow the code\n\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 × 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Mana…\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# ℹ 44 more rows\n#\n# Edge Data: 1,372 × 4\n   from    to Weekday Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\n\n\n\n\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 4541 edges.\nThe command also prints the first six rows of “Node Data” and the first three of “Edge Data”.\nIt states that the Node Data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.\n\n\n\n\nThe nodes tibble data frame is activated by default, but you can change which tibble data frame is active with the activate() function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest “weight” first, we could use activate() and then arrange().\nFor example,\n\n\nShow the code\n\n\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(Weight))\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 1,372 × 4 (active)\n    from    to Weekday   Weight\n   &lt;int&gt; &lt;int&gt; &lt;ord&gt;      &lt;int&gt;\n 1    40    41 Saturday      13\n 2    41    43 Monday        11\n 3    35    31 Tuesday       10\n 4    40    41 Monday        10\n 5    40    43 Monday        10\n 6    36    32 Sunday         9\n 7    40    43 Saturday       9\n 8    41    40 Monday         9\n 9    19    15 Wednesday      8\n10    35    38 Tuesday        8\n# ℹ 1,362 more rows\n#\n# Node Data: 54 × 4\n     id label           Department     Title           \n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows\n\n\n\nVisit the reference guide of activate() to find out more about the function."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#plotting-static-network-graphs-with-ggraph-package",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#plotting-static-network-graphs-with-ggraph-package",
    "title": "Hands-on Exercise 05",
    "section": "",
    "text": "ggraph is an extension of ggplot2, making it easier to carry over basic ggplot skills to the design of network graphs.\nAs in all network graph, there are three main aspects to a ggraph’s network graph, they are:\n\nnodes,\nedges and\nlayouts.\n\nFor a comprehensive discussion of each of this aspect of graph, please refer to their respective vignettes provided.\n\n\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using GAStech_graph. Before your get started, it is advisable to read their respective reference guide at least once.\n\n\nShow the code\n\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nThe basic plotting function is ggraph(), which takes the data to be used for the graph and the type of layout desired. Both of the arguments for ggraph() are built around igraph. Therefore, ggraph() can use either an igraph object or a tbl_graph object.\n\n\n\n\n\n\nIn this section, you will use theme_graph() to remove the x and y axes. Before your get started, it is advisable to read it’s reference guide at least once.\n\n\nShow the code\n\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nggraph introduces a special ggplot theme that provides better defaults for network graphs than the normal ggplot defaults. theme_graph(), besides removing axes, grids, and border, changes the font to Arial Narrow (this can be overridden).\nThe ggraph theme can be set for a series of plots with the set_graph_style() command run before the graphs are plotted or by using theme_graph() in the individual plots.\n\n\n\n\n\n\nFurthermore, theme_graph() makes it easy to change the coloring of the plot.\n\n\nShow the code\n\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n\n\n\n\n\n\nggraph support many layout for standard used, they are: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl. Figures below and on the right show layouts supported by ggraph().\n\n\n\n\nThe code chunks below will be used to plot the network graph using Fruchterman and Reingold layout.\n\n\nShow the code\n\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above:\n\nlayout argument is used to define the layout to be used.\n\n\n\n\nIn this section, we will colour each node by referring to their respective departments.\n\n\nShow the code\n\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunks above:\n\ngeom_node_point is equivalent in functionality to geo_point of ggplot2. It allows for simple plotting of nodes in different shapes, colours and sizes. In the codes chnuks above colour and size are used.\n\n\n\n\nIn the code chunk below, the thickness of the edges will be mapped with the Weight variable.\n\n\nShow the code\n\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunks above:\n\ngeom_edge_link draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. In the example above, argument width is used to map the width of the line in proportional to the Weight attribute and argument alpha is used to introduce opacity on the line."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#creating-facet-graphs",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#creating-facet-graphs",
    "title": "Hands-on Exercise 05",
    "section": "",
    "text": "Another very useful feature of ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaning way by spreading nodes and edges out based on their attributes. In this section, we will learn how to use faceting technique to visualise network data.\nThere are three functions in ggraph to implement faceting, they are:\n\nfacet_nodes() whereby edges are only draw in a panel if both terminal nodes are present here,\nfacet_edges() whereby nodes are always drawn in al panels even if the node data contains an attribute named the same as the one used for the edge facetting, and\nfacet_graph() faceting on two variables simultaneously.\n\n\n\nIn the code chunk below, facet_edges() is used. Before getting started, it is advisable for us to read it’s reference guide at least once.\n\n\nShow the code\n\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n\n\n\n\n\nThe code chunk below uses theme() to change the position of the legend.\n\n\nShow the code\n\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n\n\n\n\n\nThe code chunk below adds frame to each graph.\n\n\nShow the code\n\n\nset_graph_style() \n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\n\n\n\n\nIn the code chunkc below, facet_nodes() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\n\nShow the code\n\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#network-metrics-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#network-metrics-analysis",
    "title": "Hands-on Exercise 05",
    "section": "",
    "text": "Centrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures, namely: degree, betweenness, closeness and eigenvector. It is beyond the scope of this hands-on exercise to cover the principles and mathematics of these measure here. Students are encouraged to refer to Chapter 7: Actor Prominence of A User’s Guide to Network Analysis in R to gain better understanding of theses network measures.\n\n\nShow the code\n\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\nmutate() of dplyr is used to perform the computation.\nthe algorithm used, on the other hand, is the centrality_betweenness() of tidygraph.\n\n\n\n\nIt is important to note that from ggraph v2.0 onward tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\n\n\nShow the code\n\n\ng &lt;- GAStech_graph %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\n\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. Use this link to find out more about community detection functions provided by tidygraph,\nIn the code chunk below group_edge_betweenness() is used.\n\n\nShow the code\n\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(community = as.factor(\n    group_edge_betweenness(\n      weights = Weight, \n      directed = TRUE))) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(\n    aes(\n      width=Weight), \n    alpha=0.2) +\n  scale_edge_width(\n    range = c(0.1, 5)) +\n  geom_node_point(\n    aes(colour = community))  \n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\nIn order to support effective visual investigation, the community network above has been revised by using geom_mark_hull() of ggforce package.\n\n\n\n\n\n\nImportant\n\n\n\n\nPlease be reminded that you must to install and include ggforce and concaveman packages before running the code chunk below.\n\n\n\n\n\nShow the code\n\n\ng &lt;- GAStech_graph %&gt;%\n  activate(nodes) %&gt;%\n  mutate(community = as.factor(\n    group_optimal(weights = Weight)),\n         betweenness_measure = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") +\n  geom_mark_hull(\n    aes(x, y, \n        group = community, \n        fill = community),  \n    alpha = 0.2,  \n    expand = unit(0.3, \"cm\"),  # Expand\n    radius = unit(0.3, \"cm\")  # Smoothness\n  ) + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(fill = Department,\n                      size = betweenness_measure),\n                      color = \"black\",\n                      shape = 21)\n  \ng + theme_graph()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#building-interactive-network-graph-with-visnetwork",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#building-interactive-network-graph-with-visnetwork",
    "title": "Hands-on Exercise 05",
    "section": "",
    "text": "visNetwork() is a R package for network visualization, using vis.js javascript library.\nvisNetwork() function uses a nodes list and edges list to create an interactive graph.\n\nThe nodes list must include an “id” column, and the edge list must have “from” and “to” columns.\nThe function also plots the labels for the nodes, using the names of the actors from the “label” column in the node list.\n\nThe resulting graph is fun to play around with.\n\nwe can move the nodes and the graph will use an algorithm to keep the nodes properly spaced.\nwe can also zoom in and out on the plot and move it around to re-center it.\n\n\n\n\nBefore we can plot the interactive network graph, we need to prepare the data model by using the code chunk below.\n\n\nShow the code\n\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %&gt;%\n  rename(from = id) %&gt;%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %&gt;%\n  rename(to = id) %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(from, to) %&gt;%\n    summarise(weight = n()) %&gt;%\n  filter(from!=to) %&gt;%\n  filter(weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n\n\nThe code chunk below will be used to plot an interactive network graph by using the data prepared.\n\n\nShow the code\n\n\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n\n\n\n\n\n\nIn the code chunk below, Fruchterman and Reingold layout is used.\n\n\nShow the code\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") \n\n\n\n\n\n\nVisit Igraph to find out more about visIgraphLayout’s argument.\n\n\n\nvisNetwork() looks for a field called “group” in the nodes object and colour the nodes according to the values of the group field.\nThe code chunk below rename Department field to group.\n\n\nShow the code\n\n\nGAStech_nodes &lt;- GAStech_nodes %&gt;%\n  rename(group = Department) \n\n\nWhen we rerun the code chunk below, visNetwork shades the nodes by assigning unique colour to each category in the group field.\n\n\nShow the code\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n\nIn the code run below visEdges() is used to symbolise the edges.\n- The argument arrows is used to define where to place the arrow.\n- The smooth argument is used to plot the edges using a smooth curve.\n\n\nShow the code\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\nVisit Option to find out more about visEdges’s argument.\n\n\n\nIn the code chunk below, visOptions() is used to incorporate interactivity features in the data visualisation.\n\nThe argument highlightNearest highlights nearest when clicking a node.\nThe argument nodesIdSelection adds an id node selection creating an HTML select element.\n\n\n\nShow the code\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\nVisit Option to find out more about visOption’s argument."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#readings",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#readings",
    "title": "Hands-on Exercise 05",
    "section": "",
    "text": "Visual Analysis of Complex Networks for Business Intelligence with Gephi\nGraph Drawing\nGraph Analytics - Lesson Learned and Challenges Ahead\nLearning to Read and Interpret Network Graph Data Visualizations\nThe Visualization of Networks\nViZster: Visualizing Online Social Networks\nAdam Perer. “Finding Beautiful Insights in the Chaos of Social Network Visualizations”. In ’‘’Beautiful Visualization’’’. O’Reilly Press.\nVisual Complexity"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04C/Hands-on_Ex04C.html",
    "href": "Hands-on_Ex/Hands-on_Ex04C/Hands-on_Ex04C.html",
    "title": "Hands-on Exercise 04C",
    "section": "",
    "text": "Visualizing uncertainty is relatively new in statistical graphics. In this exercise, we will gain hands-on experience on creating statistical graphics for visualising uncertainty. By the end of this exercise we will be able:\n\nto plot statistics error bars by using ggplot2,\nto plot interactive error bars by combining ggplot2, plotly and DT,\nto create advanced by using ggdist, and\nto create hypothetical outcome plots (HOPs) by using ungeviz package.\n\n\n\n\n\n\nFor the purpose of this exercise, the following R packages will be used, they are:\n\ntidyverse, a family of R packages for data science process,\nplotly for creating interactive plot,\ngganimate for creating animation plot,\nDT for displaying interactive html table,\ncrosstalk for for implementing cross-widget interactions (currently, linked brushing and filtering), and\nggdist for visualising distribution and uncertainty.\n\n\npacman::p_load(plotly, crosstalk, DT, \n               ggdist, ggridges, colorspace,\n               gganimate, tidyverse)\n\n\n\n\nFor the purpose of this exercise, Exam_data.csv will be used.\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\n\nA point estimate is a single number, such as a mean. Uncertainty, on the other hand, is expressed as standard error, confidence interval, or credible interval.\n\n\n\n\n\n\nImportant\n\n\n\n\nDon’t confuse the uncertainty of a point estimate with the variation in the sample\n\n\n\nIn this section, we will learn how to plot error bars of maths scores by race by using data provided in exam tibble data frame.\nFirstly, code chunk below will be used to derive the necessary summary statistics.\n\nmy_sum &lt;- exam %&gt;%\n  group_by(RACE) %&gt;%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %&gt;%\n  mutate(se=sd/sqrt(n-1))\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\ngroup_by() of dplyr package is used to group the observation by RACE,\nsummarise() is used to compute the count of observations, mean, standard deviation\nmutate() is used to derive standard error of Maths by RACE, and\nthe output is save as a tibble data table called my_sum.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nFor the mathematical explanation, please refer to Slide 20 of Lesson 4.\n\n\n\nNext, the code chunk below will be used to display my_sum tibble data frame in an html table format.\n\nThe code chunkThe Table\n\n\n\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n\nRACE\nn\nmean\nsd\nse\n\n\n\n\nChinese\n193\n76.50777\n15.69040\n1.132357\n\n\nIndian\n12\n60.66667\n23.35237\n7.041005\n\n\nMalay\n108\n57.44444\n21.13478\n2.043177\n\n\nOthers\n9\n69.66667\n10.72381\n3.791438\n\n\n\n\n\n\n\n\n\n\n\n\nNow we are ready to plot the standard error bars of mean maths score by race as shown below.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    linewidth=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean maths score by rac\")\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nThe error bars are computed by using the formula mean+/-se.\nFor geom_point(), it is important to indicate stat=“identity”.\n\n\n\n\n\n\n\n\n\nInstead of plotting the standard error bar of point estimates, we can also plot the confidence intervals of mean maths score by race.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=reorder(RACE, -mean), \n        ymin=mean-1.96*se, \n        ymax=mean+1.96*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    linewidth=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  labs(x = \"Maths score\",\n       title = \"95% confidence interval of mean maths score by race\")\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nThe confidence intervals are computed by using the formula mean+/-1.96*se.\nThe error bars is sorted by using the average maths scores.\nlabs() argument of ggplot2 is used to change the x-axis label.\n\n\n\n\n\n\n\n\n\nIn this section, you will learn how to plot interactive error bars for the 99% confidence interval of mean maths score by race as shown in the figure below.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshared_df = SharedData$new(my_sum)\n\nbscols(widths = c(4,8),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(RACE, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=RACE, \n                     y=mean, \n                     text = paste(\"Race:\", `RACE`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Scores:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Race\") + \n                   ylab(\"Average Scores\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence interval of average /&lt;br&gt;maths scores by race\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 10,\n                                    scrollX=T), \n                     colnames = c(\"No. of pupils\", \n                                  \"Avg Scores\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))\n\n\n\n\n\n\n\n\n\nggdist is an R package that provides a flexible set of ggplot2 geoms and stats designed especially for visualising distributions and uncertainty.\nIt is designed for both frequentist and Bayesian uncertainty visualization, taking the view that uncertainty visualization can be unified through the perspective of distribution visualization:\n\nfor frequentist models, one visualises confidence distributions or bootstrap distributions (see vignette(“freq-uncertainty-vis”));\nfor Bayesian models, one visualises probability distributions (see the tidybayes package, which builds on top of ggdist).\n\n\n\n\n\nIn the code chunk below, stat_pointinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\nFor example, in the code chunk below the following arguments are used:\n.width = 0.95 .point = median .interval = qi\n\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising confidence intervals of median math score\",\n    subtitle = \"Median Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\n\n\n\nPlot below shows 95% and 99% Confidence Interval\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(\n    .width = 0.95,\n    .point = \"median\",\n    .interval = \"quantile\",\n    aes(colour = \"95% CI\")) +\n  stat_pointinterval(\n    .width = 0.99,\n    .point = \"median\",\n    .interval = \"quantile\",\n    aes(colour = \"99% CI\")) +\n  scale_colour_manual(\n    values = c(\"95% CI\" = \"green\", \"99% CI\" = \"red\"),\n    labels = c(\"95% CI\", \"99% CI\")) +\n  labs(\n    title = \"Visualising confidence intervals of median math score\",\n    subtitle = \"Median Point + Multiple-interval plot\") +\n  theme_minimal()\n\n\n\n\n\n\n\nIn the code chunk below, stat_gradientinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndevtools::install_github(\"wilkelab/ungeviz\")\n\nNote: We only need to perform this step once.\n\n\n\n\nlibrary(ungeviz)\n\n\n\n\nNext, the code chunk below will be used to build the HOPs.\n\nggplot(data = exam, \n       (aes(x = factor(RACE), \n            y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, \n    width = 0.05), \n    size = 0.4, \n    color = \"#0072B2\", \n    alpha = 1/2) +\n  geom_hpline(data = sampler(25, \n                             group = RACE), \n              height = 0.6, \n              color = \"#D55E00\") +\n  theme_bw() + \n  transition_states(.draw, 1, 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04C/Hands-on_Ex04C.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex04C/Hands-on_Ex04C.html#learning-outcome",
    "title": "Hands-on Exercise 04C",
    "section": "",
    "text": "Visualizing uncertainty is relatively new in statistical graphics. In this exercise, we will gain hands-on experience on creating statistical graphics for visualising uncertainty. By the end of this exercise we will be able:\n\nto plot statistics error bars by using ggplot2,\nto plot interactive error bars by combining ggplot2, plotly and DT,\nto create advanced by using ggdist, and\nto create hypothetical outcome plots (HOPs) by using ungeviz package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04C/Hands-on_Ex04C.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex04C/Hands-on_Ex04C.html#getting-started",
    "title": "Hands-on Exercise 04C",
    "section": "",
    "text": "For the purpose of this exercise, the following R packages will be used, they are:\n\ntidyverse, a family of R packages for data science process,\nplotly for creating interactive plot,\ngganimate for creating animation plot,\nDT for displaying interactive html table,\ncrosstalk for for implementing cross-widget interactions (currently, linked brushing and filtering), and\nggdist for visualising distribution and uncertainty.\n\n\npacman::p_load(plotly, crosstalk, DT, \n               ggdist, ggridges, colorspace,\n               gganimate, tidyverse)\n\n\n\n\nFor the purpose of this exercise, Exam_data.csv will be used.\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04C/Hands-on_Ex04C.html#visualizing-the-uncertainty-of-point-estimates-ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04C/Hands-on_Ex04C.html#visualizing-the-uncertainty-of-point-estimates-ggplot2-methods",
    "title": "Hands-on Exercise 04C",
    "section": "",
    "text": "A point estimate is a single number, such as a mean. Uncertainty, on the other hand, is expressed as standard error, confidence interval, or credible interval.\n\n\n\n\n\n\nImportant\n\n\n\n\nDon’t confuse the uncertainty of a point estimate with the variation in the sample\n\n\n\nIn this section, we will learn how to plot error bars of maths scores by race by using data provided in exam tibble data frame.\nFirstly, code chunk below will be used to derive the necessary summary statistics.\n\nmy_sum &lt;- exam %&gt;%\n  group_by(RACE) %&gt;%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %&gt;%\n  mutate(se=sd/sqrt(n-1))\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\ngroup_by() of dplyr package is used to group the observation by RACE,\nsummarise() is used to compute the count of observations, mean, standard deviation\nmutate() is used to derive standard error of Maths by RACE, and\nthe output is save as a tibble data table called my_sum.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nFor the mathematical explanation, please refer to Slide 20 of Lesson 4.\n\n\n\nNext, the code chunk below will be used to display my_sum tibble data frame in an html table format.\n\nThe code chunkThe Table\n\n\n\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n\nRACE\nn\nmean\nsd\nse\n\n\n\n\nChinese\n193\n76.50777\n15.69040\n1.132357\n\n\nIndian\n12\n60.66667\n23.35237\n7.041005\n\n\nMalay\n108\n57.44444\n21.13478\n2.043177\n\n\nOthers\n9\n69.66667\n10.72381\n3.791438\n\n\n\n\n\n\n\n\n\n\n\n\nNow we are ready to plot the standard error bars of mean maths score by race as shown below.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    linewidth=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean maths score by rac\")\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nThe error bars are computed by using the formula mean+/-se.\nFor geom_point(), it is important to indicate stat=“identity”.\n\n\n\n\n\n\n\n\n\nInstead of plotting the standard error bar of point estimates, we can also plot the confidence intervals of mean maths score by race.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=reorder(RACE, -mean), \n        ymin=mean-1.96*se, \n        ymax=mean+1.96*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    linewidth=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  labs(x = \"Maths score\",\n       title = \"95% confidence interval of mean maths score by race\")\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nThe confidence intervals are computed by using the formula mean+/-1.96*se.\nThe error bars is sorted by using the average maths scores.\nlabs() argument of ggplot2 is used to change the x-axis label.\n\n\n\n\n\n\n\n\n\nIn this section, you will learn how to plot interactive error bars for the 99% confidence interval of mean maths score by race as shown in the figure below.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshared_df = SharedData$new(my_sum)\n\nbscols(widths = c(4,8),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(RACE, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=RACE, \n                     y=mean, \n                     text = paste(\"Race:\", `RACE`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Scores:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Race\") + \n                   ylab(\"Average Scores\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence interval of average /&lt;br&gt;maths scores by race\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 10,\n                                    scrollX=T), \n                     colnames = c(\"No. of pupils\", \n                                  \"Avg Scores\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04C/Hands-on_Ex04C.html#visualizing-uncertainty-ggdist-package",
    "href": "Hands-on_Ex/Hands-on_Ex04C/Hands-on_Ex04C.html#visualizing-uncertainty-ggdist-package",
    "title": "Hands-on Exercise 04C",
    "section": "",
    "text": "ggdist is an R package that provides a flexible set of ggplot2 geoms and stats designed especially for visualising distributions and uncertainty.\nIt is designed for both frequentist and Bayesian uncertainty visualization, taking the view that uncertainty visualization can be unified through the perspective of distribution visualization:\n\nfor frequentist models, one visualises confidence distributions or bootstrap distributions (see vignette(“freq-uncertainty-vis”));\nfor Bayesian models, one visualises probability distributions (see the tidybayes package, which builds on top of ggdist).\n\n\n\n\n\nIn the code chunk below, stat_pointinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\nFor example, in the code chunk below the following arguments are used:\n.width = 0.95 .point = median .interval = qi\n\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising confidence intervals of median math score\",\n    subtitle = \"Median Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\n\n\n\nPlot below shows 95% and 99% Confidence Interval\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(\n    .width = 0.95,\n    .point = \"median\",\n    .interval = \"quantile\",\n    aes(colour = \"95% CI\")) +\n  stat_pointinterval(\n    .width = 0.99,\n    .point = \"median\",\n    .interval = \"quantile\",\n    aes(colour = \"99% CI\")) +\n  scale_colour_manual(\n    values = c(\"95% CI\" = \"green\", \"99% CI\" = \"red\"),\n    labels = c(\"95% CI\", \"99% CI\")) +\n  labs(\n    title = \"Visualising confidence intervals of median math score\",\n    subtitle = \"Median Point + Multiple-interval plot\") +\n  theme_minimal()\n\n\n\n\n\n\n\nIn the code chunk below, stat_gradientinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04C/Hands-on_Ex04C.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "href": "Hands-on_Ex/Hands-on_Ex04C/Hands-on_Ex04C.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "title": "Hands-on Exercise 04C",
    "section": "",
    "text": "devtools::install_github(\"wilkelab/ungeviz\")\n\nNote: We only need to perform this step once.\n\n\n\n\nlibrary(ungeviz)\n\n\n\n\nNext, the code chunk below will be used to build the HOPs.\n\nggplot(data = exam, \n       (aes(x = factor(RACE), \n            y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, \n    width = 0.05), \n    size = 0.4, \n    color = \"#0072B2\", \n    alpha = 1/2) +\n  geom_hpline(data = sampler(25, \n                             group = RACE), \n              height = 0.6, \n              color = \"#D55E00\") +\n  theme_bw() + \n  transition_states(.draw, 1, 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04A/Hands-on_Ex04A.html",
    "href": "Hands-on_Ex/Hands-on_Ex04A/Hands-on_Ex04A.html",
    "title": "Hands-on Exercise 04A",
    "section": "",
    "text": "Visualizing distribution is not new in statistical analysis. In chapter 1 we have shared with you some of the popular statistical graphics methods for visualizing distribution are histogram, probability density curve (pdf), boxplot, notch plot and violin plot and how they can be created by using ggplot2. In this chapter, we are going to share with you two relatively new statistical graphic methods for visualizing distribution, namely ridgeline plot and raincloud plot by using ggplot2 and its extensions.\n\n\n\n\n\nFor the purpose of this exercise, the following R packages will be used, they are:\n\nggridges, a ggplot2 extension specially designed for plotting ridgeline plots,\nggdist, a ggplot2 extension spacially desgin for visualizing distribution and uncertainty,\ntidyverse, a family of R packages to meet the modern data science and visual communication needs,\nggthemes, a ggplot extension that provides the user additional themes, scales, and geoms for the ggplots package, and\ncolorspace, an R package provides a broad toolbox for selecting individual colors or color palettes, manipulating these colors, and employing them in various kinds of visualizations.\n\nThe code chunk below will be used load these R packages into RStudio environment.\n\npacman::p_load(ggdist, ggridges, ggthemes,\n               colorspace, tidyverse)\n\n\n\n\nFor the purpose of this exercise, Exam_data.csv will be used.\nIn the code chunk below, read_csv() of readr package is used to import Exam_data.csv into R and saved it into a tibble data.frame.\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\n\nRidgeline plot (sometimes called Joyplot) is a data visualization technique for revealing the distribution of a numeric value for several groups. Distribution can be represented using histograms or density plots, all aligned to the same horizontal scale and presented with a slight overlap.\nFigure below is a ridgelines plot showing the distribution of English score by class.\n\n# Load libraries\nlibrary(ggplot2)\nlibrary(ggridges)\n\n# Plot\nggplot(exam, aes(x = ENGLISH, y = CLASS)) +\n  geom_density_ridges(\n    scale = 1.2,\n    rel_min_height = 0.01,\n    fill = \"gray\",\n    color = \"black\"\n  ) +\n  scale_x_continuous(\n    name = \"ENGLISH\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(\n    name = \"CLASS\",\n    expand = expansion(add = c(0.2, 0.5))\n  ) +\n  theme_minimal(base_size = 14) +  # Clean base theme\n  theme(\n    panel.grid.major.x = element_line(color =\"whitesmoke\"),  # vertical lines\n    panel.grid.major.y = element_line(color = \"whitesmoke\"),  # horizontal lines\n    panel.grid.minor = element_blank(),                   # no minor gridlines\n    panel.background = element_rect(fill = \"white\", color = NA),  # white background\n    plot.background = element_rect(fill = \"white\", color = NA)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nRidgeline plots make sense when the number of groups to represent is medium to high. A classic window separation would take up too much space. The overlapping design of ridgelines makes better use of space. If you have fewer than 5 groups, other distribution plots may be more effective.\nRidgeline plots work well when there’s a clear pattern or ranking among groups. Otherwise, excessive overlap may lead to a messy plot with little insight.\n\n\n\n\n\nThere are several ways to plot ridgeline plot with R. In this section, you will learn how to plot ridgeline plot by using ggridges package.\nggridges package provides two main geom to plot gridgeline plots, they are: geom_ridgeline() and geom_density_ridges(). The former takes height values directly to draw the ridgelines, and the latter first estimates data densities and then draws those using ridgelines.\nThe ridgeline plot below is plotted by using geom_density_ridges().\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\", .3),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\nSometimes we would like to have the area under a ridgeline not filled with a single solid color but rather with colors that vary in some form along the x axis. This effect can be achieved by using either geom_ridgeline_gradient() or geom_density_ridges_gradient(). Both geoms work just like geom_ridgeline() and geom_density_ridges(), except that they allow for varying fill colors. However, they do not allow for alpha transparency in the fill. For technical reasons, we can have changing fill colors or transparency but not both.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Temp. [F]\",\n                       option = \"C\") +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\nBeside providing additional geom objects to support the need to plot ridgeline plot, ggridges package also provides a stat function called stat_density_ridges() that replaces stat_density() of ggplot2.\nFigure below is plotted by mapping the probabilities calculated by using stat(ecdf) which represent the empirical cumulative density function for the distribution of English score.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is important to include the argument calc_ecdf = TRUE in stat_density_ridges().\n\n\n\n\n\nBy using geom_density_ridges_gradient(), we can colour the ridgeline plot by quantile, via the calculated stat(quantile) aesthetic as shown in the figure below.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\nInstead of using number to define the quantiles, we can also specify quantiles by cut points such as 2.5% and 97.5% tails to colour the ridgeline plot as shown in the figure below.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.025, 0.975)\n    ) +\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  ) +\n  theme_ridges()\n\n\n\n\n\n\n\n\nRaincloud Plot is a data visualization techniques that produces a half-density to a distribution plot. It gets the name because the density plot is in the shape of a “raincloud”. The raincloud (half-density) plot enhances the traditional box-plot by highlighting multiple modalities (an indicator that groups may exist). The boxplot does not show where densities are clustered, but the raincloud plot does!\nIn this section, you will learn how to create a raincloud plot to visualize the distribution of English score by race. It will be created by using functions provided by ggdist and ggplot2 packages.\n\n\nFirst, we will plot a Half-Eye graph by using stat_halfeye() of ggdist package.\nThis produces a Half Eye visualization, which is contains a half-density and a slab-interval.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA)\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\nWe remove the slab interval by setting .width = 0 and point_colour = NA.\n\n\n\n\n\n\n\n\nNext, we will add the second geometry layer using geom_boxplot() of ggplot2. This produces a narrow boxplot. We reduce the width and adjust the opacity.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA)\n\n\n\n\n\n\n\nNext, we will add the third geometry layer using stat_dots() of ggdist package. This produces a half-dotplot, which is similar to a histogram that indicates the number of samples (number of dots) in each bin. We select side = “left” to indicate we want it on the left-hand side.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2)\n\n\n\n\n\n\n\nLastly, coord_flip() of ggplot2 package will be used to flip the raincloud chart horizontally to give it the raincloud appearance. At the same time, theme_economist() of ggthemes package is used to give the raincloud chart a professional publishing standard look.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 1.5) +\n  coord_flip() +\n  theme_economist()\n\n\n\n\n\n\n\n\n\nIntroducing Ridgeline Plots (formerly Joyplots)\nClaus O. Wilke Fundamentals of Data Visualization especially Chapter 6, 7, 8, 9 and 10.\nAllen M, Poggiali D, Whitaker K et al. “Raincloud plots: a multi-platform tool for robust data. visualization” [version 2; peer review: 2 approved]. Welcome Open Res 2021, pp. 4:63.\nDots + interval stats and geoms"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04A/Hands-on_Ex04A.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex04A/Hands-on_Ex04A.html#learning-outcome",
    "title": "Hands-on Exercise 04A",
    "section": "",
    "text": "Visualizing distribution is not new in statistical analysis. In chapter 1 we have shared with you some of the popular statistical graphics methods for visualizing distribution are histogram, probability density curve (pdf), boxplot, notch plot and violin plot and how they can be created by using ggplot2. In this chapter, we are going to share with you two relatively new statistical graphic methods for visualizing distribution, namely ridgeline plot and raincloud plot by using ggplot2 and its extensions."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04A/Hands-on_Ex04A.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex04A/Hands-on_Ex04A.html#getting-started",
    "title": "Hands-on Exercise 04A",
    "section": "",
    "text": "For the purpose of this exercise, the following R packages will be used, they are:\n\nggridges, a ggplot2 extension specially designed for plotting ridgeline plots,\nggdist, a ggplot2 extension spacially desgin for visualizing distribution and uncertainty,\ntidyverse, a family of R packages to meet the modern data science and visual communication needs,\nggthemes, a ggplot extension that provides the user additional themes, scales, and geoms for the ggplots package, and\ncolorspace, an R package provides a broad toolbox for selecting individual colors or color palettes, manipulating these colors, and employing them in various kinds of visualizations.\n\nThe code chunk below will be used load these R packages into RStudio environment.\n\npacman::p_load(ggdist, ggridges, ggthemes,\n               colorspace, tidyverse)\n\n\n\n\nFor the purpose of this exercise, Exam_data.csv will be used.\nIn the code chunk below, read_csv() of readr package is used to import Exam_data.csv into R and saved it into a tibble data.frame.\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04A/Hands-on_Ex04A.html#visualizing-distribution-with-ridgeline-plot",
    "href": "Hands-on_Ex/Hands-on_Ex04A/Hands-on_Ex04A.html#visualizing-distribution-with-ridgeline-plot",
    "title": "Hands-on Exercise 04A",
    "section": "",
    "text": "Ridgeline plot (sometimes called Joyplot) is a data visualization technique for revealing the distribution of a numeric value for several groups. Distribution can be represented using histograms or density plots, all aligned to the same horizontal scale and presented with a slight overlap.\nFigure below is a ridgelines plot showing the distribution of English score by class.\n\n# Load libraries\nlibrary(ggplot2)\nlibrary(ggridges)\n\n# Plot\nggplot(exam, aes(x = ENGLISH, y = CLASS)) +\n  geom_density_ridges(\n    scale = 1.2,\n    rel_min_height = 0.01,\n    fill = \"gray\",\n    color = \"black\"\n  ) +\n  scale_x_continuous(\n    name = \"ENGLISH\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(\n    name = \"CLASS\",\n    expand = expansion(add = c(0.2, 0.5))\n  ) +\n  theme_minimal(base_size = 14) +  # Clean base theme\n  theme(\n    panel.grid.major.x = element_line(color =\"whitesmoke\"),  # vertical lines\n    panel.grid.major.y = element_line(color = \"whitesmoke\"),  # horizontal lines\n    panel.grid.minor = element_blank(),                   # no minor gridlines\n    panel.background = element_rect(fill = \"white\", color = NA),  # white background\n    plot.background = element_rect(fill = \"white\", color = NA)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nRidgeline plots make sense when the number of groups to represent is medium to high. A classic window separation would take up too much space. The overlapping design of ridgelines makes better use of space. If you have fewer than 5 groups, other distribution plots may be more effective.\nRidgeline plots work well when there’s a clear pattern or ranking among groups. Otherwise, excessive overlap may lead to a messy plot with little insight.\n\n\n\n\n\nThere are several ways to plot ridgeline plot with R. In this section, you will learn how to plot ridgeline plot by using ggridges package.\nggridges package provides two main geom to plot gridgeline plots, they are: geom_ridgeline() and geom_density_ridges(). The former takes height values directly to draw the ridgelines, and the latter first estimates data densities and then draws those using ridgelines.\nThe ridgeline plot below is plotted by using geom_density_ridges().\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\", .3),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\nSometimes we would like to have the area under a ridgeline not filled with a single solid color but rather with colors that vary in some form along the x axis. This effect can be achieved by using either geom_ridgeline_gradient() or geom_density_ridges_gradient(). Both geoms work just like geom_ridgeline() and geom_density_ridges(), except that they allow for varying fill colors. However, they do not allow for alpha transparency in the fill. For technical reasons, we can have changing fill colors or transparency but not both.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Temp. [F]\",\n                       option = \"C\") +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\nBeside providing additional geom objects to support the need to plot ridgeline plot, ggridges package also provides a stat function called stat_density_ridges() that replaces stat_density() of ggplot2.\nFigure below is plotted by mapping the probabilities calculated by using stat(ecdf) which represent the empirical cumulative density function for the distribution of English score.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is important to include the argument calc_ecdf = TRUE in stat_density_ridges().\n\n\n\n\n\nBy using geom_density_ridges_gradient(), we can colour the ridgeline plot by quantile, via the calculated stat(quantile) aesthetic as shown in the figure below.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\nInstead of using number to define the quantiles, we can also specify quantiles by cut points such as 2.5% and 97.5% tails to colour the ridgeline plot as shown in the figure below.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.025, 0.975)\n    ) +\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  ) +\n  theme_ridges()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04A/Hands-on_Ex04A.html#visualizing-distribution-with-raincloud-plot",
    "href": "Hands-on_Ex/Hands-on_Ex04A/Hands-on_Ex04A.html#visualizing-distribution-with-raincloud-plot",
    "title": "Hands-on Exercise 04A",
    "section": "",
    "text": "Raincloud Plot is a data visualization techniques that produces a half-density to a distribution plot. It gets the name because the density plot is in the shape of a “raincloud”. The raincloud (half-density) plot enhances the traditional box-plot by highlighting multiple modalities (an indicator that groups may exist). The boxplot does not show where densities are clustered, but the raincloud plot does!\nIn this section, you will learn how to create a raincloud plot to visualize the distribution of English score by race. It will be created by using functions provided by ggdist and ggplot2 packages.\n\n\nFirst, we will plot a Half-Eye graph by using stat_halfeye() of ggdist package.\nThis produces a Half Eye visualization, which is contains a half-density and a slab-interval.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA)\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\nWe remove the slab interval by setting .width = 0 and point_colour = NA.\n\n\n\n\n\n\n\n\nNext, we will add the second geometry layer using geom_boxplot() of ggplot2. This produces a narrow boxplot. We reduce the width and adjust the opacity.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA)\n\n\n\n\n\n\n\nNext, we will add the third geometry layer using stat_dots() of ggdist package. This produces a half-dotplot, which is similar to a histogram that indicates the number of samples (number of dots) in each bin. We select side = “left” to indicate we want it on the left-hand side.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2)\n\n\n\n\n\n\n\nLastly, coord_flip() of ggplot2 package will be used to flip the raincloud chart horizontally to give it the raincloud appearance. At the same time, theme_economist() of ggthemes package is used to give the raincloud chart a professional publishing standard look.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 1.5) +\n  coord_flip() +\n  theme_economist()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04A/Hands-on_Ex04A.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex04A/Hands-on_Ex04A.html#references",
    "title": "Hands-on Exercise 04A",
    "section": "",
    "text": "Introducing Ridgeline Plots (formerly Joyplots)\nClaus O. Wilke Fundamentals of Data Visualization especially Chapter 6, 7, 8, 9 and 10.\nAllen M, Poggiali D, Whitaker K et al. “Raincloud plots: a multi-platform tool for robust data. visualization” [version 2; peer review: 2 approved]. Welcome Open Res 2021, pp. 4:63.\nDots + interval stats and geoms"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04B/Hands-on_Ex04B.html",
    "href": "Hands-on_Ex/Hands-on_Ex04B/Hands-on_Ex04B.html",
    "title": "Hands-on Exercise 04B",
    "section": "",
    "text": "In this hands-on exercise, we will gain hands-on experience on using:\n\nggstatsplot package to create visual graphics with rich statistical information,\nperformance package to visualise model diagnostics, and\nparameters package to visualise model parameters\n\n\n\n\nggstatsplot  is an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.\n\nTo provide alternative statistical inference methods by default.\nTo follow best practices for statistical reporting. For all statistical tests reported in the plots, the default template abides by the [APA](https://my.ilstu.edu/~jhkahn/apastats.html) gold standard for statistical reporting. For example, here are results from a robust t-test:\n\n\n\n\n\n\n\nIn this exercise, ggstatsplot and tidyverse will be used.\n\npacman::p_load(ggstatsplot, tidyverse)\n\n\n\n\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\n\n\n\nDo-It-Yourself\n\n\n\nImporting Exam.csv data by using appropriate tidyverse package.\n\n\n# A tibble: 322 × 7 ID CLASS GENDER RACE ENGLISH MATHS SCIENCE &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Student321 3I Male Malay 21 9 15 2 Student305 3I Female Malay 24 22 16 3 Student289 3H Male Chinese 26 16 16 4 Student227 3F Male Chinese 27 77 31 5 Student318 3I Male Malay 27 11 25 6 Student306 3I Female Malay 31 16 16 7 Student313 3I Male Chinese 31 21 25 8 Student316 3I Male Malay 31 18 27 9 Student312 3I Male Malay 33 19 15 10 Student297 3H Male Indian 34 49 37 # ℹ 312 more rows\n\n\n\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on English scores.\n\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\n\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nThat’s because the Bayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\nWhen we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10. It can be defined mathematically as\n\n\n\n\nThe Schwarz criterion is one of the easiest ways to calculate rough approximation of the Bayes Factor.\n\n\n\n\nA Bayes Factor can be any positive number. One of the most common interpretations is this one—first proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakers in 2013:\n\n\n\n\nIn the code chunk below, ggbetweenstats() is used to build a visual for two-sample mean test of Maths scores by gender.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = exam,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\n\nIn the code chunk below, ggbetweenstats() is used to build a visual for One-way ANOVA test on English score by race.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = exam,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n“ns” → only non-significant\n“s” → only significant\n“all” → everything\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the code chunk below, ggscatterstats() is used to build a visual for Significant Test of Correlation between Maths scores and English scores.\n\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n\n\n\n\n\nIn the code chunk below, the Maths scores is binned into a 4-class variable by using cut().\n\nexam1 &lt;- exam %&gt;% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100))\n)\n\nIn this code chunk below ggbarstats() is used to build a visual for Significant Test of Association\n\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04B/Hands-on_Ex04B.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex04B/Hands-on_Ex04B.html#learning-outcome",
    "title": "Hands-on Exercise 04B",
    "section": "",
    "text": "In this hands-on exercise, we will gain hands-on experience on using:\n\nggstatsplot package to create visual graphics with rich statistical information,\nperformance package to visualise model diagnostics, and\nparameters package to visualise model parameters"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04B/Hands-on_Ex04B.html#visual-statistical-analysis-with-ggstatsplot",
    "href": "Hands-on_Ex/Hands-on_Ex04B/Hands-on_Ex04B.html#visual-statistical-analysis-with-ggstatsplot",
    "title": "Hands-on Exercise 04B",
    "section": "",
    "text": "ggstatsplot  is an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.\n\nTo provide alternative statistical inference methods by default.\nTo follow best practices for statistical reporting. For all statistical tests reported in the plots, the default template abides by the [APA](https://my.ilstu.edu/~jhkahn/apastats.html) gold standard for statistical reporting. For example, here are results from a robust t-test:"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04B/Hands-on_Ex04B.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex04B/Hands-on_Ex04B.html#getting-started",
    "title": "Hands-on Exercise 04B",
    "section": "",
    "text": "In this exercise, ggstatsplot and tidyverse will be used.\n\npacman::p_load(ggstatsplot, tidyverse)\n\n\n\n\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\n\n\n\nDo-It-Yourself\n\n\n\nImporting Exam.csv data by using appropriate tidyverse package.\n\n\n# A tibble: 322 × 7 ID CLASS GENDER RACE ENGLISH MATHS SCIENCE &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Student321 3I Male Malay 21 9 15 2 Student305 3I Female Malay 24 22 16 3 Student289 3H Male Chinese 26 16 16 4 Student227 3F Male Chinese 27 77 31 5 Student318 3I Male Malay 27 11 25 6 Student306 3I Female Malay 31 16 16 7 Student313 3I Male Chinese 31 21 25 8 Student316 3I Male Malay 31 18 27 9 Student312 3I Male Malay 33 19 15 10 Student297 3H Male Indian 34 49 37 # ℹ 312 more rows\n\n\n\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on English scores.\n\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\n\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nThat’s because the Bayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\nWhen we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10. It can be defined mathematically as\n\n\n\n\nThe Schwarz criterion is one of the easiest ways to calculate rough approximation of the Bayes Factor.\n\n\n\n\nA Bayes Factor can be any positive number. One of the most common interpretations is this one—first proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakers in 2013:\n\n\n\n\nIn the code chunk below, ggbetweenstats() is used to build a visual for two-sample mean test of Maths scores by gender.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = exam,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\n\nIn the code chunk below, ggbetweenstats() is used to build a visual for One-way ANOVA test on English score by race.\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = exam,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n“ns” → only non-significant\n“s” → only significant\n“all” → everything\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the code chunk below, ggscatterstats() is used to build a visual for Significant Test of Correlation between Maths scores and English scores.\n\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n\n\n\n\n\nIn the code chunk below, the Maths scores is binned into a 4-class variable by using cut().\n\nexam1 &lt;- exam %&gt;% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100))\n)\n\nIn this code chunk below ggbarstats() is used to build a visual for Significant Test of Association\n\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04D/Hands-on_Ex04D.html",
    "href": "Hands-on_Ex/Hands-on_Ex04D/Hands-on_Ex04D.html",
    "title": "Hands-on Exercise 04D",
    "section": "",
    "text": "Funnel plot is a specially designed data visualisation for conducting unbiased comparison between outlets, stores or business entities. By the end of this hands-on exercise, we will gain hands-on experience on:\n\nplotting funnel plots by using funnelPlotR package,\nplotting static funnel plot by using ggplot2 package, and\nplotting interactive funnel plot by using both plotly R and ggplot2 packages.\n\n\n\n\nIn this exercise, four R packages will be used. They are:\n\nreadr for importing csv into R.\nFunnelPlotR for creating funnel plot.\nggplot2 for creating funnel plot manually.\nknitr for building static html table.\nplotly for creating interactive funnel plot.\n\n\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)\n\n\n\n\nIn this section, COVID-19_DKI_Jakarta will be used. The data was downloaded from Open Data Covid-19 Provinsi DKI Jakarta portal. For this hands-on exercise, we are going to compare the cumulative COVID-19 cases and death by sub-district (i.e. kelurahan) as at 31st July 2021, DKI Jakarta.\nThe code chunk below imports the data into R and save it into a tibble data frame object called covid19.\n\ncovid19 &lt;- read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %&gt;%\n  mutate_if(is.character, as.factor)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSub-district ID\nCity\nDistrict\nSub-district\nPositive\nRecovered\nDeath\n\n\n\n\n3172051003\nJAKARTA UTARA\nPADEMANGAN\nANCOL\n1776\n1691\n26\n\n\n3173041007\nJAKARTA BARAT\nTAMBORA\nANGKE\n1783\n1720\n29\n\n\n3175041005\nJAKARTA TIMUR\nKRAMAT JATI\nBALE KAMBANG\n2049\n1964\n31\n\n\n3175031003\nJAKARTA TIMUR\nJATINEGARA\nBALI MESTER\n827\n797\n13\n\n\n3175101006\nJAKARTA TIMUR\nCIPAYUNG\nBAMBU APUS\n2866\n2792\n27\n\n\n3174031002\nJAKARTA SELATAN\nMAMPANG PRAPATAN\nBANGKA\n1828\n1757\n26\n\n\n\n\n\n\nFunnelPlotR package uses ggplot to generate funnel plots. It requires a numerator (events of interest), denominator (population to be considered) and group. The key arguments selected for customisation are:\n\nlimit: plot limits (95 or 99).\nlabel_outliers: to label outliers (true or false).\nPoisson_limits: to add Poisson limits to the plot.\nOD_adjust: to add overdispersed limits to the plot.\nxrange and yrange: to specify the range to display for axes, acts like a zoom function.\nOther aesthetic components such as graph title, axis labels etc.\n\n\n\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Positive,\n  denominator = Death,\n  group = `Sub-district`\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\nA funnel plot object with 267 points of which 0 are outliers.  Plot is adjusted for overdispersion. \nThings to learn from the code chunk above.\n\ngroup in this function is different from the scatterplot. Here, it defines the level of the points to be plotted i.e. Sub-district, District or City. If Cityc is chosen, there are only six data points.\nBy default, data_typeargument is “SR”.\nlimit: Plot limits, accepted values are: 95 or 99, corresponding to 95% or 99.8% quantiles of the distribution.\n\n\n\n\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Death,\n  denominator = Positive,\n  group = `Sub-district`,\n  data_type = \"PR\",     #&lt;&lt;\n  xrange = c(0, 6500),  #&lt;&lt;\n  yrange = c(0, 0.05)   #&lt;&lt;\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nA funnel plot object with 267 points of which 7 are outliers.  Plot is adjusted for overdispersion. \nThings to learn from the code chunk above. + data_type argument is used to change from default “SR” to “PR” (i.e. proportions). + xrange and yrange are used to set the range of x-axis and y-axis\n\n\n\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Death,\n  denominator = Positive,\n  group = `Sub-district`,\n  data_type = \"PR\",   \n  xrange = c(0, 6500),  \n  yrange = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by Cumulative Total Number of COVID-19 Positive Cases\", #&lt;&lt;           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #&lt;&lt;\n  y_label = \"Cumulative Fatality Rate\"  #&lt;&lt;\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nA funnel plot object with 267 points of which 7 are outliers.  Plot is adjusted for overdispersion. \nThings to learn from the code chunk above.\n\nlabel = NA argument is to removed the default label outliers feature.\ntitle argument is used to add plot title.\nx_label and y_label arguments are used to add/edit x-axis and y-axis titles.\n\n\n\n\n\nIn this section, you will gain hands-on experience on building funnel plots step-by-step by using ggplot2. It aims to enhance you working experience of ggplot2 to customise speciallised data visualization like funnel plot.\n\n\nTo plot the funnel plot from scratch, we need to derive cumulative death rate and standard error of cumulative death rate.\n\ndf &lt;- covid19 %&gt;%\n  mutate(rate = Death / Positive) %&gt;%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %&gt;%\n  filter(rate &gt; 0)\n\nNext, the fit.mean is computed by using the code chunk below.\n\nfit.mean &lt;- weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\n\nThe code chunk below is used to compute the lower and upper limits for 95% confidence interval.\n\nnumber.seq &lt;- seq(1, max(df$Positive), 1)\nnumber.ll95 &lt;- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 &lt;- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 &lt;- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 &lt;- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n\n\n\nIn the code chunk below, ggplot2 functions are used to plot a static funnel plot.\n\np &lt;- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np\n\n\n\n\n\n\n\n\n\n\n\nThe funnel plot created using ggplot2 functions can be made interactive with ggplotly() of plotly r package.\n\nfp_ggplotly &lt;- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly\n\n\n\n\n\n\n\n\n\n\nfunnelPlotR package.\nFunnel Plots for Indirectly-standardised ratios.\nChanging funnel plot options\nggplot2 package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04D/Hands-on_Ex04D.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex04D/Hands-on_Ex04D.html#overview",
    "title": "Hands-on Exercise 04D",
    "section": "",
    "text": "Funnel plot is a specially designed data visualisation for conducting unbiased comparison between outlets, stores or business entities. By the end of this hands-on exercise, we will gain hands-on experience on:\n\nplotting funnel plots by using funnelPlotR package,\nplotting static funnel plot by using ggplot2 package, and\nplotting interactive funnel plot by using both plotly R and ggplot2 packages."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04D/Hands-on_Ex04D.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex04D/Hands-on_Ex04D.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 04D",
    "section": "",
    "text": "In this exercise, four R packages will be used. They are:\n\nreadr for importing csv into R.\nFunnelPlotR for creating funnel plot.\nggplot2 for creating funnel plot manually.\nknitr for building static html table.\nplotly for creating interactive funnel plot.\n\n\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04D/Hands-on_Ex04D.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex04D/Hands-on_Ex04D.html#importing-data",
    "title": "Hands-on Exercise 04D",
    "section": "",
    "text": "In this section, COVID-19_DKI_Jakarta will be used. The data was downloaded from Open Data Covid-19 Provinsi DKI Jakarta portal. For this hands-on exercise, we are going to compare the cumulative COVID-19 cases and death by sub-district (i.e. kelurahan) as at 31st July 2021, DKI Jakarta.\nThe code chunk below imports the data into R and save it into a tibble data frame object called covid19.\n\ncovid19 &lt;- read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %&gt;%\n  mutate_if(is.character, as.factor)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSub-district ID\nCity\nDistrict\nSub-district\nPositive\nRecovered\nDeath\n\n\n\n\n3172051003\nJAKARTA UTARA\nPADEMANGAN\nANCOL\n1776\n1691\n26\n\n\n3173041007\nJAKARTA BARAT\nTAMBORA\nANGKE\n1783\n1720\n29\n\n\n3175041005\nJAKARTA TIMUR\nKRAMAT JATI\nBALE KAMBANG\n2049\n1964\n31\n\n\n3175031003\nJAKARTA TIMUR\nJATINEGARA\nBALI MESTER\n827\n797\n13\n\n\n3175101006\nJAKARTA TIMUR\nCIPAYUNG\nBAMBU APUS\n2866\n2792\n27\n\n\n3174031002\nJAKARTA SELATAN\nMAMPANG PRAPATAN\nBANGKA\n1828\n1757\n26"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04D/Hands-on_Ex04D.html#funnelplotr-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04D/Hands-on_Ex04D.html#funnelplotr-methods",
    "title": "Hands-on Exercise 04D",
    "section": "",
    "text": "FunnelPlotR package uses ggplot to generate funnel plots. It requires a numerator (events of interest), denominator (population to be considered) and group. The key arguments selected for customisation are:\n\nlimit: plot limits (95 or 99).\nlabel_outliers: to label outliers (true or false).\nPoisson_limits: to add Poisson limits to the plot.\nOD_adjust: to add overdispersed limits to the plot.\nxrange and yrange: to specify the range to display for axes, acts like a zoom function.\nOther aesthetic components such as graph title, axis labels etc.\n\n\n\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Positive,\n  denominator = Death,\n  group = `Sub-district`\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\nA funnel plot object with 267 points of which 0 are outliers.  Plot is adjusted for overdispersion. \nThings to learn from the code chunk above.\n\ngroup in this function is different from the scatterplot. Here, it defines the level of the points to be plotted i.e. Sub-district, District or City. If Cityc is chosen, there are only six data points.\nBy default, data_typeargument is “SR”.\nlimit: Plot limits, accepted values are: 95 or 99, corresponding to 95% or 99.8% quantiles of the distribution.\n\n\n\n\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Death,\n  denominator = Positive,\n  group = `Sub-district`,\n  data_type = \"PR\",     #&lt;&lt;\n  xrange = c(0, 6500),  #&lt;&lt;\n  yrange = c(0, 0.05)   #&lt;&lt;\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nA funnel plot object with 267 points of which 7 are outliers.  Plot is adjusted for overdispersion. \nThings to learn from the code chunk above. + data_type argument is used to change from default “SR” to “PR” (i.e. proportions). + xrange and yrange are used to set the range of x-axis and y-axis\n\n\n\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Death,\n  denominator = Positive,\n  group = `Sub-district`,\n  data_type = \"PR\",   \n  xrange = c(0, 6500),  \n  yrange = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by Cumulative Total Number of COVID-19 Positive Cases\", #&lt;&lt;           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #&lt;&lt;\n  y_label = \"Cumulative Fatality Rate\"  #&lt;&lt;\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nA funnel plot object with 267 points of which 7 are outliers.  Plot is adjusted for overdispersion. \nThings to learn from the code chunk above.\n\nlabel = NA argument is to removed the default label outliers feature.\ntitle argument is used to add plot title.\nx_label and y_label arguments are used to add/edit x-axis and y-axis titles."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04D/Hands-on_Ex04D.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex04D/Hands-on_Ex04D.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "title": "Hands-on Exercise 04D",
    "section": "",
    "text": "In this section, you will gain hands-on experience on building funnel plots step-by-step by using ggplot2. It aims to enhance you working experience of ggplot2 to customise speciallised data visualization like funnel plot.\n\n\nTo plot the funnel plot from scratch, we need to derive cumulative death rate and standard error of cumulative death rate.\n\ndf &lt;- covid19 %&gt;%\n  mutate(rate = Death / Positive) %&gt;%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %&gt;%\n  filter(rate &gt; 0)\n\nNext, the fit.mean is computed by using the code chunk below.\n\nfit.mean &lt;- weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\n\nThe code chunk below is used to compute the lower and upper limits for 95% confidence interval.\n\nnumber.seq &lt;- seq(1, max(df$Positive), 1)\nnumber.ll95 &lt;- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 &lt;- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 &lt;- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 &lt;- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n\n\n\nIn the code chunk below, ggplot2 functions are used to plot a static funnel plot.\n\np &lt;- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np\n\n\n\n\n\n\n\n\n\n\n\nThe funnel plot created using ggplot2 functions can be made interactive with ggplotly() of plotly r package.\n\nfp_ggplotly &lt;- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04D/Hands-on_Ex04D.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex04D/Hands-on_Ex04D.html#references",
    "title": "Hands-on Exercise 04D",
    "section": "",
    "text": "funnelPlotR package.\nFunnel Plots for Indirectly-standardised ratios.\nChanging funnel plot options\nggplot2 package."
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex_05/MC1.html",
    "href": "In-Class_Ex/In-Class_Ex_05/MC1.html",
    "title": "In-Class Exercise 05",
    "section": "",
    "text": "MC 01\npacman::p_load(tidyverse, jsonlite, SmartEDA, tidygraph, ggraph)\nkg &lt;- fromJSON(\"data/MC1_graph.json\")"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex_05/MC1.html#initial-eda",
    "href": "In-Class_Ex/In-Class_Ex_05/MC1.html#initial-eda",
    "title": "In-Class Exercise 05",
    "section": "Initial EDA",
    "text": "Initial EDA\n\nggplot(data = edges_tbl, aes(y = `Edge Type`)) + \n  geom_bar()"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex_05/MC1.html#creating-knowledge-graph",
    "href": "In-Class_Ex/In-Class_Ex_05/MC1.html#creating-knowledge-graph",
    "title": "In-Class Exercise 05",
    "section": "Creating knowledge graph",
    "text": "Creating knowledge graph\n\nStep 1: Mapping from node id to row index\n\nid_map &lt;- tibble(id = nodes_tbl$id, index = seq_len(nrow(nodes_tbl)))\n\n\n\nStep 2: Map source and target IDs to row indices\n\nedges_tbl &lt;- edges_tbl %&gt;%\n  left_join(id_map, by = c(\"source\" = \"id\"), suffix = c(\"\", \"_source\")) %&gt;%\n  rename(from = index) %&gt;%\n  left_join(id_map, by = c(\"target\" = \"id\"), suffix = c(\"\", \"_target\")) %&gt;%\n  rename(to = index)\n\n\n\nStep 3\n\nedges_tbl &lt;- edges_tbl %&gt;%\n  filter(!is.na(from), !is.na(to))\n\n\n\nStep 4: Creating the graph\n\ngraph &lt;- tbl_graph(nodes = nodes_tbl, edges = edges_tbl, \n                   directed = kg$directed)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex_05/MC1.html#visualizing-the-knowledge-graph",
    "href": "In-Class_Ex/In-Class_Ex_05/MC1.html#visualizing-the-knowledge-graph",
    "title": "In-Class Exercise 05",
    "section": "Visualizing the knowledge graph",
    "text": "Visualizing the knowledge graph\n\nset.seed(1234)\n\n\nVisualizing the whole Graph\n\nggraph(graph, layout = \"fr\") + \n  geom_edge_link(alpha = 0.3, colour = \"gray\") +\n  geom_node_point(aes(color = `Node Type`), size = 4) +\n  geom_node_text(aes(label = name), repel = TRUE, size = 2.5) +\n  theme_void()\n\n\nStep 1: Filter edges to only “MemberOf”\n\ngraph_memberof &lt;- graph %&gt;%\n  activate(edges) %&gt;%\n  filter(`Edge Type` == \"MemberOf\")\n\n\n\nStep 2: Extract only connected nodes (ie. used in these edges)\n\nused_node_indices &lt;- graph_memberof %&gt;%\n  activate(edges) %&gt;%\n  as_tibble() %&gt;%\n  select(from, to) %&gt;%\n  unlist() %&gt;%\n  unique()\n\n\n\nStep 3: Keep only those nodes\n\ngraph_memberof &lt;- graph_memberof %&gt;%\n  activate(nodes) %&gt;%\n  mutate(row_id = row_number()) %&gt;%\n  filter(row_id %in% used_node_indices) %&gt;%\n  select(-row_id)  #optional cleanup\n\n\n\nPlot the sub-graph\n\nggraph(graph_memberof, layout = \"fr\") + \n  geom_edge_link(alpha = 0.5, colour = \"gray\") +\n  geom_node_point(aes(color = `Node Type`), size = 1) +\n  geom_node_text(aes(label = name), repel = TRUE, size = 2.5) +\n  theme_void()"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_1/Take-Home_Ex1.html",
    "href": "Take-home_Ex/Take-home_Ex_1/Take-Home_Ex1.html",
    "title": "Take-Home Exercise 1",
    "section": "",
    "text": "Creating enlightening and truthful data visualizations involves focusing on accuracy, transparency, and the ability to effectively communicate insights. It’s about presenting data in a way that is both informative and aesthetically pleasing, ensuring the audience can grasp the information quickly and accurately.\n\n\n\nA local online media company that publishes daily content on digital platforms is planning to release an article on demographic structures and distribution of Singapore in 2024.\n\n\n\nAssuming the role of the graphical editor of the media company, we are tasked to prepare at most three data visualization for the article.\n\n\n\n\n\nWe load the following R packages using the pacman::p_load() function:\n\ntidyverse: R packages designed for data science\nggrepel: to provides geoms for ggplot2 to repel overlapping text labels\nggthemes: to use additional themes for ggplot2\npatchwork: to prepare composite figure created using ggplot2\nscales: to provide the internal scaling infrastructure used by ggplot2\nggpubr to create publication ready ggplot2 plots.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed in the computer.\n\npacman::p_load(tidyverse, ggrepel, patchwork, ggthemes, scales,\n               ggpubr) \n\n\n\n\nTo accomplish the task, Singapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2024 dataset shared by Department of Statistics, Singapore (DOS) will be used and we wil load it as follows:\n\ndata &lt;- read_csv(\"data/respopagesex2024.csv\", col_names = TRUE)\n\n\n\n\n\n\nWe first take a look at the data. Using the code below, we can get the details of the dataset which contains 60,424 rows and 6 columns.\n\nglimpse(data)\n\nRows: 60,424\nColumns: 6\n$ PA   &lt;chr&gt; \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo K…\n$ SZ   &lt;chr&gt; \"Ang Mo Kio Town Centre\", \"Ang Mo Kio Town Centre\", \"Ang Mo Kio T…\n$ Age  &lt;chr&gt; \"0\", \"0\", \"1\", \"1\", \"2\", \"2\", \"3\", \"3\", \"4\", \"4\", \"5\", \"5\", \"6\", …\n$ Sex  &lt;chr&gt; \"Males\", \"Females\", \"Males\", \"Females\", \"Males\", \"Females\", \"Male…\n$ Pop  &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 30, 10, 20, 10, 20, 30, 30, 10, 3…\n$ Time &lt;dbl&gt; 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024,…\n\n\n\n\n\n\nWe notice that there is only one value in Time column (2024) which will not be used for further analysis, we will delete this column as per the code chunk below:\n\n\ndata &lt;- data %&gt;% select(-Time)\n\n\nWe will rename the column names in the dataset for clarity, as detailed provided by the Department of Statistics (DOS), as follows:\n\nPA → Planning Area\nSZ → Subzone\nPop → Resident Count\n\n\n\n\n\n\nSide Note:\n\n\n\nPlease note: according to the DOS accompanying documentation of this dataset, the population figures in the csv file have been rounded to the nearest 10, and as such, total counts may not sum exactly due to rounding adjustments.\n\n\n\ncolnames(data) &lt;- c(\"PlanningArea\", \"SubZone\", \"Age\", \"Sex\", \"ResidentCount\")\n\n\nNext, we observe that for the Age column, there is a value of : “90_and_over”. We will replace this value with “90” and change the data type from string/character to numeric and then create a new column to classify the age according to the age bracket in the interval of 5 years as per the standard age group published in DOS:\n\n\ndata &lt;- data %&gt;%\n  mutate(\n    # Normalize Age values to lowercase\n    Age = str_to_lower(Age),\n    \n    # Replace \"90_and_over\" with \"90\"\n    Age = ifelse(Age == \"90_and_over\", \"90\", Age),\n    \n    # Convert to numeric\n    Age = as.numeric(Age),\n    \n    # Create AgeGroup with standard bins\n    AgeGroup = cut(Age,\n                   breaks = c(0,4,9,14,19,24,29,34,39,44,49,54,59,64,69,74,79,84,89, Inf),\n                   labels = c(\"0–4\", \"5–9\", \"10–14\", \"15–19\", \"20–24\", \"25–29\",\n                              \"30–34\", \"35–39\", \"40–44\", \"45–49\", \"50–54\", \n                              \"55–59\", \"60–64\", \"65–69\", \"70–74\", \"75–79\", \n                              \"80–84\", \"85–89\", \"90+\"),\n                   right = TRUE, include.lowest = TRUE)\n  )\n\n\nFurther observation of the dataset, we discover there are multiple rows with “0” values in the “Pop”/“ResidentCount” column. We will remove these rows as per the code chunk below, and calculate the number of rows and total population before and after the deletion to ensure completeness:\n\n\n# Total population before removing zero-pop rows\ntotal_population_before &lt;- sum(data$ResidentCount, na.rm = TRUE)\ntotal_rows_before &lt;- nrow(data)\n\n# Count and show how many rows have 0 population\nzero_count &lt;- data %&gt;%\n  filter(ResidentCount == 0) %&gt;%\n  nrow()\n\ncat(\"Total population before cleaning:\", format(total_population_before, big.mark = \",\"), \"\\n\")\n\nTotal population before cleaning: 4,193,530 \n\ncat(\"Total rows before cleaning:\", total_rows_before, \"\\n\")\n\nTotal rows before cleaning: 60424 \n\ncat(\"Rows with 0 ResidentCount removed:\", zero_count, \"\\n\")\n\nRows with 0 ResidentCount removed: 23181 \n\n# Remove rows with 0 population\ndata &lt;- data %&gt;%\n  filter(ResidentCount &gt; 0)\n\n# Recalculate totals after cleaning\ntotal_population_after &lt;- sum(data$ResidentCount, na.rm = TRUE)\ntotal_rows_after &lt;- nrow(data)\n\ncat(\"Total population after cleaning:\", format(total_population_after, big.mark = \",\"), \"\\n\")\n\nTotal population after cleaning: 4,193,530 \n\ncat(\"Remaining rows:\", total_rows_after, \"\\n\")\n\nRemaining rows: 37243 \n\n\n\n\n\nNext, Using the duplicated function, we see that there are no duplicate entries in the data.\n\ndata[duplicated(data),]\n\n# A tibble: 0 × 6\n# ℹ 6 variables: PlanningArea &lt;chr&gt;, SubZone &lt;chr&gt;, Age &lt;dbl&gt;, Sex &lt;chr&gt;,\n#   ResidentCount &lt;dbl&gt;, AgeGroup &lt;fct&gt;\n\n\n\n\n\nWe run the code below to check for any missing values, and there is none.\n\ncolSums(is.na(data))\n\n PlanningArea       SubZone           Age           Sex ResidentCount \n            0             0             0             0             0 \n     AgeGroup \n            0 \n\n\n\n\n\nWe run an overview of the final dataset again before proceeding to the visualization. Final dataset contains 37,243 rows and 7 columns:\n\nglimpse(data)\n\nRows: 37,243\nColumns: 6\n$ PlanningArea  &lt;chr&gt; \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", …\n$ SubZone       &lt;chr&gt; \"Ang Mo Kio Town Centre\", \"Ang Mo Kio Town Centre\", \"Ang…\n$ Age           &lt;dbl&gt; 0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9,…\n$ Sex           &lt;chr&gt; \"Males\", \"Females\", \"Males\", \"Females\", \"Males\", \"Female…\n$ ResidentCount &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 30, 10, 20, 10, 20, 30, …\n$ AgeGroup      &lt;fct&gt; 0–4, 0–4, 0–4, 0–4, 0–4, 0–4, 0–4, 0–4, 0–4, 0–4, 5–9, 5…\n\n\n\n\n\n\n\n\n\nInsights:\n\nThe population pyramid reveals a dominant working-age group between ages 30–44, forming the broadest segment of the chart, with largest age group for Female: from 35-39 age group: 166,150 and Males: from 30-34 age group with 155,630\nThe base is narrower, especially for those aged 0–14, which highlights the ongoing trend of declining birth rates.\nFemales significantly outnumber males from age 65 onwards, highlighting gender differences in life expectancy\nThe median age of 42 reinforces Singapore’s aging trend, with implications for healthcare and eldercare planning.\nThe median age of 42 underscores Singapore’s aging population, signaling increasing needs in healthcare, retirement, and eldercare.\n\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npyramid_data &lt;- data %&gt;%\n  group_by(AgeGroup, Sex) %&gt;%\n  summarise(ResidentCount = sum(ResidentCount, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    ResidentCountSigned = ifelse(Sex == \"Males\", -ResidentCount, ResidentCount),\n    fill_color = case_when(\n      Sex == \"Males\" ~ \"#4292c6\",\n      Sex == \"Females\" ~ \"#e377c2\",\n      TRUE ~ \"gray\"\n    ),\n    AgeGroup = factor(AgeGroup, levels = c(\"0–4\", \"5–9\", \"10–14\", \"15–19\", \"20–24\",\n                                           \"25–29\", \"30–34\", \"35–39\", \"40–44\", \"45–49\",\n                                           \"50–54\", \"55–59\", \"60–64\", \"65–69\", \"70–74\",\n                                           \"75–79\", \"80–84\", \"85–89\", \"90+\"))\n  )\n\nmedian_age &lt;- data %&gt;%\n  group_by(Age) %&gt;%\n  summarise(Total = sum(ResidentCount, na.rm = TRUE)) %&gt;%\n  arrange(Age) %&gt;%\n  mutate(cum_pop = cumsum(Total), prop = cum_pop / sum(Total)) %&gt;%\n  filter(prop &gt;= 0.5) %&gt;%\n  slice(1) %&gt;%\n  pull(Age)\n\nage_group_labels &lt;- levels(pyramid_data$AgeGroup)\nmedian_group_index &lt;- findInterval(median_age, seq(0, 100, by = 5))\nmedian_group &lt;- age_group_labels[median_group_index]\n\ntotal_males &lt;- pyramid_data %&gt;% filter(Sex == \"Males\") %&gt;% summarise(sum = sum(abs(ResidentCountSigned))) %&gt;% pull(sum)\ntotal_females &lt;- pyramid_data %&gt;% filter(Sex == \"Females\") %&gt;% summarise(sum = sum(ResidentCountSigned)) %&gt;% pull(sum)\n\nggplot(pyramid_data, aes(y = AgeGroup, x = ResidentCountSigned, fill = fill_color)) +\n  geom_col(width = 0.9) +\n  geom_text(aes(label = abs(ResidentCountSigned),\n                x = ifelse(ResidentCountSigned &lt; 0, ResidentCountSigned - 5000, ResidentCountSigned + 5000)),\n            hjust = ifelse(pyramid_data$ResidentCountSigned &lt; 0, 1, 0),\n            size = 3, color = \"black\") +\n  annotate(\"segment\",\n           x = -max(abs(pyramid_data$ResidentCountSigned)) * 1.5,\n           xend = max(abs(pyramid_data$ResidentCountSigned)) * 1.5,\n           y = median_group, yend = median_group,\n           linetype = \"dotted\", color = \"#A9A9A9\", linewidth = 0.9) +\n  annotate(\"text\",\n           x = max(abs(pyramid_data$ResidentCountSigned)) * 1.5,\n           y = median_group,\n           label = paste0(\"Median: \", median_age),\n           hjust = 0, size = 2.8, color = \"black\", fontface = \"bold\") +\n  annotate(\"text\", y = \"0–4\",\n           x = -max(abs(pyramid_data$ResidentCountSigned)) * 0.95,\n           label = paste0(\"Males\\nTotal: \", format(total_males, big.mark = \",\")),\n           size = 2.6, color = \"#1E90FF\", fontface = \"bold\", hjust = 1) +\n  annotate(\"text\", y = \"0–4\",\n           x = max(abs(pyramid_data$ResidentCountSigned)) * 0.95,\n           label = paste0(\"Females\\nTotal: \", format(total_females, big.mark = \",\")),\n           size = 2.6, color = \"#c51b8a\", fontface = \"bold\", hjust = 0) +\n  scale_fill_identity() +\n  scale_x_continuous(labels = abs, expand = expansion(mult = c(0.12, 0.12))) +\n  labs(\n    title = \"Singapore’s Shifting Age Structure (June 2024)\",\n    subtitle = \"Middle-age Population Dominates; Youth Base Shrinking, Elderly Segment Rising\",\n    x = NULL,\n    y = \"Age Group (Years)\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    panel.background = element_rect(fill = \"#FFFCE8\", color = NA),\n    plot.background = element_rect(fill = \"#FFFCE8\", color = NA),\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\", margin = margin(b = 6)),\n    plot.subtitle = element_text(hjust = 0.5, size = 12, margin = margin(b = 12)),\n    axis.text.y = element_text(size = 10),\n    axis.title.y = element_text(size = 11, face = \"bold\"),\n    axis.text.x = element_blank(),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\nInsights:\n\nThe top chart shows the proportion of elderly residents in the top 10 planning areas. Outram has the highest share, with 26.9% of its population are seniors, followed by Ang Mo Kio (24.3%) and Bukit Merah (23.4%). These established towns may benefit from enhanced elderly-supportive environments, such as barrier-free access, senior-oriented amenities, and close-proximity services.\nThe bottom chart presents the elderly resident count, with Bedok having the largest at 60,770, followed by Tampines (49,700) and Hougang (44,640). This is largely due to their larger area size and population base. These towns would benefit from service scaling, such as Active Ageing Centres (AACs), public transport connectivity, and healthcare access.\nWith Singapore’s elderly population projected to reach one in four residents (DOS, 2024), it is important to consider both distribution by proportion and resident count for effective planning.\nThis dual perspective supports the Ministry of Health’s 2023 Action Plan, which aims to double eldercare centres by 2025 and enhance community-based support (MOH, 2023).\n\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nelderly_data &lt;- data %&gt;% filter(Age &gt;= 65)\n\ntotal_pop &lt;- data %&gt;%\n  group_by(PlanningArea) %&gt;%\n  summarise(Total_Pop = sum(ResidentCount, na.rm = TRUE))\n\nelderly_count &lt;- elderly_data %&gt;%\n  group_by(PlanningArea) %&gt;%\n  summarise(Elderly_Pop = sum(ResidentCount, na.rm = TRUE)) %&gt;%\n  arrange(desc(Elderly_Pop)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  mutate(PlanningArea = fct_reorder(PlanningArea, Elderly_Pop))\n\nelderly_prop &lt;- elderly_data %&gt;%\n  group_by(PlanningArea) %&gt;%\n  summarise(Elderly_Pop = sum(ResidentCount, na.rm = TRUE)) %&gt;%\n  left_join(total_pop, by = \"PlanningArea\") %&gt;%\n  mutate(Elderly_Proportion = Elderly_Pop / Total_Pop) %&gt;%\n  arrange(desc(Elderly_Proportion)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  mutate(PlanningArea = fct_reorder(PlanningArea, Elderly_Proportion))\n\n# Plot\np1 &lt;- ggplot(elderly_prop, aes(x = Elderly_Proportion, y = PlanningArea)) +\n  geom_col(fill = \"#4DAF4A\", width = 0.85) +\n  scale_x_continuous(\n    breaks = seq(0, 0.35, by = 0.05),\n    labels = percent_format(accuracy = 1),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  labs(\n    title = \"Top 10 Planning Areas by Elderly Proportion (Age 65+)\",\n    x = \"Proportion of Elderly Residents\",\n    y = NULL\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    panel.background = element_rect(fill = \"#FFFCE8\", color = NA),\n    plot.background = element_rect(fill = \"#FFFCE8\", color = NA),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n    axis.title.x = element_text(size = 11),\n    panel.grid.major.x = element_line(color = \"grey90\"),\n    panel.grid.minor = element_blank()\n  )\n\n\np2 &lt;- ggplot(elderly_count, aes(x = Elderly_Pop, y = PlanningArea)) +\n  geom_col(fill = \"#4DAF4A\", width = 0.85) +\n  scale_x_continuous(\n    breaks = seq(0, 70000, by = 10000),\n    labels = comma_format(),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  labs(\n    title = \"Top 10 Planning Areas by Elderly Resident Count (Age 65+)\",\n    x = \"Number of Elderly Residents\",\n    y = NULL\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    panel.background = element_rect(fill = \"#FFFCE8\", color = NA),\n    plot.background = element_rect(fill = \"#FFFCE8\", color = NA),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n    axis.title.x = element_text(size = 11),\n    panel.grid.major.x = element_line(color = \"grey90\"),\n    panel.grid.minor = element_blank()\n  )\n\np1 / p2\n\n\n\n\n\n\n\nInsights:\n\nTampines, Bedok, and Sengkang are top the list with over 250,000 residents each.\nMillennials (age 28–43) are the largest group in most PA, while Gen X dominates in Bedok, a mature estate.\nYounger generations (Gen Alpha and Gen Z) are more concentrated in newer towns like Sengkang, Punggol, and Jurong West, aligned with recent BTO developments that attract young families.\nThe distribution reflects a balanced generational mix, highlighting Singapore’s multigenerational living pattern—with both aging residents and young households sharing town spaces.\nThese trends align with Singapore’s Smart Nation and HDB’s ‘Designing for Life’ vision: fostering harmonious, inclusive communities where families of all ages can live, age, and thrive together through well-integrated facilities, technology, and people-first urban design.\n\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata &lt;- data %&gt;%\n  mutate(Age = as.numeric(Age),\n         Generation = case_when(\n           Age &lt;= 9 ~ \"Gen Alpha (≤9)\",\n           Age &lt;= 27 ~ \"Gen Z (10–27)\",\n           Age &lt;= 43 ~ \"Millennials (28–43)\",\n           Age &lt;= 59 ~ \"Gen X (44–59)\",\n           Age &lt;= 77 ~ \"Baby Boomers (60–77)\",\n           TRUE ~ \"Silent Gen (78+)\"\n         ))\n\ngen_by_area &lt;- data %&gt;%\n  group_by(PlanningArea, Generation) %&gt;%\n  summarise(ResidentCount = sum(ResidentCount, na.rm = TRUE), .groups = \"drop\")\n\ntop10_areas &lt;- gen_by_area %&gt;%\n  group_by(PlanningArea) %&gt;%\n  summarise(TotalPop = sum(ResidentCount)) %&gt;%\n  arrange(desc(TotalPop)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  pull(PlanningArea)\n\ngen_top10 &lt;- gen_by_area %&gt;%\n  filter(PlanningArea %in% top10_areas) %&gt;%\n  mutate(\n    PlanningArea = fct_reorder(PlanningArea, ResidentCount, .fun = sum, .desc = TRUE),\n    Generation = factor(Generation, levels = c(\"Silent Gen (78+)\", \"Baby Boomers (60–77)\",\n                                               \"Gen X (44–59)\", \"Millennials (28–43)\",\n                                               \"Gen Z (10–27)\", \"Gen Alpha (≤9)\"))\n  )\n\ngen_colors &lt;- c(\n  \"Silent Gen (78+)\" = \"#c6dbef\",\n  \"Baby Boomers (60–77)\" = \"#6baed6\",\n  \"Gen X (44–59)\" = \"#b2df8a\",\n  \"Millennials (28–43)\" = \"#33a02c\",\n  \"Gen Z (10–27)\" = \"#fb9a99\",\n  \"Gen Alpha (≤9)\" = \"#e31a1c\"\n)\n\n#Plot\nggplot(gen_top10, aes(x = PlanningArea, y = ResidentCount, fill = Generation)) +\n  geom_col(width = 0.8, color = \"white\") +\n  scale_fill_manual(values = gen_colors) +\n  scale_y_continuous(\n    labels = comma,\n    breaks = seq(0, 300000, 50000),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  labs(\n    title = \"Generational Composition of Top 10 Most Populated Planning Areas\",\n    subtitle = \"Younger generations dominate newer towns, while older cohorts concentrate in mature estates\",\n    x = \"Planning Area\",\n    y = \"Resident Count\",\n    fill = \"Generation\"\n  ) +\n  theme_clean(base_size = 12) +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(size = 11, hjust = 0.5),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid.major.x = element_blank(),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\nSingapore’s demographic structure, based on June 2024 data, highlights a maturing society with a dominant working-age group and a median age of 42. The growing share of seniors and the narrowing base of younger age groups reflect the effects of population aging and low birth rates. Mature estates such as Outram have the highest proportion of elderly residents, while Bedok and Tampines house the largest absolute numbers. In contrast, newer towns like Sengkang and Punggol show higher concentrations of younger generations—particularly Gen Alpha and Gen Z—driven by recent BTO developments attracting young families. Millennials remain the largest generational group across most areas, reinforcing their role in shaping urban life. This evolving yet balanced generational landscape underscores the need for inclusive community planning that supports both young families and seniors—fostering intergenerational harmony and enabling families to live, age, and thrive together.\n\n\n\n\nDepartment of Statistics Singapore. (2024). Population Trends 2024.\nRetrieved from: https://www.singstat.gov.sg/publications/population/population-trends\nMinistry of Health Singapore. (2023). Action Plan for Successful Ageing.\nRetrieved from: https://www.moh.gov.sg/newsroom/launch-of-the-2023-action-plan-for-successful-ageing\nHousing & Development Board (HDB). (2021). Designing for Life: Community Planning and Design Guide.\nRetrieved from: https://www.hdb.gov.sg/cs/infoweb/designing-for-life\nSmart Nation and Digital Government Office. (2023). Smart Nation: Empowering Everyone Through Technology.\nRetrieved from: https://www.smartnation.gov.sg\nSingapore Department of Statistics. (n.d.). National Statistical Standards.\nRetrieved from: https://www.singstat.gov.sg/standards"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_1/Take-Home_Ex1.html#overview",
    "href": "Take-home_Ex/Take-home_Ex_1/Take-Home_Ex1.html#overview",
    "title": "Take-Home Exercise 1",
    "section": "",
    "text": "Creating enlightening and truthful data visualizations involves focusing on accuracy, transparency, and the ability to effectively communicate insights. It’s about presenting data in a way that is both informative and aesthetically pleasing, ensuring the audience can grasp the information quickly and accurately."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_1/Take-Home_Ex1.html#setting-the-scene",
    "href": "Take-home_Ex/Take-home_Ex_1/Take-Home_Ex1.html#setting-the-scene",
    "title": "Take-Home Exercise 1",
    "section": "",
    "text": "A local online media company that publishes daily content on digital platforms is planning to release an article on demographic structures and distribution of Singapore in 2024."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_1/Take-Home_Ex1.html#the-task",
    "href": "Take-home_Ex/Take-home_Ex_1/Take-Home_Ex1.html#the-task",
    "title": "Take-Home Exercise 1",
    "section": "",
    "text": "Assuming the role of the graphical editor of the media company, we are tasked to prepare at most three data visualization for the article."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_1/Take-Home_Ex1.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex_1/Take-Home_Ex1.html#getting-started",
    "title": "Take-Home Exercise 1",
    "section": "",
    "text": "We load the following R packages using the pacman::p_load() function:\n\ntidyverse: R packages designed for data science\nggrepel: to provides geoms for ggplot2 to repel overlapping text labels\nggthemes: to use additional themes for ggplot2\npatchwork: to prepare composite figure created using ggplot2\nscales: to provide the internal scaling infrastructure used by ggplot2\nggpubr to create publication ready ggplot2 plots.\n\nThe code chunk below uses the p_load() function in the pacman package to check if the packages are installed in the computer.\n\npacman::p_load(tidyverse, ggrepel, patchwork, ggthemes, scales,\n               ggpubr) \n\n\n\n\nTo accomplish the task, Singapore Residents by Planning Area / Subzone, Single Year of Age and Sex, June 2024 dataset shared by Department of Statistics, Singapore (DOS) will be used and we wil load it as follows:\n\ndata &lt;- read_csv(\"data/respopagesex2024.csv\", col_names = TRUE)\n\n\n\n\n\n\nWe first take a look at the data. Using the code below, we can get the details of the dataset which contains 60,424 rows and 6 columns.\n\nglimpse(data)\n\nRows: 60,424\nColumns: 6\n$ PA   &lt;chr&gt; \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo K…\n$ SZ   &lt;chr&gt; \"Ang Mo Kio Town Centre\", \"Ang Mo Kio Town Centre\", \"Ang Mo Kio T…\n$ Age  &lt;chr&gt; \"0\", \"0\", \"1\", \"1\", \"2\", \"2\", \"3\", \"3\", \"4\", \"4\", \"5\", \"5\", \"6\", …\n$ Sex  &lt;chr&gt; \"Males\", \"Females\", \"Males\", \"Females\", \"Males\", \"Females\", \"Male…\n$ Pop  &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 30, 10, 20, 10, 20, 30, 30, 10, 3…\n$ Time &lt;dbl&gt; 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024,…\n\n\n\n\n\n\nWe notice that there is only one value in Time column (2024) which will not be used for further analysis, we will delete this column as per the code chunk below:\n\n\ndata &lt;- data %&gt;% select(-Time)\n\n\nWe will rename the column names in the dataset for clarity, as detailed provided by the Department of Statistics (DOS), as follows:\n\nPA → Planning Area\nSZ → Subzone\nPop → Resident Count\n\n\n\n\n\n\nSide Note:\n\n\n\nPlease note: according to the DOS accompanying documentation of this dataset, the population figures in the csv file have been rounded to the nearest 10, and as such, total counts may not sum exactly due to rounding adjustments.\n\n\n\ncolnames(data) &lt;- c(\"PlanningArea\", \"SubZone\", \"Age\", \"Sex\", \"ResidentCount\")\n\n\nNext, we observe that for the Age column, there is a value of : “90_and_over”. We will replace this value with “90” and change the data type from string/character to numeric and then create a new column to classify the age according to the age bracket in the interval of 5 years as per the standard age group published in DOS:\n\n\ndata &lt;- data %&gt;%\n  mutate(\n    # Normalize Age values to lowercase\n    Age = str_to_lower(Age),\n    \n    # Replace \"90_and_over\" with \"90\"\n    Age = ifelse(Age == \"90_and_over\", \"90\", Age),\n    \n    # Convert to numeric\n    Age = as.numeric(Age),\n    \n    # Create AgeGroup with standard bins\n    AgeGroup = cut(Age,\n                   breaks = c(0,4,9,14,19,24,29,34,39,44,49,54,59,64,69,74,79,84,89, Inf),\n                   labels = c(\"0–4\", \"5–9\", \"10–14\", \"15–19\", \"20–24\", \"25–29\",\n                              \"30–34\", \"35–39\", \"40–44\", \"45–49\", \"50–54\", \n                              \"55–59\", \"60–64\", \"65–69\", \"70–74\", \"75–79\", \n                              \"80–84\", \"85–89\", \"90+\"),\n                   right = TRUE, include.lowest = TRUE)\n  )\n\n\nFurther observation of the dataset, we discover there are multiple rows with “0” values in the “Pop”/“ResidentCount” column. We will remove these rows as per the code chunk below, and calculate the number of rows and total population before and after the deletion to ensure completeness:\n\n\n# Total population before removing zero-pop rows\ntotal_population_before &lt;- sum(data$ResidentCount, na.rm = TRUE)\ntotal_rows_before &lt;- nrow(data)\n\n# Count and show how many rows have 0 population\nzero_count &lt;- data %&gt;%\n  filter(ResidentCount == 0) %&gt;%\n  nrow()\n\ncat(\"Total population before cleaning:\", format(total_population_before, big.mark = \",\"), \"\\n\")\n\nTotal population before cleaning: 4,193,530 \n\ncat(\"Total rows before cleaning:\", total_rows_before, \"\\n\")\n\nTotal rows before cleaning: 60424 \n\ncat(\"Rows with 0 ResidentCount removed:\", zero_count, \"\\n\")\n\nRows with 0 ResidentCount removed: 23181 \n\n# Remove rows with 0 population\ndata &lt;- data %&gt;%\n  filter(ResidentCount &gt; 0)\n\n# Recalculate totals after cleaning\ntotal_population_after &lt;- sum(data$ResidentCount, na.rm = TRUE)\ntotal_rows_after &lt;- nrow(data)\n\ncat(\"Total population after cleaning:\", format(total_population_after, big.mark = \",\"), \"\\n\")\n\nTotal population after cleaning: 4,193,530 \n\ncat(\"Remaining rows:\", total_rows_after, \"\\n\")\n\nRemaining rows: 37243 \n\n\n\n\n\nNext, Using the duplicated function, we see that there are no duplicate entries in the data.\n\ndata[duplicated(data),]\n\n# A tibble: 0 × 6\n# ℹ 6 variables: PlanningArea &lt;chr&gt;, SubZone &lt;chr&gt;, Age &lt;dbl&gt;, Sex &lt;chr&gt;,\n#   ResidentCount &lt;dbl&gt;, AgeGroup &lt;fct&gt;\n\n\n\n\n\nWe run the code below to check for any missing values, and there is none.\n\ncolSums(is.na(data))\n\n PlanningArea       SubZone           Age           Sex ResidentCount \n            0             0             0             0             0 \n     AgeGroup \n            0 \n\n\n\n\n\nWe run an overview of the final dataset again before proceeding to the visualization. Final dataset contains 37,243 rows and 7 columns:\n\nglimpse(data)\n\nRows: 37,243\nColumns: 6\n$ PlanningArea  &lt;chr&gt; \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", …\n$ SubZone       &lt;chr&gt; \"Ang Mo Kio Town Centre\", \"Ang Mo Kio Town Centre\", \"Ang…\n$ Age           &lt;dbl&gt; 0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9,…\n$ Sex           &lt;chr&gt; \"Males\", \"Females\", \"Males\", \"Females\", \"Males\", \"Female…\n$ ResidentCount &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 30, 10, 20, 10, 20, 30, …\n$ AgeGroup      &lt;fct&gt; 0–4, 0–4, 0–4, 0–4, 0–4, 0–4, 0–4, 0–4, 0–4, 0–4, 5–9, 5…"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_1/Take-Home_Ex1.html#data-visualization",
    "href": "Take-home_Ex/Take-home_Ex_1/Take-Home_Ex1.html#data-visualization",
    "title": "Take-Home Exercise 1",
    "section": "",
    "text": "Insights:\n\nThe population pyramid reveals a dominant working-age group between ages 30–44, forming the broadest segment of the chart, with largest age group for Female: from 35-39 age group: 166,150 and Males: from 30-34 age group with 155,630\nThe base is narrower, especially for those aged 0–14, which highlights the ongoing trend of declining birth rates.\nFemales significantly outnumber males from age 65 onwards, highlighting gender differences in life expectancy\nThe median age of 42 reinforces Singapore’s aging trend, with implications for healthcare and eldercare planning.\nThe median age of 42 underscores Singapore’s aging population, signaling increasing needs in healthcare, retirement, and eldercare.\n\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npyramid_data &lt;- data %&gt;%\n  group_by(AgeGroup, Sex) %&gt;%\n  summarise(ResidentCount = sum(ResidentCount, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    ResidentCountSigned = ifelse(Sex == \"Males\", -ResidentCount, ResidentCount),\n    fill_color = case_when(\n      Sex == \"Males\" ~ \"#4292c6\",\n      Sex == \"Females\" ~ \"#e377c2\",\n      TRUE ~ \"gray\"\n    ),\n    AgeGroup = factor(AgeGroup, levels = c(\"0–4\", \"5–9\", \"10–14\", \"15–19\", \"20–24\",\n                                           \"25–29\", \"30–34\", \"35–39\", \"40–44\", \"45–49\",\n                                           \"50–54\", \"55–59\", \"60–64\", \"65–69\", \"70–74\",\n                                           \"75–79\", \"80–84\", \"85–89\", \"90+\"))\n  )\n\nmedian_age &lt;- data %&gt;%\n  group_by(Age) %&gt;%\n  summarise(Total = sum(ResidentCount, na.rm = TRUE)) %&gt;%\n  arrange(Age) %&gt;%\n  mutate(cum_pop = cumsum(Total), prop = cum_pop / sum(Total)) %&gt;%\n  filter(prop &gt;= 0.5) %&gt;%\n  slice(1) %&gt;%\n  pull(Age)\n\nage_group_labels &lt;- levels(pyramid_data$AgeGroup)\nmedian_group_index &lt;- findInterval(median_age, seq(0, 100, by = 5))\nmedian_group &lt;- age_group_labels[median_group_index]\n\ntotal_males &lt;- pyramid_data %&gt;% filter(Sex == \"Males\") %&gt;% summarise(sum = sum(abs(ResidentCountSigned))) %&gt;% pull(sum)\ntotal_females &lt;- pyramid_data %&gt;% filter(Sex == \"Females\") %&gt;% summarise(sum = sum(ResidentCountSigned)) %&gt;% pull(sum)\n\nggplot(pyramid_data, aes(y = AgeGroup, x = ResidentCountSigned, fill = fill_color)) +\n  geom_col(width = 0.9) +\n  geom_text(aes(label = abs(ResidentCountSigned),\n                x = ifelse(ResidentCountSigned &lt; 0, ResidentCountSigned - 5000, ResidentCountSigned + 5000)),\n            hjust = ifelse(pyramid_data$ResidentCountSigned &lt; 0, 1, 0),\n            size = 3, color = \"black\") +\n  annotate(\"segment\",\n           x = -max(abs(pyramid_data$ResidentCountSigned)) * 1.5,\n           xend = max(abs(pyramid_data$ResidentCountSigned)) * 1.5,\n           y = median_group, yend = median_group,\n           linetype = \"dotted\", color = \"#A9A9A9\", linewidth = 0.9) +\n  annotate(\"text\",\n           x = max(abs(pyramid_data$ResidentCountSigned)) * 1.5,\n           y = median_group,\n           label = paste0(\"Median: \", median_age),\n           hjust = 0, size = 2.8, color = \"black\", fontface = \"bold\") +\n  annotate(\"text\", y = \"0–4\",\n           x = -max(abs(pyramid_data$ResidentCountSigned)) * 0.95,\n           label = paste0(\"Males\\nTotal: \", format(total_males, big.mark = \",\")),\n           size = 2.6, color = \"#1E90FF\", fontface = \"bold\", hjust = 1) +\n  annotate(\"text\", y = \"0–4\",\n           x = max(abs(pyramid_data$ResidentCountSigned)) * 0.95,\n           label = paste0(\"Females\\nTotal: \", format(total_females, big.mark = \",\")),\n           size = 2.6, color = \"#c51b8a\", fontface = \"bold\", hjust = 0) +\n  scale_fill_identity() +\n  scale_x_continuous(labels = abs, expand = expansion(mult = c(0.12, 0.12))) +\n  labs(\n    title = \"Singapore’s Shifting Age Structure (June 2024)\",\n    subtitle = \"Middle-age Population Dominates; Youth Base Shrinking, Elderly Segment Rising\",\n    x = NULL,\n    y = \"Age Group (Years)\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    panel.background = element_rect(fill = \"#FFFCE8\", color = NA),\n    plot.background = element_rect(fill = \"#FFFCE8\", color = NA),\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\", margin = margin(b = 6)),\n    plot.subtitle = element_text(hjust = 0.5, size = 12, margin = margin(b = 12)),\n    axis.text.y = element_text(size = 10),\n    axis.title.y = element_text(size = 11, face = \"bold\"),\n    axis.text.x = element_blank(),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\nInsights:\n\nThe top chart shows the proportion of elderly residents in the top 10 planning areas. Outram has the highest share, with 26.9% of its population are seniors, followed by Ang Mo Kio (24.3%) and Bukit Merah (23.4%). These established towns may benefit from enhanced elderly-supportive environments, such as barrier-free access, senior-oriented amenities, and close-proximity services.\nThe bottom chart presents the elderly resident count, with Bedok having the largest at 60,770, followed by Tampines (49,700) and Hougang (44,640). This is largely due to their larger area size and population base. These towns would benefit from service scaling, such as Active Ageing Centres (AACs), public transport connectivity, and healthcare access.\nWith Singapore’s elderly population projected to reach one in four residents (DOS, 2024), it is important to consider both distribution by proportion and resident count for effective planning.\nThis dual perspective supports the Ministry of Health’s 2023 Action Plan, which aims to double eldercare centres by 2025 and enhance community-based support (MOH, 2023).\n\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nelderly_data &lt;- data %&gt;% filter(Age &gt;= 65)\n\ntotal_pop &lt;- data %&gt;%\n  group_by(PlanningArea) %&gt;%\n  summarise(Total_Pop = sum(ResidentCount, na.rm = TRUE))\n\nelderly_count &lt;- elderly_data %&gt;%\n  group_by(PlanningArea) %&gt;%\n  summarise(Elderly_Pop = sum(ResidentCount, na.rm = TRUE)) %&gt;%\n  arrange(desc(Elderly_Pop)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  mutate(PlanningArea = fct_reorder(PlanningArea, Elderly_Pop))\n\nelderly_prop &lt;- elderly_data %&gt;%\n  group_by(PlanningArea) %&gt;%\n  summarise(Elderly_Pop = sum(ResidentCount, na.rm = TRUE)) %&gt;%\n  left_join(total_pop, by = \"PlanningArea\") %&gt;%\n  mutate(Elderly_Proportion = Elderly_Pop / Total_Pop) %&gt;%\n  arrange(desc(Elderly_Proportion)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  mutate(PlanningArea = fct_reorder(PlanningArea, Elderly_Proportion))\n\n# Plot\np1 &lt;- ggplot(elderly_prop, aes(x = Elderly_Proportion, y = PlanningArea)) +\n  geom_col(fill = \"#4DAF4A\", width = 0.85) +\n  scale_x_continuous(\n    breaks = seq(0, 0.35, by = 0.05),\n    labels = percent_format(accuracy = 1),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  labs(\n    title = \"Top 10 Planning Areas by Elderly Proportion (Age 65+)\",\n    x = \"Proportion of Elderly Residents\",\n    y = NULL\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    panel.background = element_rect(fill = \"#FFFCE8\", color = NA),\n    plot.background = element_rect(fill = \"#FFFCE8\", color = NA),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n    axis.title.x = element_text(size = 11),\n    panel.grid.major.x = element_line(color = \"grey90\"),\n    panel.grid.minor = element_blank()\n  )\n\n\np2 &lt;- ggplot(elderly_count, aes(x = Elderly_Pop, y = PlanningArea)) +\n  geom_col(fill = \"#4DAF4A\", width = 0.85) +\n  scale_x_continuous(\n    breaks = seq(0, 70000, by = 10000),\n    labels = comma_format(),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  labs(\n    title = \"Top 10 Planning Areas by Elderly Resident Count (Age 65+)\",\n    x = \"Number of Elderly Residents\",\n    y = NULL\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    panel.background = element_rect(fill = \"#FFFCE8\", color = NA),\n    plot.background = element_rect(fill = \"#FFFCE8\", color = NA),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n    axis.title.x = element_text(size = 11),\n    panel.grid.major.x = element_line(color = \"grey90\"),\n    panel.grid.minor = element_blank()\n  )\n\np1 / p2\n\n\n\n\n\n\n\nInsights:\n\nTampines, Bedok, and Sengkang are top the list with over 250,000 residents each.\nMillennials (age 28–43) are the largest group in most PA, while Gen X dominates in Bedok, a mature estate.\nYounger generations (Gen Alpha and Gen Z) are more concentrated in newer towns like Sengkang, Punggol, and Jurong West, aligned with recent BTO developments that attract young families.\nThe distribution reflects a balanced generational mix, highlighting Singapore’s multigenerational living pattern—with both aging residents and young households sharing town spaces.\nThese trends align with Singapore’s Smart Nation and HDB’s ‘Designing for Life’ vision: fostering harmonious, inclusive communities where families of all ages can live, age, and thrive together through well-integrated facilities, technology, and people-first urban design.\n\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata &lt;- data %&gt;%\n  mutate(Age = as.numeric(Age),\n         Generation = case_when(\n           Age &lt;= 9 ~ \"Gen Alpha (≤9)\",\n           Age &lt;= 27 ~ \"Gen Z (10–27)\",\n           Age &lt;= 43 ~ \"Millennials (28–43)\",\n           Age &lt;= 59 ~ \"Gen X (44–59)\",\n           Age &lt;= 77 ~ \"Baby Boomers (60–77)\",\n           TRUE ~ \"Silent Gen (78+)\"\n         ))\n\ngen_by_area &lt;- data %&gt;%\n  group_by(PlanningArea, Generation) %&gt;%\n  summarise(ResidentCount = sum(ResidentCount, na.rm = TRUE), .groups = \"drop\")\n\ntop10_areas &lt;- gen_by_area %&gt;%\n  group_by(PlanningArea) %&gt;%\n  summarise(TotalPop = sum(ResidentCount)) %&gt;%\n  arrange(desc(TotalPop)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  pull(PlanningArea)\n\ngen_top10 &lt;- gen_by_area %&gt;%\n  filter(PlanningArea %in% top10_areas) %&gt;%\n  mutate(\n    PlanningArea = fct_reorder(PlanningArea, ResidentCount, .fun = sum, .desc = TRUE),\n    Generation = factor(Generation, levels = c(\"Silent Gen (78+)\", \"Baby Boomers (60–77)\",\n                                               \"Gen X (44–59)\", \"Millennials (28–43)\",\n                                               \"Gen Z (10–27)\", \"Gen Alpha (≤9)\"))\n  )\n\ngen_colors &lt;- c(\n  \"Silent Gen (78+)\" = \"#c6dbef\",\n  \"Baby Boomers (60–77)\" = \"#6baed6\",\n  \"Gen X (44–59)\" = \"#b2df8a\",\n  \"Millennials (28–43)\" = \"#33a02c\",\n  \"Gen Z (10–27)\" = \"#fb9a99\",\n  \"Gen Alpha (≤9)\" = \"#e31a1c\"\n)\n\n#Plot\nggplot(gen_top10, aes(x = PlanningArea, y = ResidentCount, fill = Generation)) +\n  geom_col(width = 0.8, color = \"white\") +\n  scale_fill_manual(values = gen_colors) +\n  scale_y_continuous(\n    labels = comma,\n    breaks = seq(0, 300000, 50000),\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  labs(\n    title = \"Generational Composition of Top 10 Most Populated Planning Areas\",\n    subtitle = \"Younger generations dominate newer towns, while older cohorts concentrate in mature estates\",\n    x = \"Planning Area\",\n    y = \"Resident Count\",\n    fill = \"Generation\"\n  ) +\n  theme_clean(base_size = 12) +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(size = 11, hjust = 0.5),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid.major.x = element_blank(),\n    legend.position = \"right\"\n  )"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_1/Take-Home_Ex1.html#summary",
    "href": "Take-home_Ex/Take-home_Ex_1/Take-Home_Ex1.html#summary",
    "title": "Take-Home Exercise 1",
    "section": "",
    "text": "Singapore’s demographic structure, based on June 2024 data, highlights a maturing society with a dominant working-age group and a median age of 42. The growing share of seniors and the narrowing base of younger age groups reflect the effects of population aging and low birth rates. Mature estates such as Outram have the highest proportion of elderly residents, while Bedok and Tampines house the largest absolute numbers. In contrast, newer towns like Sengkang and Punggol show higher concentrations of younger generations—particularly Gen Alpha and Gen Z—driven by recent BTO developments attracting young families. Millennials remain the largest generational group across most areas, reinforcing their role in shaping urban life. This evolving yet balanced generational landscape underscores the need for inclusive community planning that supports both young families and seniors—fostering intergenerational harmony and enabling families to live, age, and thrive together."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_1/Take-Home_Ex1.html#references",
    "href": "Take-home_Ex/Take-home_Ex_1/Take-Home_Ex1.html#references",
    "title": "Take-Home Exercise 1",
    "section": "",
    "text": "Department of Statistics Singapore. (2024). Population Trends 2024.\nRetrieved from: https://www.singstat.gov.sg/publications/population/population-trends\nMinistry of Health Singapore. (2023). Action Plan for Successful Ageing.\nRetrieved from: https://www.moh.gov.sg/newsroom/launch-of-the-2023-action-plan-for-successful-ageing\nHousing & Development Board (HDB). (2021). Designing for Life: Community Planning and Design Guide.\nRetrieved from: https://www.hdb.gov.sg/cs/infoweb/designing-for-life\nSmart Nation and Digital Government Office. (2023). Smart Nation: Empowering Everyone Through Technology.\nRetrieved from: https://www.smartnation.gov.sg\nSingapore Department of Statistics. (n.d.). National Statistical Standards.\nRetrieved from: https://www.singstat.gov.sg/standards"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "title": "Hands-on Exercise 06",
    "section": "",
    "text": "By the end of this hands-on exercise we will be able create the followings data visualisation by using R packages:\n\nplotting a calender heatmap by using ggplot2 functions,\nplotting a cycle plot by using ggplot2 function,\nplotting a slopegraph\nplotting a horizon chart\n\n\n\n\n\n\n\nWrite a code chunk to check, install and launch the following R packages: scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table and tidyverse.\n\n\nShow the code\n\n\npacman::p_load(scales, viridis, lubridate, ggthemes,\n               gridExtra, readxl, knitr, data.table,\n               CGPfunctions, ggHoriPlot, tidyverse)\n\n\n\n\n\nIn this section, we will learn how to plot a calender heatmap programmatically by using ggplot2 package.\n\nBy the end of this section, we will be able to:\n\nplot a calender heatmap by using ggplot2 functions and extension,\nto write function using R programming,\nto derive specific date and time related field by using base R and lubridate packages\nto perform data preparation task by using tidyr and dplyr packages.\n\n\n\nFor the purpose of this hands-on exercise, eventlog.csv file will be used. This data file consists of 199,999 rows of time-series cyber attack records by country.\n\n\n\nFirst, we will use the code chunk below to import eventlog.csv file into R environment and called the data frame as attacks.\n\nattacks &lt;- read_csv(\"data/eventlog.csv\")\n\n\n\n\nIt is always a good practice to examine the imported data frame before further analysis is performed.\nFor example, kable() can be used to review the structure of the imported data frame.\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are three columns, namely timestamp, source_country and tz.\ntimestamp field stores date-time values in POSIXct format. source_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code. tz field stores time zone of the source IP address.\n\n\n\nStep 1: Deriving weekday and hour of day fields\nBefore we can plot the calender heatmap, two new fields namely wkday and hour need to be derived. In this step, we will write a function to perform the task.\n\nmake_hr_wkday &lt;- function(ts, sc, tz) {\n  real_times &lt;- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt &lt;- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n}\n\n\n\n\n\n\n\nNote\n\n\n\n\nymd_hms() and hour() are from lubridate package, and\nweekdays() is a base R function.\n\n\n\nStep 2: Deriving the attacks tibble data frame\n\nwkday_levels &lt;- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\n\n\n\n\n\n\nNote\n\n\n\nBeside extracting the necessary data into attacks data frame, mutate() of dplyr package is used to convert wkday and hour fields into factor so they’ll be ordered when plotting\n\n\nTable below shows the tidy tibble table after processing.\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11\n\n\n\n\n\n\n\n\n\ngrouped &lt;- attacks %&gt;% \n  count(wkday, hour) %&gt;% \n  ungroup() %&gt;%\n  na.omit()\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte(base_family = \"Helvetica\") + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk\n\n\n\n\na tibble data table called grouped is derived by aggregating the attack by wkday and hour fields.\na new field called n is derived by using group_by() and count() functions.\nna.omit() is used to exclude missing value.\ngeom_tile() is used to plot tiles (grids) at each x and y position. color and size arguments are used to specify the border color and line size of the tiles.\ntheme_tufte() of ggthemes package is used to remove unnecessary chart junk. To learn which visual components of default ggplot2 have been excluded, you are encouraged to comment out this line to examine the default plot.\ncoord_equal() is used to ensure the plot will have an aspect ratio of 1:1.\nscale_fill_gradient() function is used to creates a two colour gradient (low-high).\n\n\n\nThen we can simply group the count by hour and wkday and plot it, since we know that we have values for every combination there’s no need to further preprocess the data.\n\n\n\nChallenge: Building multiple heatmaps for the top four countries with the highest number of attacks.\n\n\n\n\nStep 1: Deriving attack by country object\nIn order to identify the top 4 countries with the highest number of attacks, you are required to do the followings:\n\ncount the number of attacks by country,\ncalculate the percent of attackes by country, and\nsave the results in a tibble data frame.\n\n\nattacks_by_country &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\nStep 2: Preparing the tidy data frame\nIn this step, you are required to extract the attack records of the top 4 countries from attacks data frame and save the data in a new tibble data frame (i.e. top4_attacks).\n\ntop4 &lt;- attacks_by_country$source_country[1:4]\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  na.omit()\n\n\n\n\nStep 3: Plotting the Multiple Calender Heatmap by using ggplot2 package.\n\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, you will learn how to plot a cycle plot showing the time-series patterns and trend of visitor arrivals from Vietnam programmatically by using ggplot2 functions.\n\n\n\nFor the purpose of this hands-on exercise, arrivals_by_air.xlsx will be used.\nThe code chunk below imports arrivals_by_air.xlsx by using read_excel() of readxl package and save it as a tibble data frame called air.\n\nair &lt;- read_excel(\"data/arrivals_by_air.xlsx\")\n\n\n\n\nNext, two new fields called month and year are derived from Month-Year field.\n\nair$month &lt;- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year &lt;- year(ymd(air$`Month-Year`))\n\n\n\n\nNext, the code chunk below is use to extract data for the target country (i.e. Vietnam)\n\nVietnam &lt;- air %&gt;% \n  select(`Vietnam`, \n         month, \n         year) %&gt;%\n  filter(year &gt;= 2010)\n\n\n\n\nThe code chunk below uses group_by() and summarise() of dplyr to compute year average arrivals by month.\n\nhline.data &lt;- Vietnam %&gt;% \n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(`Vietnam`))\n\n\n\n\nThe code chunk below is used to plot the cycle plot as shown in Slide 12/23.\n\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") +\n  theme_tufte(base_family = \"Helvetica\")\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section we will learn how to plot a slopegraph by using R.\nBefore getting start, make sure that CGPfunctions has been installed and loaded onto R environment. Then, refer to Using newggslopegraph to learn more about the function. Lastly, read more about newggslopegraph() and its arguments by referring to this link.\n\n\nImport the rice data set into R environment by using the code chunk below.\n\nrice &lt;- read_csv(\"data/rice.csv\")\n\n\n\n\nNext, code chunk below will be used to plot a basic slopegraph as shown below.\n\nrice %&gt;% \n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Prepared by: Dr. Kam Tin Seong\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk\n\n\n\nFor effective data visualisation design, factor() is used convert the value type of Year field from numeric to factor.\n\n\n\n\n\n\n\n\n\nA taxonomy of temporal data visualization techniques\nEdward Tufte’s “Slopegraphs”\nIntroduction to Cycle Plots\nVisualizing Change: An Innovation in Time-Series Analysis\nThe Development of the Horizon Graph\n\n\n\n\n\nwhat is a slopegraph?\nSlopegraph Update\nTime on the Horizon\nTimeSearcher\nWhat is a slopegraph?\nDonahue, Rafe M.J. Fundamental Statistical Concepts in Presenting Data: Principles for Constructing Better Graphics. This article provide a real world example of building truthful and functional time series graph.\nHockey stick graph at wiki.\nMichael E. Mann, Raymond S. Bradley, Malcolm K. Hughes (1999) “Northern hemisphere temperatures during the past millennium: Inferences, uncertainties, and limitations”. Geophysical Research Letters, Vol. 26, No. pp. 759-762.\nThe Guardian (2010) “Hockey stick graph took pride of place in IPCC report, despite doubts”."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#learning-outcome",
    "title": "Hands-on Exercise 06",
    "section": "",
    "text": "By the end of this hands-on exercise we will be able create the followings data visualisation by using R packages:\n\nplotting a calender heatmap by using ggplot2 functions,\nplotting a cycle plot by using ggplot2 function,\nplotting a slopegraph\nplotting a horizon chart"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#do-it-yourself",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#do-it-yourself",
    "title": "Hands-on Exercise 06",
    "section": "",
    "text": "Write a code chunk to check, install and launch the following R packages: scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table and tidyverse.\n\n\nShow the code\n\n\npacman::p_load(scales, viridis, lubridate, ggthemes,\n               gridExtra, readxl, knitr, data.table,\n               CGPfunctions, ggHoriPlot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-calendar-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-calendar-heatmap",
    "title": "Hands-on Exercise 06",
    "section": "",
    "text": "In this section, we will learn how to plot a calender heatmap programmatically by using ggplot2 package.\n\nBy the end of this section, we will be able to:\n\nplot a calender heatmap by using ggplot2 functions and extension,\nto write function using R programming,\nto derive specific date and time related field by using base R and lubridate packages\nto perform data preparation task by using tidyr and dplyr packages.\n\n\n\nFor the purpose of this hands-on exercise, eventlog.csv file will be used. This data file consists of 199,999 rows of time-series cyber attack records by country.\n\n\n\nFirst, we will use the code chunk below to import eventlog.csv file into R environment and called the data frame as attacks.\n\nattacks &lt;- read_csv(\"data/eventlog.csv\")\n\n\n\n\nIt is always a good practice to examine the imported data frame before further analysis is performed.\nFor example, kable() can be used to review the structure of the imported data frame.\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are three columns, namely timestamp, source_country and tz.\ntimestamp field stores date-time values in POSIXct format. source_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code. tz field stores time zone of the source IP address.\n\n\n\nStep 1: Deriving weekday and hour of day fields\nBefore we can plot the calender heatmap, two new fields namely wkday and hour need to be derived. In this step, we will write a function to perform the task.\n\nmake_hr_wkday &lt;- function(ts, sc, tz) {\n  real_times &lt;- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt &lt;- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n}\n\n\n\n\n\n\n\nNote\n\n\n\n\nymd_hms() and hour() are from lubridate package, and\nweekdays() is a base R function.\n\n\n\nStep 2: Deriving the attacks tibble data frame\n\nwkday_levels &lt;- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\n\n\n\n\n\n\nNote\n\n\n\nBeside extracting the necessary data into attacks data frame, mutate() of dplyr package is used to convert wkday and hour fields into factor so they’ll be ordered when plotting\n\n\nTable below shows the tidy tibble table after processing.\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11\n\n\n\n\n\n\n\n\n\ngrouped &lt;- attacks %&gt;% \n  count(wkday, hour) %&gt;% \n  ungroup() %&gt;%\n  na.omit()\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte(base_family = \"Helvetica\") + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk\n\n\n\n\na tibble data table called grouped is derived by aggregating the attack by wkday and hour fields.\na new field called n is derived by using group_by() and count() functions.\nna.omit() is used to exclude missing value.\ngeom_tile() is used to plot tiles (grids) at each x and y position. color and size arguments are used to specify the border color and line size of the tiles.\ntheme_tufte() of ggthemes package is used to remove unnecessary chart junk. To learn which visual components of default ggplot2 have been excluded, you are encouraged to comment out this line to examine the default plot.\ncoord_equal() is used to ensure the plot will have an aspect ratio of 1:1.\nscale_fill_gradient() function is used to creates a two colour gradient (low-high).\n\n\n\nThen we can simply group the count by hour and wkday and plot it, since we know that we have values for every combination there’s no need to further preprocess the data.\n\n\n\nChallenge: Building multiple heatmaps for the top four countries with the highest number of attacks.\n\n\n\n\nStep 1: Deriving attack by country object\nIn order to identify the top 4 countries with the highest number of attacks, you are required to do the followings:\n\ncount the number of attacks by country,\ncalculate the percent of attackes by country, and\nsave the results in a tibble data frame.\n\n\nattacks_by_country &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\nStep 2: Preparing the tidy data frame\nIn this step, you are required to extract the attack records of the top 4 countries from attacks data frame and save the data in a new tibble data frame (i.e. top4_attacks).\n\ntop4 &lt;- attacks_by_country$source_country[1:4]\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  na.omit()\n\n\n\n\nStep 3: Plotting the Multiple Calender Heatmap by using ggplot2 package.\n\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-cycle-plot",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-cycle-plot",
    "title": "Hands-on Exercise 06",
    "section": "",
    "text": "In this section, you will learn how to plot a cycle plot showing the time-series patterns and trend of visitor arrivals from Vietnam programmatically by using ggplot2 functions.\n\n\n\nFor the purpose of this hands-on exercise, arrivals_by_air.xlsx will be used.\nThe code chunk below imports arrivals_by_air.xlsx by using read_excel() of readxl package and save it as a tibble data frame called air.\n\nair &lt;- read_excel(\"data/arrivals_by_air.xlsx\")\n\n\n\n\nNext, two new fields called month and year are derived from Month-Year field.\n\nair$month &lt;- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year &lt;- year(ymd(air$`Month-Year`))\n\n\n\n\nNext, the code chunk below is use to extract data for the target country (i.e. Vietnam)\n\nVietnam &lt;- air %&gt;% \n  select(`Vietnam`, \n         month, \n         year) %&gt;%\n  filter(year &gt;= 2010)\n\n\n\n\nThe code chunk below uses group_by() and summarise() of dplyr to compute year average arrivals by month.\n\nhline.data &lt;- Vietnam %&gt;% \n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(`Vietnam`))\n\n\n\n\nThe code chunk below is used to plot the cycle plot as shown in Slide 12/23.\n\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") +\n  theme_tufte(base_family = \"Helvetica\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-slopegraph",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-slopegraph",
    "title": "Hands-on Exercise 06",
    "section": "",
    "text": "In this section we will learn how to plot a slopegraph by using R.\nBefore getting start, make sure that CGPfunctions has been installed and loaded onto R environment. Then, refer to Using newggslopegraph to learn more about the function. Lastly, read more about newggslopegraph() and its arguments by referring to this link.\n\n\nImport the rice data set into R environment by using the code chunk below.\n\nrice &lt;- read_csv(\"data/rice.csv\")\n\n\n\n\nNext, code chunk below will be used to plot a basic slopegraph as shown below.\n\nrice %&gt;% \n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Prepared by: Dr. Kam Tin Seong\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk\n\n\n\nFor effective data visualisation design, factor() is used convert the value type of Year field from numeric to factor."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html",
    "href": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "In this exercise, we explore Mini-Challenge 2 (MC2) from the VAST Challenge 2025, which centers on the theme of conflict over societal change within the fictional island nation of Oceanus.\nOceanus is a historically fishing-based island economy that is now rapidly transforming due to the growth of tourism. This economic shift has sparked conflict between two influential groups:\n\nFishing is Living and Heritage (FILAH) – advocates for the preservation of Oceanus’s fishing heritage and industry.\nTourism Raises OceanUs Together (TROUT) – champions the modern tourism economy as the path to prosperity.\n\nBoth groups claim the Commission on Overseeing the Economic Future of Oceanus (COOTEFOO) of bias in its support of development initiatives. As data analysts, our goal is to reconstruct an unbiased understanding of COOTEFOO’s activities and assess the validity of these accusations using data from both camps and a more complete combined dataset.\n\n\n\nBased on the datasets that TROUT & FILAH have provided, use visual analytics to determine if each group’s accusations are supported by their own record set. In other words, develop a visualization to highlight bias (if present) in TROUT & FILAHS datasets. Is there evidence of bias in the COOTEFOO member actions in either dataset?\nAs a journalist, Ms. Moray would like a more complete picture of the COOTEFOO’s actions and activities. She has arranged to combine the data provided by TROUT and FILAH into a single knowledge graph along with additional records. Design visual analytics approaches for this combined knowledge graph to see how members of COOTEFOO spend their time. Is the committee as a whole biased? Provide visual evidence for your conclusions.\nThe TROUT and FILAH datasets are incomplete. Use your visualizations to compare and contrast conclusions drawn from the TROUT and FILAH datasets separately with behaviors in the whole dataset. Are the accusations of TROUT strengthened, weakened or unchanged when taken in context of the whole dataset?\nDesign a visualization that allows Ms. Moray to pick a person and highlight the differences in that person’s behavior as illustrated through the different datasets. Focus on the contrast in the story each dataset tells.\n\nPick at least one COOTEFOO member accused by TROUT. Illustrate how your understanding of their activities changed when using the more complete dataset.\nWhat are the key pieces of evidence missing from the original TROUT data that most influenced the change in judgement.\nWhose behaviors are most impacted by sampling bias when looking at the FILAH dataset in context of the other data?\nIllustrate the bias of the FILAH data in the context of the whole dataset.\n\n\n\n\n\nThe following packages are used to support data wrangling, spatial analysis, visualization, and network exploration:\n\ntidyverse: Core data science suite (dplyr, ggplot2, etc.)\njsonlite: Read/write JSON structured data\nsf: Handle and visualize spatial vector data (simple features)\nSmartEDA: Generate automated EDA reports\ntidygraph: Tidy framework for network/graph data\nggraph: Network plotting built on ggplot2\nggrepel: Prevent overlapping text labels in plots\ndata.table: High-performance data manipulation\nDT: Render interactive tables for web display\nvisNetwork: Interactive visualizations of graph objects\ntidyr: Tools for tidying and reshaping data\nnaniar: Explore and visualize missing data\nskimr: Compact summaries for data frames\nggridges: Create ridgeline density plots\nggalt: Additional geoms and stats for ggplot2\n\n\n\nShow the code\n\n\npacman::p_load(\n  tidyverse, jsonlite, sf, SmartEDA, tidygraph, ggrepel, ggraph,\n  data.table, DT, visNetwork, tidyr, naniar, skimr,\n  ggplot2, ggridges, ggalt\n)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#readings",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#readings",
    "title": "Hands-on Exercise 06",
    "section": "",
    "text": "A taxonomy of temporal data visualization techniques\nEdward Tufte’s “Slopegraphs”\nIntroduction to Cycle Plots\nVisualizing Change: An Innovation in Time-Series Analysis\nThe Development of the Horizon Graph\n\n\n\n\n\nwhat is a slopegraph?\nSlopegraph Update\nTime on the Horizon\nTimeSearcher\nWhat is a slopegraph?\nDonahue, Rafe M.J. Fundamental Statistical Concepts in Presenting Data: Principles for Constructing Better Graphics. This article provide a real world example of building truthful and functional time series graph.\nHockey stick graph at wiki.\nMichael E. Mann, Raymond S. Bradley, Malcolm K. Hughes (1999) “Northern hemisphere temperatures during the past millennium: Inferences, uncertainties, and limitations”. Geophysical Research Letters, Vol. 26, No. pp. 759-762.\nThe Guardian (2010) “Hockey stick graph took pride of place in IPCC report, despite doubts”."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#getting-started",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "The following packages are used to support data wrangling, spatial analysis, visualization, and network exploration:\n\ntidyverse: Core data science suite (dplyr, ggplot2, etc.)\njsonlite: Read/write JSON structured data\nsf: Handle and visualize spatial vector data (simple features)\nSmartEDA: Generate automated EDA reports\ntidygraph: Tidy framework for network/graph data\nggraph: Network plotting built on ggplot2\nggrepel: Prevent overlapping text labels in plots\ndata.table: High-performance data manipulation\nDT: Render interactive tables for web display\nvisNetwork: Interactive visualizations of graph objects\ntidyr: Tools for tidying and reshaping data\nnaniar: Explore and visualize missing data\nskimr: Compact summaries for data frames\nggridges: Create ridgeline density plots\nggalt: Additional geoms and stats for ggplot2\n\n\n\nShow the code\n\n\npacman::p_load(\n  tidyverse, jsonlite, sf, SmartEDA, tidygraph, ggrepel, ggraph,\n  data.table, DT, visNetwork, tidyr, naniar, skimr,\n  ggplot2, ggridges, ggalt\n)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#importing-knowledge-graph-data",
    "href": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#importing-knowledge-graph-data",
    "title": "Take-home Exercise 02",
    "section": "2 Importing Knowledge Graph Data",
    "text": "2 Importing Knowledge Graph Data\nFirst, we will analyze the 3 files by importing them into R using the code chunk below:\n\nfilah &lt;- fromJSON(\"data/FILAH.json\")\ntrout &lt;- fromJSON(\"data/TROUT.json\")\njournalist &lt;- fromJSON(\"data/journalist.json\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#data-preparation",
    "href": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#data-preparation",
    "title": "Take-home Exercise 02",
    "section": "3 Data Preparation",
    "text": "3 Data Preparation\n\n3.1 Inspecting Knowledge Graph Structure\nThe datasets contain graph data, where nodes can be accessed via nodes and edges via links. The datasets have a lot of columns but we will only filter the relevant columns in this analysis.\n\nFILAHTROUTJOURNALIST\n\n\n\nglimpse(filah)\n\nList of 5\n $ directed  : logi TRUE\n $ multigraph: logi TRUE\n $ graph     : Named list()\n $ nodes     :'data.frame': 396 obs. of  17 variables:\n  ..$ type       : chr [1:396] \"meeting\" \"meeting\" \"meeting\" \"meeting\" ...\n  ..$ date       : chr [1:396] \"Meeting 1\" \"Meeting 2\" \"Meeting 3\" \"Meeting 4\" ...\n  ..$ label      : chr [1:396] \"Meeting 1\" \"Meeting 2\" \"Meeting 3\" \"Meeting 4\" ...\n  ..$ id         : chr [1:396] \"Meeting_1\" \"Meeting_2\" \"Meeting_3\" \"Meeting_4\" ...\n  ..$ name       : chr [1:396] NA NA NA NA ...\n  ..$ role       : chr [1:396] NA NA NA NA ...\n  ..$ short_topic: chr [1:396] NA NA NA NA ...\n  ..$ long_topic : chr [1:396] NA NA NA NA ...\n  ..$ short_title: chr [1:396] NA NA NA NA ...\n  ..$ long_title : chr [1:396] NA NA NA NA ...\n  ..$ plan_type  : chr [1:396] NA NA NA NA ...\n  ..$ lat        : num [1:396] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ lon        : num [1:396] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ zone       : chr [1:396] NA NA NA NA ...\n  ..$ zone_detail: chr [1:396] NA NA NA NA ...\n  ..$ start      : chr [1:396] NA NA NA NA ...\n  ..$ end        : chr [1:396] NA NA NA NA ...\n $ links     :'data.frame': 765 obs. of  9 variables:\n  ..$ role     : chr [1:765] \"part_of\" \"part_of\" \"part_of\" \"part_of\" ...\n  ..$ source   : chr [1:765] \"Meeting_1\" \"Meeting_1\" \"Meeting_1\" \"Meeting_1\" ...\n  ..$ target   : chr [1:765] \"fish_vacuum_Meeting_1_Introduction_Discussion\" \"fish_vacuum_Meeting_1_Introduction\" \"seafood_festival_Meeting_1_Discussion\" \"seafood_festival_Meeting_1_Feasibility\" ...\n  ..$ key      : int [1:765] 0 0 0 0 0 0 0 0 0 0 ...\n  ..$ sentiment: num [1:765] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ reason   : chr [1:765] NA NA NA NA ...\n  ..$ industry :List of 765\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. .. [list output truncated]\n  ..$ status   : chr [1:765] NA NA NA NA ...\n  ..$ time     : chr [1:765] NA NA NA NA ...\n\n\n\n\n\nglimpse(trout)\n\nList of 5\n $ directed  : logi TRUE\n $ multigraph: logi TRUE\n $ graph     : Named list()\n $ nodes     :'data.frame': 164 obs. of  17 variables:\n  ..$ type       : chr [1:164] \"plan\" \"plan\" \"plan\" \"meeting\" ...\n  ..$ short_title: chr [1:164] \"marine_life_deck_Meeting_12_Environmental_Impact_Report\" \"low_volume_crane_Meeting_8_Report\" \"deep_fishing_dock_Meeting_3_Maintenance_Plan\" NA ...\n  ..$ long_title : chr [1:164] \"Present findings from the environmental impact study\" \"Report on travel and submit letter of support for low-volume unload crane\" \"Discuss maintenance plan and designate travel representatives\" NA ...\n  ..$ plan_type  : chr [1:164] \"Report\" \"report\" \"proposal\" NA ...\n  ..$ label      : chr [1:164] \"marine_life_deck_Meeting_12_Environmental_Impact_Report\" \"low_volume_crane_Meeting_8_Report\" \"deep_fishing_dock_Meeting_3_Maintenance_Plan\" \"Meeting 16\" ...\n  ..$ id         : chr [1:164] \"marine_life_deck_Meeting_12_Environmental_Impact_Report\" \"low_volume_crane_Meeting_8_Report\" \"deep_fishing_dock_Meeting_3_Maintenance_Plan\" \"Meeting_16\" ...\n  ..$ date       : chr [1:164] NA NA NA \"Meeting 16\" ...\n  ..$ short_topic: chr [1:164] NA NA NA NA ...\n  ..$ long_topic : chr [1:164] NA NA NA NA ...\n  ..$ lat        : num [1:164] NA NA NA NA NA ...\n  ..$ lon        : num [1:164] NA NA NA NA NA ...\n  ..$ zone       : chr [1:164] NA NA NA NA ...\n  ..$ zone_detail: chr [1:164] NA NA NA NA ...\n  ..$ name       : chr [1:164] NA NA NA NA ...\n  ..$ role       : chr [1:164] NA NA NA NA ...\n  ..$ start      : chr [1:164] NA NA NA NA ...\n  ..$ end        : chr [1:164] NA NA NA NA ...\n $ links     :'data.frame': 378 obs. of  9 variables:\n  ..$ role     : chr [1:378] \"plan\" \"participant\" \"plan\" \"participant\" ...\n  ..$ source   : chr [1:378] \"marine_life_deck_Meeting_12_Environmental_Impact_Report\" \"marine_life_deck_Meeting_12_Environmental_Impact_Report\" \"low_volume_crane_Meeting_8_Report\" \"low_volume_crane_Meeting_8_Report\" ...\n  ..$ target   : chr [1:378] \"marine_life_deck\" \"Teddy Goldstein\" \"low_volume_crane\" \"Seal\" ...\n  ..$ key      : int [1:378] 0 0 0 0 0 0 0 0 0 0 ...\n  ..$ sentiment: num [1:378] NA -0.5 NA 0.1 NA NA NA NA NA NA ...\n  ..$ reason   : chr [1:378] NA \"Prefers resources to be allocated toward the fishing industry.\" NA \"Recognizes the crane's benefit to small-scale operations.\" ...\n  ..$ industry :List of 378\n  .. ..$ : NULL\n  .. ..$ : chr \"tourism\"\n  .. ..$ : NULL\n  .. ..$ : chr \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr \"tourism\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"large vessel\" \"small vessel\"\n  .. ..$ : chr [1:2] \"large vessel\" \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"large vessel\" \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr \"tourism\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"large vessel\" \"small vessel\"\n  .. ..$ : chr [1:2] \"large vessel\" \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr \"large vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"large vessel\" \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"large vessel\" \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : chr \"tourism\"\n  .. ..$ : NULL\n  .. ..$ : list()\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr \"tourism\"\n  .. ..$ : NULL\n  .. ..$ : chr \"large vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr \"tourism\"\n  .. ..$ : chr \"tourism\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr \"large vessel\"\n  .. ..$ : NULL\n  .. ..$ : chr \"tourism\"\n  .. ..$ : chr \"tourism\"\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"large vessel\" \"small vessel\"\n  .. ..$ : chr [1:2] \"large vessel\" \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : list()\n  .. .. [list output truncated]\n  ..$ status   : chr [1:378] NA NA NA NA ...\n  ..$ time     : chr [1:378] NA NA NA NA ...\n\n\n\n\n\nglimpse(journalist)\n\nList of 5\n $ directed  : logi TRUE\n $ multigraph: logi TRUE\n $ graph     : Named list()\n $ nodes     :'data.frame': 740 obs. of  17 variables:\n  ..$ type       : chr [1:740] \"meeting\" \"meeting\" \"meeting\" \"meeting\" ...\n  ..$ date       : chr [1:740] \"Meeting 1\" \"Meeting 2\" \"Meeting 3\" \"Meeting 4\" ...\n  ..$ label      : chr [1:740] \"Meeting 1\" \"Meeting 2\" \"Meeting 3\" \"Meeting 4\" ...\n  ..$ id         : chr [1:740] \"Meeting_1\" \"Meeting_2\" \"Meeting_3\" \"Meeting_4\" ...\n  ..$ name       : chr [1:740] NA NA NA NA ...\n  ..$ role       : chr [1:740] NA NA NA NA ...\n  ..$ short_topic: chr [1:740] NA NA NA NA ...\n  ..$ long_topic : chr [1:740] NA NA NA NA ...\n  ..$ short_title: chr [1:740] NA NA NA NA ...\n  ..$ long_title : chr [1:740] NA NA NA NA ...\n  ..$ plan_type  : chr [1:740] NA NA NA NA ...\n  ..$ lat        : num [1:740] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ lon        : num [1:740] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ zone       : chr [1:740] NA NA NA NA ...\n  ..$ zone_detail: chr [1:740] NA NA NA NA ...\n  ..$ start      : chr [1:740] NA NA NA NA ...\n  ..$ end        : chr [1:740] NA NA NA NA ...\n $ links     :'data.frame': 2436 obs. of  9 variables:\n  ..$ role     : chr [1:2436] \"part_of\" \"part_of\" \"part_of\" \"part_of\" ...\n  ..$ source   : chr [1:2436] \"Meeting_1\" \"Meeting_1\" \"Meeting_1\" \"Meeting_1\" ...\n  ..$ target   : chr [1:2436] \"fish_vacuum_Meeting_1_Introduction_Discussion\" \"fish_vacuum_Meeting_1_Introduction\" \"seafood_festival_Meeting_1_Discussion\" \"seafood_festival_Meeting_1_Feasibility\" ...\n  ..$ key      : int [1:2436] 0 0 0 0 0 0 0 0 0 0 ...\n  ..$ sentiment: num [1:2436] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ reason   : chr [1:2436] NA NA NA NA ...\n  ..$ industry :List of 2436\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. .. [list output truncated]\n  ..$ status   : chr [1:2436] NA NA NA NA ...\n  ..$ time     : chr [1:2436] NA NA NA NA ...\n\n\n\n\n\n\n\n3.2 Extracting the edges and nodes tables\nNext, as_tibble() of tibble package package is used to extract the nodes and links tibble data frames from each of the dataframe into two separate tibble dataframes called nodes and edges respectively.\n\nFILAHTROUTJOURNALIST\n\n\n\nfilah_nodes &lt;- as_tibble(filah$nodes)\nfilah_edges &lt;- as_tibble(filah$links)\n\n\n\n\ntrout_nodes &lt;- as_tibble(trout$nodes)\ntrout_edges &lt;- as_tibble(trout$links)\n\n\n\n\njournalist_nodes &lt;- as_tibble(journalist$nodes)\njournalist_edges &lt;- as_tibble(journalist$links)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#initial-eda",
    "href": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#initial-eda",
    "title": "Take-home Exercise 02",
    "section": "4 Initial EDA",
    "text": "4 Initial EDA\n\n4.1 Frequency Distribution\n\n4.1.1 Nodes Categorical Frequency Distribution\nIn the code chunk below, ExpCatViz() of SmartEDA package is used to reveal the frequency distribution of all categorical fields in each nodes tibble dataframe.\n\nFILAHTROUTJOURNALIST\n\n\n\nExpCatViz(data=filah_nodes,\n          col=\"lightblue\")\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to Note: Summary of Node Attribute Exploration\n\n\n\n\nNode Type (type)\n\nNearly half of all nodes (48%) are trip nodes, indicating a strong focus on travel records.\n\nOther significant types include discussion (15%), place (15%), and plan (10%).\n\n3% of node types are missing (NA), which may require cleaning or tagging.\n\nRole (role)\n\n99% of nodes have missing role values, making it difficult to analyze by committee roles.\n\nOnly a small portion are labeled as Member or Committee Chair.\n\nRole information may need to be imputed, enriched, or excluded depending on analysis goals.\n\nPlan Type (plan_type)\n\n90% of plan nodes lack plan_type values.\n\nOnly a small percentage are labeled as Travel, Proposal, etc.\n\nThis limits the usefulness of this variable unless cleaned or externally enriched.\n\nZone (zone)\n\n82% of place nodes are missing zone classification.\n\nMost of the known zones are commercial (14%) with small amounts of industrial and tourism.\n\nThis hinders any meaningful zoning or geographic segmentation.\n\nZone Detail (zone_detail)\n\n98% of zone_detail entries are missing.\n\nOnly 2% are labeled restaurant.\n\nThis variable may not be useful without enrichment from other sources.\n\n\n\n\n\n\n\nExpCatViz(data=trout_nodes,\n          col=\"lightblue\")\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n\n\n\nExpCatViz(data=journalist_nodes,\n          col=\"lightblue\")\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1.2 Edges Categorical Frequency Distribution\nWe will also be using code chunk below that uses ExpCATViz() of SmartEDA package to reveal the frequency distribution of all categorical fields in the edges tibble dataframe.\n\nFILAHTROUTJOURNALIST\n\n\n\nExpCatViz(data=filah_edges,\n          col=\"lightblue\")\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n\n\nExpCatViz(data=trout_edges,\n          col=\"lightblue\")\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n\n\nExpCatViz(data=journalist_edges,\n          col=\"lightblue\")\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1.3 Nodes Numerical Frequency Distribution\nNext, we will use the code chunk below that uses ExpNumViz() of SmartEDA package to reveal the frequency distribution of all numerical fields in the nodes tibble dataframe.\n\nFILAHTROUTJOURNALIST\n\n\n\nExpNumViz(filah_nodes)\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\nExpNumViz(trout_nodes)\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\nExpNumViz(journalist_nodes)\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1.4 Nodes Numerical Frequency Distribution\nBelow, we will use the code chunk below that uses ExpNumViz() of SmartEDA package to reveal the frequency distribution of all numerical fields in the edges tibble dataframe.\n\nFILAHTROUTJOURNALIST\n\n\n\nExpNumViz(filah_edges)\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n\n\nExpNumViz(trout_edges)\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n\n\nExpNumViz(journalist_edges)\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n\n\nNode Type Distribution\n\nfilah_nodes %&gt;%\n  count(type, sort = TRUE) %&gt;%\n  ggplot(aes(x = reorder(type, n), y = n, fill = type)) +\n  geom_col() +\n  coord_flip() +\n  labs(title = \"Node Type Distribution in FILAH\", x = \"Node Type\", y = \"Count\") +\n  theme_minimal()"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#data-cleaning-and-wrangling",
    "href": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#data-cleaning-and-wrangling",
    "title": "Take-home Exercise 02",
    "section": "5 Data Cleaning and Wrangling",
    "text": "5 Data Cleaning and Wrangling\n\n5.1 Cleaning and wrangling nodes\n\nfilah_nodes_cleaned &lt;- filah_nodes %&gt;%\n  mutate(id = as.character(id)) %&gt;%\n  filter(!is.na(id)) %&gt;%\n  distinct(id, .keep_all = TRUE) %&gt;%\n  select(id, type, label)   \n\n\n\n5.2 Cleaning and wrangling edges\n\nfilah_edges_cleaned &lt;- filah_edges %&gt;%\n  rename(from = source, to = target) %&gt;%\n  mutate(across(c(from, to), as.character)) %&gt;%\n  filter(from %in% filah_nodes_cleaned$id, to %in% filah_nodes_cleaned$id)\n\n# Remove problematic columns from edge table for graph building\nfilah_edges_min &lt;- filah_edges_cleaned %&gt;%\n  select(from, to, role)  # Only basic fields needed for graph structure\n\n\n\n5.3 Building the tidygraph object\n\nfilah_graph &lt;- tbl_graph(\n  nodes = filah_nodes_cleaned, \n  edges = filah_edges_min, \n  directed = TRUE)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#visualising-the-knowledge-graph",
    "href": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#visualising-the-knowledge-graph",
    "title": "Take-home Exercise 02",
    "section": "6 Visualising the knowledge graph",
    "text": "6 Visualising the knowledge graph\nIn this section, we will use ggraph’s functions to visualise and analyse the graph object.\n\nset.seed(1234)\n\nIn the code chunk below, ggraph functions are used to create the whole graph.\n\nggraph(filah_graph, \n       layout = \"fr\") +\n  geom_edge_link(alpha = 0.3, \n                 colour = \"gray\") +\n  geom_node_point(aes(color = `type`), \n                  size = 4) +\n  geom_node_text(aes(label = type), \n                 repel = TRUE, \n                 size = 2.5) +\n  theme_void()"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#data-overview",
    "href": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#data-overview",
    "title": "Take-home Exercise 02",
    "section": "3 Data Overview",
    "text": "3 Data Overview\n\n3.1 Inspecting Knowledge Graph Structure\nThe datasets contain graph data, where nodes can be accessed via nodes and edges via links. The datasets have a lot of columns but we will only filter the relevant data during wrangling step.\n\nFILAHTROUTJOURNALIST\n\n\n\nglimpse(filah)\n\nList of 5\n $ directed  : logi TRUE\n $ multigraph: logi TRUE\n $ graph     : Named list()\n $ nodes     :'data.frame': 396 obs. of  17 variables:\n  ..$ type       : chr [1:396] \"meeting\" \"meeting\" \"meeting\" \"meeting\" ...\n  ..$ date       : chr [1:396] \"Meeting 1\" \"Meeting 2\" \"Meeting 3\" \"Meeting 4\" ...\n  ..$ label      : chr [1:396] \"Meeting 1\" \"Meeting 2\" \"Meeting 3\" \"Meeting 4\" ...\n  ..$ id         : chr [1:396] \"Meeting_1\" \"Meeting_2\" \"Meeting_3\" \"Meeting_4\" ...\n  ..$ name       : chr [1:396] NA NA NA NA ...\n  ..$ role       : chr [1:396] NA NA NA NA ...\n  ..$ short_topic: chr [1:396] NA NA NA NA ...\n  ..$ long_topic : chr [1:396] NA NA NA NA ...\n  ..$ short_title: chr [1:396] NA NA NA NA ...\n  ..$ long_title : chr [1:396] NA NA NA NA ...\n  ..$ plan_type  : chr [1:396] NA NA NA NA ...\n  ..$ lat        : num [1:396] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ lon        : num [1:396] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ zone       : chr [1:396] NA NA NA NA ...\n  ..$ zone_detail: chr [1:396] NA NA NA NA ...\n  ..$ start      : chr [1:396] NA NA NA NA ...\n  ..$ end        : chr [1:396] NA NA NA NA ...\n $ links     :'data.frame': 765 obs. of  9 variables:\n  ..$ role     : chr [1:765] \"part_of\" \"part_of\" \"part_of\" \"part_of\" ...\n  ..$ source   : chr [1:765] \"Meeting_1\" \"Meeting_1\" \"Meeting_1\" \"Meeting_1\" ...\n  ..$ target   : chr [1:765] \"fish_vacuum_Meeting_1_Introduction_Discussion\" \"fish_vacuum_Meeting_1_Introduction\" \"seafood_festival_Meeting_1_Discussion\" \"seafood_festival_Meeting_1_Feasibility\" ...\n  ..$ key      : int [1:765] 0 0 0 0 0 0 0 0 0 0 ...\n  ..$ sentiment: num [1:765] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ reason   : chr [1:765] NA NA NA NA ...\n  ..$ industry :List of 765\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. .. [list output truncated]\n  ..$ status   : chr [1:765] NA NA NA NA ...\n  ..$ time     : chr [1:765] NA NA NA NA ...\n\n\n\n\n\nglimpse(trout)\n\nList of 5\n $ directed  : logi TRUE\n $ multigraph: logi TRUE\n $ graph     : Named list()\n $ nodes     :'data.frame': 164 obs. of  17 variables:\n  ..$ type       : chr [1:164] \"plan\" \"plan\" \"plan\" \"meeting\" ...\n  ..$ short_title: chr [1:164] \"marine_life_deck_Meeting_12_Environmental_Impact_Report\" \"low_volume_crane_Meeting_8_Report\" \"deep_fishing_dock_Meeting_3_Maintenance_Plan\" NA ...\n  ..$ long_title : chr [1:164] \"Present findings from the environmental impact study\" \"Report on travel and submit letter of support for low-volume unload crane\" \"Discuss maintenance plan and designate travel representatives\" NA ...\n  ..$ plan_type  : chr [1:164] \"Report\" \"report\" \"proposal\" NA ...\n  ..$ label      : chr [1:164] \"marine_life_deck_Meeting_12_Environmental_Impact_Report\" \"low_volume_crane_Meeting_8_Report\" \"deep_fishing_dock_Meeting_3_Maintenance_Plan\" \"Meeting 16\" ...\n  ..$ id         : chr [1:164] \"marine_life_deck_Meeting_12_Environmental_Impact_Report\" \"low_volume_crane_Meeting_8_Report\" \"deep_fishing_dock_Meeting_3_Maintenance_Plan\" \"Meeting_16\" ...\n  ..$ date       : chr [1:164] NA NA NA \"Meeting 16\" ...\n  ..$ short_topic: chr [1:164] NA NA NA NA ...\n  ..$ long_topic : chr [1:164] NA NA NA NA ...\n  ..$ lat        : num [1:164] NA NA NA NA NA ...\n  ..$ lon        : num [1:164] NA NA NA NA NA ...\n  ..$ zone       : chr [1:164] NA NA NA NA ...\n  ..$ zone_detail: chr [1:164] NA NA NA NA ...\n  ..$ name       : chr [1:164] NA NA NA NA ...\n  ..$ role       : chr [1:164] NA NA NA NA ...\n  ..$ start      : chr [1:164] NA NA NA NA ...\n  ..$ end        : chr [1:164] NA NA NA NA ...\n $ links     :'data.frame': 378 obs. of  9 variables:\n  ..$ role     : chr [1:378] \"plan\" \"participant\" \"plan\" \"participant\" ...\n  ..$ source   : chr [1:378] \"marine_life_deck_Meeting_12_Environmental_Impact_Report\" \"marine_life_deck_Meeting_12_Environmental_Impact_Report\" \"low_volume_crane_Meeting_8_Report\" \"low_volume_crane_Meeting_8_Report\" ...\n  ..$ target   : chr [1:378] \"marine_life_deck\" \"Teddy Goldstein\" \"low_volume_crane\" \"Seal\" ...\n  ..$ key      : int [1:378] 0 0 0 0 0 0 0 0 0 0 ...\n  ..$ sentiment: num [1:378] NA -0.5 NA 0.1 NA NA NA NA NA NA ...\n  ..$ reason   : chr [1:378] NA \"Prefers resources to be allocated toward the fishing industry.\" NA \"Recognizes the crane's benefit to small-scale operations.\" ...\n  ..$ industry :List of 378\n  .. ..$ : NULL\n  .. ..$ : chr \"tourism\"\n  .. ..$ : NULL\n  .. ..$ : chr \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr \"tourism\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"large vessel\" \"small vessel\"\n  .. ..$ : chr [1:2] \"large vessel\" \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"large vessel\" \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr \"tourism\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"large vessel\" \"small vessel\"\n  .. ..$ : chr [1:2] \"large vessel\" \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr \"large vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"large vessel\" \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"large vessel\" \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : chr \"tourism\"\n  .. ..$ : NULL\n  .. ..$ : list()\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr \"tourism\"\n  .. ..$ : NULL\n  .. ..$ : chr \"large vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr \"tourism\"\n  .. ..$ : chr \"tourism\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr \"large vessel\"\n  .. ..$ : NULL\n  .. ..$ : chr \"tourism\"\n  .. ..$ : chr \"tourism\"\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"large vessel\" \"small vessel\"\n  .. ..$ : chr [1:2] \"large vessel\" \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : list()\n  .. .. [list output truncated]\n  ..$ status   : chr [1:378] NA NA NA NA ...\n  ..$ time     : chr [1:378] NA NA NA NA ...\n\n\n\n\n\nglimpse(journalist)\n\nList of 5\n $ directed  : logi TRUE\n $ multigraph: logi TRUE\n $ graph     : Named list()\n $ nodes     :'data.frame': 740 obs. of  17 variables:\n  ..$ type       : chr [1:740] \"meeting\" \"meeting\" \"meeting\" \"meeting\" ...\n  ..$ date       : chr [1:740] \"Meeting 1\" \"Meeting 2\" \"Meeting 3\" \"Meeting 4\" ...\n  ..$ label      : chr [1:740] \"Meeting 1\" \"Meeting 2\" \"Meeting 3\" \"Meeting 4\" ...\n  ..$ id         : chr [1:740] \"Meeting_1\" \"Meeting_2\" \"Meeting_3\" \"Meeting_4\" ...\n  ..$ name       : chr [1:740] NA NA NA NA ...\n  ..$ role       : chr [1:740] NA NA NA NA ...\n  ..$ short_topic: chr [1:740] NA NA NA NA ...\n  ..$ long_topic : chr [1:740] NA NA NA NA ...\n  ..$ short_title: chr [1:740] NA NA NA NA ...\n  ..$ long_title : chr [1:740] NA NA NA NA ...\n  ..$ plan_type  : chr [1:740] NA NA NA NA ...\n  ..$ lat        : num [1:740] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ lon        : num [1:740] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ zone       : chr [1:740] NA NA NA NA ...\n  ..$ zone_detail: chr [1:740] NA NA NA NA ...\n  ..$ start      : chr [1:740] NA NA NA NA ...\n  ..$ end        : chr [1:740] NA NA NA NA ...\n $ links     :'data.frame': 2436 obs. of  9 variables:\n  ..$ role     : chr [1:2436] \"part_of\" \"part_of\" \"part_of\" \"part_of\" ...\n  ..$ source   : chr [1:2436] \"Meeting_1\" \"Meeting_1\" \"Meeting_1\" \"Meeting_1\" ...\n  ..$ target   : chr [1:2436] \"fish_vacuum_Meeting_1_Introduction_Discussion\" \"fish_vacuum_Meeting_1_Introduction\" \"seafood_festival_Meeting_1_Discussion\" \"seafood_festival_Meeting_1_Feasibility\" ...\n  ..$ key      : int [1:2436] 0 0 0 0 0 0 0 0 0 0 ...\n  ..$ sentiment: num [1:2436] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ reason   : chr [1:2436] NA NA NA NA ...\n  ..$ industry :List of 2436\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. .. [list output truncated]\n  ..$ status   : chr [1:2436] NA NA NA NA ...\n  ..$ time     : chr [1:2436] NA NA NA NA ...\n\n\n\n\n\n\n\n3.2 Extracting the edges and nodes tables\nNext, as_tibble() of tibble package package is used to extract the nodes and links tibble data frames from each of the dataframe into two separate tibble dataframes called nodes and edges respectively.\n\nFILAHTROUTJOURNALIST\n\n\n\nfilah_nodes &lt;- as_tibble(filah$nodes)\nfilah_edges &lt;- as_tibble(filah$links)\n\n\n\n\ntrout_nodes &lt;- as_tibble(trout$nodes)\ntrout_edges &lt;- as_tibble(trout$links)\n\n\n\n\njournalist_nodes &lt;- as_tibble(journalist$nodes)\njournalist_edges &lt;- as_tibble(journalist$links)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#overview",
    "href": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#overview",
    "title": "Take-home Exercise 02",
    "section": "",
    "text": "In this exercise, we will be using Mini-Challenge 2 from VAST Challenge 2025, where we will take on the theme of conflict over societal change directly. Two opposing groups (FILAH and TROUT) representing the fishing and tourism industries are vying for economic development funding and claiming a local government board of bias. We will try to reconstruct a true timeline with data that comes from both sides.\n\n\n\nBased on the datasets that TROUT & FILAH have provided, use visual analytics to determine if each group’s accusations are supported by their own record set. In other words, develop a visualization to highlight bias (if present) in TROUT & FILAHS datasets. Is there evidence of bias in the COOTEFOO member actions in either dataset?\nAs a journalist, Ms. Moray would like a more complete picture of the COOTEFOO’s actions and activities. She has arranged to combine the data provided by TROUT and FILAH into a single knowledge graph along with additional records. Design visual analytics approaches for this combined knowledge graph to see how members of COOTEFOO spend their time. Is the committee as a whole biased? Provide visual evidence for your conclusions.\nThe TROUT and FILAH datasets are incomplete. Use your visualizations to compare and contrast conclusions drawn from the TROUT and FILAH datasets separately with behaviors in the whole dataset. Are the accusations of TROUT strengthened, weakened or unchanged when taken in context of the whole dataset?\nDesign a visualization that allows Ms. Moray to pick a person and highlight the differences in that person’s behavior as illustrated through the different datasets. Focus on the contrast in the story each dataset tells.\n\nPick at least one COOTEFOO member accused by TROUT. Illustrate how your understanding of their activities changed when using the more complete dataset.\nWhat are the key pieces of evidence missing from the original TROUT data that most influenced the change in judgement.\nWhose behaviors are most impacted by sampling bias when looking at the FILAH dataset in context of the other data?\nIllustrate the bias of the FILAH data in the context of the whole dataset.\n\n\n\n\n\nWe will use the following packages and using the p_load() of pacman package to load the R packages into R environment.\n\npacman::p_load(tidyverse, jsonlite, SmartEDA, tidygraph, ggraph, data.table, DT, visNetwork, tidyr, naniar)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#tasks-and-questions",
    "href": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#tasks-and-questions",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "Based on the datasets that TROUT & FILAH have provided, use visual analytics to determine if each group’s accusations are supported by their own record set. In other words, develop a visualization to highlight bias (if present) in TROUT & FILAHS datasets. Is there evidence of bias in the COOTEFOO member actions in either dataset?\nAs a journalist, Ms. Moray would like a more complete picture of the COOTEFOO’s actions and activities. She has arranged to combine the data provided by TROUT and FILAH into a single knowledge graph along with additional records. Design visual analytics approaches for this combined knowledge graph to see how members of COOTEFOO spend their time. Is the committee as a whole biased? Provide visual evidence for your conclusions.\nThe TROUT and FILAH datasets are incomplete. Use your visualizations to compare and contrast conclusions drawn from the TROUT and FILAH datasets separately with behaviors in the whole dataset. Are the accusations of TROUT strengthened, weakened or unchanged when taken in context of the whole dataset?\nDesign a visualization that allows Ms. Moray to pick a person and highlight the differences in that person’s behavior as illustrated through the different datasets. Focus on the contrast in the story each dataset tells.\n\nPick at least one COOTEFOO member accused by TROUT. Illustrate how your understanding of their activities changed when using the more complete dataset.\nWhat are the key pieces of evidence missing from the original TROUT data that most influenced the change in judgement.\nWhose behaviors are most impacted by sampling bias when looking at the FILAH dataset in context of the other data?\nIllustrate the bias of the FILAH data in the context of the whole dataset."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#inspecting-knowledge-graph-structure",
    "href": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#inspecting-knowledge-graph-structure",
    "title": "Take-home Exercise 2",
    "section": "3.1 Inspecting Knowledge Graph Structure",
    "text": "3.1 Inspecting Knowledge Graph Structure\nThe datasets contain graph data, where nodes can be accessed via nodes and edges via links. The datasets have a lot of columns but we will only filter the relevant columns in this analysis.\n\n\nShow the code\n\n\nFILAHTROUTJOURNALIST\n\n\n\nglimpse(filah)\n\nList of 5\n $ directed  : logi TRUE\n $ multigraph: logi TRUE\n $ graph     : Named list()\n $ nodes     :'data.frame': 396 obs. of  17 variables:\n  ..$ type       : chr [1:396] \"meeting\" \"meeting\" \"meeting\" \"meeting\" ...\n  ..$ date       : chr [1:396] \"Meeting 1\" \"Meeting 2\" \"Meeting 3\" \"Meeting 4\" ...\n  ..$ label      : chr [1:396] \"Meeting 1\" \"Meeting 2\" \"Meeting 3\" \"Meeting 4\" ...\n  ..$ id         : chr [1:396] \"Meeting_1\" \"Meeting_2\" \"Meeting_3\" \"Meeting_4\" ...\n  ..$ name       : chr [1:396] NA NA NA NA ...\n  ..$ role       : chr [1:396] NA NA NA NA ...\n  ..$ short_topic: chr [1:396] NA NA NA NA ...\n  ..$ long_topic : chr [1:396] NA NA NA NA ...\n  ..$ short_title: chr [1:396] NA NA NA NA ...\n  ..$ long_title : chr [1:396] NA NA NA NA ...\n  ..$ plan_type  : chr [1:396] NA NA NA NA ...\n  ..$ lat        : num [1:396] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ lon        : num [1:396] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ zone       : chr [1:396] NA NA NA NA ...\n  ..$ zone_detail: chr [1:396] NA NA NA NA ...\n  ..$ start      : chr [1:396] NA NA NA NA ...\n  ..$ end        : chr [1:396] NA NA NA NA ...\n $ links     :'data.frame': 765 obs. of  9 variables:\n  ..$ role     : chr [1:765] \"part_of\" \"part_of\" \"part_of\" \"part_of\" ...\n  ..$ source   : chr [1:765] \"Meeting_1\" \"Meeting_1\" \"Meeting_1\" \"Meeting_1\" ...\n  ..$ target   : chr [1:765] \"fish_vacuum_Meeting_1_Introduction_Discussion\" \"fish_vacuum_Meeting_1_Introduction\" \"seafood_festival_Meeting_1_Discussion\" \"seafood_festival_Meeting_1_Feasibility\" ...\n  ..$ key      : int [1:765] 0 0 0 0 0 0 0 0 0 0 ...\n  ..$ sentiment: num [1:765] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ reason   : chr [1:765] NA NA NA NA ...\n  ..$ industry :List of 765\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. .. [list output truncated]\n  ..$ status   : chr [1:765] NA NA NA NA ...\n  ..$ time     : chr [1:765] NA NA NA NA ...\n\n\n\n\n\nglimpse(trout)\n\nList of 5\n $ directed  : logi TRUE\n $ multigraph: logi TRUE\n $ graph     : Named list()\n $ nodes     :'data.frame': 164 obs. of  17 variables:\n  ..$ type       : chr [1:164] \"plan\" \"plan\" \"plan\" \"meeting\" ...\n  ..$ short_title: chr [1:164] \"marine_life_deck_Meeting_12_Environmental_Impact_Report\" \"low_volume_crane_Meeting_8_Report\" \"deep_fishing_dock_Meeting_3_Maintenance_Plan\" NA ...\n  ..$ long_title : chr [1:164] \"Present findings from the environmental impact study\" \"Report on travel and submit letter of support for low-volume unload crane\" \"Discuss maintenance plan and designate travel representatives\" NA ...\n  ..$ plan_type  : chr [1:164] \"Report\" \"report\" \"proposal\" NA ...\n  ..$ label      : chr [1:164] \"marine_life_deck_Meeting_12_Environmental_Impact_Report\" \"low_volume_crane_Meeting_8_Report\" \"deep_fishing_dock_Meeting_3_Maintenance_Plan\" \"Meeting 16\" ...\n  ..$ id         : chr [1:164] \"marine_life_deck_Meeting_12_Environmental_Impact_Report\" \"low_volume_crane_Meeting_8_Report\" \"deep_fishing_dock_Meeting_3_Maintenance_Plan\" \"Meeting_16\" ...\n  ..$ date       : chr [1:164] NA NA NA \"Meeting 16\" ...\n  ..$ short_topic: chr [1:164] NA NA NA NA ...\n  ..$ long_topic : chr [1:164] NA NA NA NA ...\n  ..$ lat        : num [1:164] NA NA NA NA NA ...\n  ..$ lon        : num [1:164] NA NA NA NA NA ...\n  ..$ zone       : chr [1:164] NA NA NA NA ...\n  ..$ zone_detail: chr [1:164] NA NA NA NA ...\n  ..$ name       : chr [1:164] NA NA NA NA ...\n  ..$ role       : chr [1:164] NA NA NA NA ...\n  ..$ start      : chr [1:164] NA NA NA NA ...\n  ..$ end        : chr [1:164] NA NA NA NA ...\n $ links     :'data.frame': 378 obs. of  9 variables:\n  ..$ role     : chr [1:378] \"plan\" \"participant\" \"plan\" \"participant\" ...\n  ..$ source   : chr [1:378] \"marine_life_deck_Meeting_12_Environmental_Impact_Report\" \"marine_life_deck_Meeting_12_Environmental_Impact_Report\" \"low_volume_crane_Meeting_8_Report\" \"low_volume_crane_Meeting_8_Report\" ...\n  ..$ target   : chr [1:378] \"marine_life_deck\" \"Teddy Goldstein\" \"low_volume_crane\" \"Seal\" ...\n  ..$ key      : int [1:378] 0 0 0 0 0 0 0 0 0 0 ...\n  ..$ sentiment: num [1:378] NA -0.5 NA 0.1 NA NA NA NA NA NA ...\n  ..$ reason   : chr [1:378] NA \"Prefers resources to be allocated toward the fishing industry.\" NA \"Recognizes the crane's benefit to small-scale operations.\" ...\n  ..$ industry :List of 378\n  .. ..$ : NULL\n  .. ..$ : chr \"tourism\"\n  .. ..$ : NULL\n  .. ..$ : chr \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr \"tourism\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"large vessel\" \"small vessel\"\n  .. ..$ : chr [1:2] \"large vessel\" \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"large vessel\" \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr \"tourism\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"large vessel\" \"small vessel\"\n  .. ..$ : chr [1:2] \"large vessel\" \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr \"large vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"large vessel\" \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"large vessel\" \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : chr \"tourism\"\n  .. ..$ : NULL\n  .. ..$ : list()\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr \"tourism\"\n  .. ..$ : NULL\n  .. ..$ : chr \"large vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr \"tourism\"\n  .. ..$ : chr \"tourism\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : chr \"large vessel\"\n  .. ..$ : NULL\n  .. ..$ : chr \"tourism\"\n  .. ..$ : chr \"tourism\"\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"large vessel\" \"small vessel\"\n  .. ..$ : chr [1:2] \"large vessel\" \"small vessel\"\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : list()\n  .. .. [list output truncated]\n  ..$ status   : chr [1:378] NA NA NA NA ...\n  ..$ time     : chr [1:378] NA NA NA NA ...\n\n\n\n\n\nglimpse(journalist)\n\nList of 5\n $ directed  : logi TRUE\n $ multigraph: logi TRUE\n $ graph     : Named list()\n $ nodes     :'data.frame': 740 obs. of  17 variables:\n  ..$ type       : chr [1:740] \"meeting\" \"meeting\" \"meeting\" \"meeting\" ...\n  ..$ date       : chr [1:740] \"Meeting 1\" \"Meeting 2\" \"Meeting 3\" \"Meeting 4\" ...\n  ..$ label      : chr [1:740] \"Meeting 1\" \"Meeting 2\" \"Meeting 3\" \"Meeting 4\" ...\n  ..$ id         : chr [1:740] \"Meeting_1\" \"Meeting_2\" \"Meeting_3\" \"Meeting_4\" ...\n  ..$ name       : chr [1:740] NA NA NA NA ...\n  ..$ role       : chr [1:740] NA NA NA NA ...\n  ..$ short_topic: chr [1:740] NA NA NA NA ...\n  ..$ long_topic : chr [1:740] NA NA NA NA ...\n  ..$ short_title: chr [1:740] NA NA NA NA ...\n  ..$ long_title : chr [1:740] NA NA NA NA ...\n  ..$ plan_type  : chr [1:740] NA NA NA NA ...\n  ..$ lat        : num [1:740] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ lon        : num [1:740] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ zone       : chr [1:740] NA NA NA NA ...\n  ..$ zone_detail: chr [1:740] NA NA NA NA ...\n  ..$ start      : chr [1:740] NA NA NA NA ...\n  ..$ end        : chr [1:740] NA NA NA NA ...\n $ links     :'data.frame': 2436 obs. of  9 variables:\n  ..$ role     : chr [1:2436] \"part_of\" \"part_of\" \"part_of\" \"part_of\" ...\n  ..$ source   : chr [1:2436] \"Meeting_1\" \"Meeting_1\" \"Meeting_1\" \"Meeting_1\" ...\n  ..$ target   : chr [1:2436] \"fish_vacuum_Meeting_1_Introduction_Discussion\" \"fish_vacuum_Meeting_1_Introduction\" \"seafood_festival_Meeting_1_Discussion\" \"seafood_festival_Meeting_1_Feasibility\" ...\n  ..$ key      : int [1:2436] 0 0 0 0 0 0 0 0 0 0 ...\n  ..$ sentiment: num [1:2436] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ reason   : chr [1:2436] NA NA NA NA ...\n  ..$ industry :List of 2436\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. .. [list output truncated]\n  ..$ status   : chr [1:2436] NA NA NA NA ...\n  ..$ time     : chr [1:2436] NA NA NA NA ..."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#extracting-the-edges-and-nodes-tables",
    "href": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#extracting-the-edges-and-nodes-tables",
    "title": "Take-home Exercise 2",
    "section": "3.2 Extracting the edges and nodes tables",
    "text": "3.2 Extracting the edges and nodes tables\nNext, as_tibble() of tibble package package is used to extract the nodes and links tibble data frames from each of the dataframe into two separate tibble dataframes called nodes and edges respectively.\n\n\nShow the code\n\n\nFILAHTROUTJOURNALIST\n\n\n\nfilah_nodes &lt;- as_tibble(filah$nodes)\nfilah_edges &lt;- as_tibble(filah$links)\n\n\n\n\ntrout_nodes &lt;- as_tibble(trout$nodes)\ntrout_edges &lt;- as_tibble(trout$links)\n\n\n\n\njournalist_nodes &lt;- as_tibble(journalist$nodes)\njournalist_edges &lt;- as_tibble(journalist$links)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#frequency-distribution",
    "href": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#frequency-distribution",
    "title": "Take-home Exercise 2",
    "section": "4.1 Frequency Distribution",
    "text": "4.1 Frequency Distribution\n\n4.1.1 Nodes Categorical Frequency Distribution\nIn the code chunk below, ExpCatViz() of SmartEDA package is used to reveal the frequency distribution of all categorical fields in each nodes tibble dataframe.\n\nFILAHTROUTJOURNALIST\n\n\n\nExpCatViz(data=filah_nodes,\n          col=\"lightblue\")\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\n\n\n\nNode Type : Nearly half (48%) of the nodes in FILAH are trip-related, indicating a strong emphasis on travel data. There is moderate representation of discussion (15%), place (15%), and plan (10%) nodes, while missing type values (NA) at around 3%.\nRole: 99% of nodes have missing role values. There are only 3 known roles: 2 Members and 1 Committee Chair.\nZone: 82% of place nodes are missing zoning info. Among known zones, commercial dominates (14%).\n\n\n\n\n\nExpCatViz(data=trout_nodes,\n          col=\"lightblue\")\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\n\n\n\nNode Type: more evenly distributed, with Discussion (24%), Plan (20%), and Place (18%) making up the majority. Trip nodes are notably lower at 11% compared to 48% in FILAH, suggesting that TROUT places greater emphasis on planning and discourse rather than travel activity. NA values are minimal at 2%.\nZone: - 80% missing, only government zone shows some visibility (12%).\nRole: - 96% missing, but contains more variety than FILAH (includes Treasurer, Vice Chair).\n\n\n\n\n\nExpCatViz(data=journalist_nodes,\n          col=\"lightblue\")\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\n\n\n\nNode Type: Strong emphasis on trip (46%) and place (21%) nodes, with plan (10%) and discussion (14%) also notable; NA values are minimal (~3%).\nRole: 99% of role values are missing\nZone: 77% missing, but better coverage than FILAH or TROUT. commercial (12%) is the most visible zone type, followed by residential (4%) and government (4%).\n\n\n\n\n\n\n\n4.1.2 Edges Categorical Frequency Distribution\nWe will also be using code chunk below that uses ExpCATViz() of SmartEDA package to reveal the frequency distribution of all categorical fields in the edges tibble dataframe.\n\nFILAHTROUTJOURNALIST\n\n\n\nExpCatViz(data=filah_edges,\n          col=\"lightblue\")\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n\n\nExpCatViz(data=trout_edges,\n          col=\"lightblue\")\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n\n\nExpCatViz(data=journalist_edges,\n          col=\"lightblue\")\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\n\n\n\nAcross all three datasets, the role field shows a high proportion of missing (NA) values, with JOURNALIST at 70%, FILAH at 46%, and TROUT at 25%, indicating limited role clarity—especially in the full dataset. The status attribute is also largely incomplete, with over 90% of entries missing in every dataset, limiting insights into project progression. For sentiment, although a small proportion of edges carry values, positive sentiment (value = 1) dominates across all datasets—FILAH (5%), TROUT (10%), and JOURNALIST (4%)—while over 75–90% of entries are NA. This highlights a critical gap in annotation completeness, constraining deeper interpretation of engagement tone or project state.\n\n\n\n\n4.1.3 Nodes Numerical Frequency Distribution\nNext, we will use the code chunk below that uses ExpNumViz() of SmartEDA package to reveal the frequency distribution of all numerical fields in the nodes tibble dataframe.\n\nFILAHTROUTJOURNALIST\n\n\n\nExpNumViz(filah_nodes)\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\nExpNumViz(trout_nodes)\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\nExpNumViz(journalist_nodes)\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\n\n\n\nThe spatial density plots reveal notable differences in geographic coverage across the datasets. FILAH shows a bi-modal distribution in both latitude and longitude, with mild skewness, indicating a focus on two primary regions—likely aligned with fishing routes or coastal hubs.\nTROUT, on the other hand, exhibits a more skewed latitude distribution and tighter clustering in longitude, suggesting activity is concentrated in fewer, likely policy- or tourism-related zones.\nIn contrast, the JOURNALIST dataset displays the most balanced and inclusive spatial distribution, combining patterns from both FILAH and TROUT. This makes JOURNALIST the best baseline for identifying potential geographic bias or omissions in the two advocacy group datasets.\n\n\n\n\n4.1.4 Edges Numerical Frequency Distribution\nBelow, we will use the code chunk below that uses ExpNumViz() of SmartEDA package to reveal the frequency distribution of all numerical fields in the edges tibble dataframe.\n\nFILAHTROUTJOURNALIST\n\n\n\nExpNumViz(filah_edges)\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n\n\nExpNumViz(trout_edges)\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n\n\nExpNumViz(journalist_edges)\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\n\n\n\nAll three datasets show a left-skewed sentiment distribution, indicating a tendency toward positive sentiment in recorded interactions. FILAH displays moderate skewness, suggesting selective positivity likely aligned with pro-fishing narratives. TROUT is more balanced, showing a wider range of sentiment and possibly greater objectivity. JOURNALIST has the strongest left skew and a sharp peak, reflecting a high concentration of positive sentiment—likely a result of broader coverage rather than selective reporting. This highlights how sentiment framing differs across datasets, with TROUT offering more variation, while FILAH and JOURNALIST lean more positive overall."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#count-missing-values-by-column",
    "href": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#count-missing-values-by-column",
    "title": "Take-home Exercise 02",
    "section": "4.2 Count missing values by column",
    "text": "4.2 Count missing values by column\n\n# Filter to only columns with missing values\nmiss_summary_nodes &lt;- bind_rows(\n  miss_var_summary(filah_nodes) %&gt;% mutate(dataset = \"FILAH\"),\n  miss_var_summary(trout_nodes) %&gt;% mutate(dataset = \"TROUT\"),\n  miss_var_summary(journalist_nodes) %&gt;% mutate(dataset = \"JOURNALIST\")\n) %&gt;% \n  filter(n_miss &gt; 0) %&gt;% \n  select(dataset, variable, n_miss, pct_miss)\n\nmiss_summary_edges &lt;- bind_rows(\n  miss_var_summary(filah_edges) %&gt;% mutate(dataset = \"FILAH\"),\n  miss_var_summary(trout_edges) %&gt;% mutate(dataset = \"TROUT\"),\n  miss_var_summary(journalist_edges) %&gt;% mutate(dataset = \"JOURNALIST\")\n) %&gt;% \n  filter(n_miss &gt; 0) %&gt;% \n  select(dataset, variable, n_miss, pct_miss)\n\n# Display filtered tables\nDT::datatable(miss_summary_nodes)\n\n\n\n\nDT::datatable(miss_summary_edges)\n\n\n\n\n\n\n\n\n\n\n\nObservation\n\n\n\nAcross all datasets, key attributes such as role, zone, plan_type, and sentiment exhibit high missingness—exceeding 90% in most cases for FILAH and TROUT. The JOURNALIST dataset provides broader coverage but still lacks complete detail in certain fields. These gaps highlight the need to interpret findings cautiously and emphasize the importance of using the full dataset for bias assessment.\n\n\n\n# Check for duplicate IDs in each dataset\nlist(\n  FILAH = filah_nodes %&gt;% summarise(duplicates = sum(duplicated(id))),\n  TROUT = trout_nodes %&gt;% summarise(duplicates = sum(duplicated(id))),\n  JOURNALIST = journalist_nodes %&gt;% summarise(duplicates = sum(duplicated(id)))\n)\n\n$FILAH\n# A tibble: 1 × 1\n  duplicates\n       &lt;int&gt;\n1          0\n\n$TROUT\n# A tibble: 1 × 1\n  duplicates\n       &lt;int&gt;\n1          0\n\n$JOURNALIST\n# A tibble: 1 × 1\n  duplicates\n       &lt;int&gt;\n1          0\n\n\n\n\n\n\n\n\nDuplicate ID Check\n\n\n\nAll three node datasets (FILAH, TROUT, and JOURNALIST) have unique id values with no duplicates, confirming structural integrity at the node level."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#check-for-missing-values",
    "href": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#check-for-missing-values",
    "title": "Take-home Exercise 2",
    "section": "4.2 Check for Missing Values",
    "text": "4.2 Check for Missing Values\nNext, we will check the missing values in the dataset.\n\n\nShow the code\n\n\n# Filter to only columns with missing values\nmiss_summary_nodes &lt;- bind_rows(\n  miss_var_summary(filah_nodes) %&gt;% mutate(dataset = \"FILAH\"),\n  miss_var_summary(trout_nodes) %&gt;% mutate(dataset = \"TROUT\"),\n  miss_var_summary(journalist_nodes) %&gt;% mutate(dataset = \"JOURNALIST\")\n) %&gt;% \n  filter(n_miss &gt; 0) %&gt;% \n  select(dataset, variable, n_miss, pct_miss)\n\nmiss_summary_edges &lt;- bind_rows(\n  miss_var_summary(filah_edges) %&gt;% mutate(dataset = \"FILAH\"),\n  miss_var_summary(trout_edges) %&gt;% mutate(dataset = \"TROUT\"),\n  miss_var_summary(journalist_edges) %&gt;% mutate(dataset = \"JOURNALIST\")\n) %&gt;% \n  filter(n_miss &gt; 0) %&gt;% \n  select(dataset, variable, n_miss, pct_miss)\n\n# Display filtered tables\nDT::datatable(miss_summary_nodes)\n\n\n\n\nDT::datatable(miss_summary_edges)\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\n\n\n\nAcross all datasets, key attributes such as role, zone, plan_type, and sentiment exhibit high number of missing values—exceeding 90% in most cases for FILAH and TROUT. The JOURNALIST dataset provides broader coverage but still lacks complete detail in certain fields. These gaps highlight the need to interpret findings cautiously and emphasize the importance of using the full dataset for bias assessment."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#check-for-duplicate-values",
    "href": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#check-for-duplicate-values",
    "title": "Take-home Exercise 2",
    "section": "4.3 Check for Duplicate Values",
    "text": "4.3 Check for Duplicate Values\nIn the code below we will check for any duplicate values in the dataset.\n\n\nShow the code\n\n\n# Check for duplicate IDs in each dataset\nlist(\n  FILAH = filah_nodes %&gt;% summarise(duplicates = sum(duplicated(id))),\n  TROUT = trout_nodes %&gt;% summarise(duplicates = sum(duplicated(id))),\n  JOURNALIST = journalist_nodes %&gt;% summarise(duplicates = sum(duplicated(id)))\n)\n\n$FILAH\n# A tibble: 1 × 1\n  duplicates\n       &lt;int&gt;\n1          0\n\n$TROUT\n# A tibble: 1 × 1\n  duplicates\n       &lt;int&gt;\n1          0\n\n$JOURNALIST\n# A tibble: 1 × 1\n  duplicates\n       &lt;int&gt;\n1          0\n\n\n\n\n\n\n\n\n\nDuplicate ID Check\n\n\n\nAll three node datasets (FILAH, TROUT, and JOURNALIST) have unique id values with no duplicates, confirming structural integrity at the node level."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#data-cleaning-wrangling-and-visualization",
    "href": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#data-cleaning-wrangling-and-visualization",
    "title": "Take-home Exercise 02",
    "section": "5. Data Cleaning, Wrangling, and Visualization",
    "text": "5. Data Cleaning, Wrangling, and Visualization\nWe define reusable functions to clean and wrangle each dataset into a graph structure and visualize the result.\n\n# Function to clean and build graph\n\nbuild_graph_data &lt;- function(nodes_df, edges_df) {\n  # Clean nodes\n  nodes_cleaned &lt;- nodes_df %&gt;%\n    mutate(id = as.character(id)) %&gt;%\n    filter(!is.na(id)) %&gt;%\n    distinct(id, .keep_all = TRUE) %&gt;%\n    select(id, type, label)\n  \n  # Clean edges\n  edges_cleaned &lt;- edges_df %&gt;%\n    rename(from = source, to = target) %&gt;%\n    mutate(across(c(from, to), as.character)) %&gt;%\n    filter(from %in% nodes_cleaned$id, to %in% nodes_cleaned$id)\n  \n  # Simplified edge table for graph\n  edges_min &lt;- edges_cleaned %&gt;%\n    select(from, to, role)\n  \n  # Build tidygraph object\n  graph_obj &lt;- tbl_graph(\n    nodes = nodes_cleaned,\n    edges = edges_min,\n    directed = TRUE\n  )\n  \n  # Return all elements for reuse\n  return(list(\n    nodes_cleaned = nodes_cleaned,\n    edges_cleaned = edges_cleaned,\n    edges_min = edges_min,\n    graph = graph_obj\n  ))\n}\n\n\nplot_graph_overview &lt;- function(graph_obj, title = \"Graph Overview\") {\n  set.seed(1234)  # Ensure reproducibility\n  \n  ggraph(graph_obj, layout = \"fr\") +\n    geom_edge_link(alpha = 0.3, colour = \"gray\") +\n    geom_node_point(aes(color = type), size = 4) +\n    geom_node_text(aes(label = type), repel = TRUE, size = 2.5) +\n    ggtitle(title) +\n    theme_void()\n}\n\n\nFILAHTROUTJOURNALIST\n\n\n\nfilah_data &lt;- build_graph_data(filah_nodes, filah_edges)\nplot_graph_overview(filah_data$graph, \"FILAH Knowledge Graph\")\n\n\n\n\n\n\n\n\n\n\n\n\ntrout_data &lt;- build_graph_data(trout_nodes, trout_edges)\nplot_graph_overview(trout_data$graph, \"TROUT Knowledge Graph\")\n\n\n\n\n\n\n\n\n\n\n\n\njournalist_data &lt;- build_graph_data(journalist_nodes, journalist_edges)\nplot_graph_overview(journalist_data$graph, \"JOURNALIST Knowledge Graph\")\n\n\n\n\n\n\n\n\n\n\n\nPeople EDA\n\n# Extract COOTEFOO members (nodes with non-NA role)\ncootefoo_members_all &lt;- bind_rows(\n  filah_nodes %&gt;% filter(!is.na(role)) %&gt;% mutate(source = \"FILAH\"),\n  trout_nodes %&gt;% filter(!is.na(role)) %&gt;% mutate(source = \"TROUT\"),\n  journalist_nodes %&gt;% filter(!is.na(role)) %&gt;% mutate(source = \"JOURNALIST\")\n) %&gt;%\n  distinct(name, role, source)\n\n# View full member list\ncootefoo_members_all\n\n# A tibble: 15 × 3\n   name            role            source    \n   &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;     \n 1 Seal            Committee Chair FILAH     \n 2 Simone Kat      Member          FILAH     \n 3 Carol Limpet    Member          FILAH     \n 4 Teddy Goldstein Treasurer       TROUT     \n 5 Ed Helpsford    Vice Chair      TROUT     \n 6 Seal            Committee Chair TROUT     \n 7 Tante Titan     Member          TROUT     \n 8 Carol Limpet    Member          TROUT     \n 9 Simone Kat      Member          TROUT     \n10 Seal            Committee Chair JOURNALIST\n11 Ed Helpsford    Vice Chair      JOURNALIST\n12 Teddy Goldstein Treasurer       JOURNALIST\n13 Simone Kat      Member          JOURNALIST\n14 Tante Titan     Member          JOURNALIST\n15 Carol Limpet    Member          JOURNALIST\n\n\n\n# Load required package (already in pacman::p_load)\nlibrary(ggvenn)\n\n# Prepare member sets from each dataset (non-missing names with role)\nmember_sets &lt;- list(\n  FILAH = filah_nodes %&gt;% filter(!is.na(role)) %&gt;% pull(name) %&gt;% unique(),\n  TROUT = trout_nodes %&gt;% filter(!is.na(role)) %&gt;% pull(name) %&gt;% unique(),\n  JOURNALIST = journalist_nodes %&gt;% filter(!is.na(role)) %&gt;% pull(name) %&gt;% unique()\n)\n\n# Create inclusive Venn diagram using ggvenn\nggvenn(\n  member_sets,\n  fill_color = c(\"#99d8c9\", \"#fdbb84\", \"#c6dbef\"),\n  show_percentage = TRUE,\n  set_name_size = 4,\n  stroke_size = 0.5\n)\n\n\n\n\n\n\n\nReduce(intersect, list(\n  filah_nodes %&gt;% filter(!is.na(role)) %&gt;% pull(name),\n  trout_nodes %&gt;% filter(!is.na(role)) %&gt;% pull(name),\n  journalist_nodes %&gt;% filter(!is.na(role)) %&gt;% pull(name)\n))\n\n[1] \"Seal\"         \"Simone Kat\"   \"Carol Limpet\"\n\n\n\n# Filter by these three names across datasets\ncore_members &lt;- c(\"Seal\", \"Simone Kat\", \"Carol Limpet\")\n\n# Example: filter journalist_edges to just interactions involving them\njournalist_edges %&gt;%\n  filter(target %in% core_members | source %in% core_members)\n\n# A tibble: 267 × 9\n   role        source        target   key sentiment reason industry status time \n   &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt;  &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;list&gt;   &lt;chr&gt;  &lt;chr&gt;\n 1 participant expanding_to… Seal       0       0.1 Seems… &lt;chr&gt;    &lt;NA&gt;   &lt;NA&gt; \n 2 participant expanding_to… Simon…     0       0.5 Suppo… &lt;chr&gt;    &lt;NA&gt;   &lt;NA&gt; \n 3 participant expanding_to… Simon…     0       0.5 Suppo… &lt;chr&gt;    &lt;NA&gt;   &lt;NA&gt; \n 4 participant expanding_to… Seal       0       0.1 Seems… &lt;chr&gt;    &lt;NA&gt;   &lt;NA&gt; \n 5 participant expanding_to… Simon…     0       0.5 Suppo… &lt;chr&gt;    &lt;NA&gt;   &lt;NA&gt; \n 6 participant expanding_to… Seal       0       0.1 Seems… &lt;chr&gt;    &lt;NA&gt;   &lt;NA&gt; \n 7 participant expanding_to… Seal       0       0.1 Seems… &lt;chr&gt;    &lt;NA&gt;   &lt;NA&gt; \n 8 participant statue_john_… Seal       0       0.2 Appre… &lt;list&gt;   &lt;NA&gt;   &lt;NA&gt; \n 9 participant statue_john_… Seal       0       0.2 Appre… &lt;list&gt;   &lt;NA&gt;   &lt;NA&gt; \n10 participant deep_fishing… Simon…     0      NA   &lt;NA&gt;   &lt;NULL&gt;   &lt;NA&gt;   &lt;NA&gt; \n# ℹ 257 more rows\n\n\n\n# Step 1: Get full list of COOTEFOO members from journalist\nall_members &lt;- journalist_nodes %&gt;%\n  filter(!is.na(role)) %&gt;%\n  pull(name)\n\n# Step 2: Analyze participation and sentiment\nmember_stats &lt;- journalist_edges %&gt;%\n  filter(role == \"participant\", target %in% all_members | source %in% all_members) %&gt;%\n  mutate(member = if_else(target %in% all_members, target, source)) %&gt;%\n  group_by(member) %&gt;%\n  summarise(\n    n = n(),  # participation count\n    avg_sentiment = mean(sentiment, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(n))\n\n# View result\nmember_stats\n\n# A tibble: 6 × 3\n  member              n avg_sentiment\n  &lt;chr&gt;           &lt;int&gt;         &lt;dbl&gt;\n1 Tante Titan        54         0.819\n2 Simone Kat         45         0.469\n3 Teddy Goldstein    22         0.344\n4 Carol Limpet       21         0.655\n5 Ed Helpsford       20         0.7  \n6 Seal               12         0.108\n\n\n\nggplot(member_stats, aes(x = reorder(member, avg_sentiment), y = avg_sentiment, fill = n)) +\n  geom_col() +\n  coord_flip() +\n  labs(\n    title = \"COOTEFOO Members: Sentiment vs. Activity\",\n    x = \"Member\",\n    y = \"Average Sentiment\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Step 1: Get full member list from JOURNALIST\ncootefoo_members &lt;- journalist_nodes %&gt;%\n  filter(!is.na(role)) %&gt;%\n  pull(name) %&gt;%\n  unique()\n\n# Step 2: Function to extract member activity from any dataset\nget_member_stats &lt;- function(edges, dataset_name) {\n  edges %&gt;%\n    filter(role == \"participant\", source %in% cootefoo_members | target %in% cootefoo_members) %&gt;%\n    mutate(member = if_else(source %in% cootefoo_members, source, target)) %&gt;%\n    group_by(member) %&gt;%\n    summarise(\n      n = n(),\n      avg_sentiment = mean(sentiment, na.rm = TRUE),\n      dataset = dataset_name\n    )\n}\n\n# Step 3: Apply to all three datasets\nfilah_stats &lt;- get_member_stats(filah_edges, \"FILAH\")\ntrout_stats &lt;- get_member_stats(trout_edges, \"TROUT\")\njournalist_stats &lt;- get_member_stats(journalist_edges, \"JOURNALIST\")\n\n# Combine results\nall_member_stats &lt;- bind_rows(filah_stats, trout_stats, journalist_stats)\n\n# Step 4: Plotting\nlibrary(ggplot2)\n\nggplot(all_member_stats, aes(x = reorder(member, avg_sentiment), y = avg_sentiment, fill = dataset)) +\n  geom_col(position = \"dodge\") +\n  coord_flip() +\n  labs(\n    title = \"COOTEFOO Member Sentiment by Dataset\",\n    x = \"Member\",\n    y = \"Average Sentiment\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#building-the-function",
    "href": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#building-the-function",
    "title": "Take-home Exercise 2",
    "section": "5.1 Building the Function",
    "text": "5.1 Building the Function\nWe define reusable functions to clean and wrangle the nodes and edges of each dataset into a graph structure and visualize the result. We will use id, type and label to build the function for Knowledge Graph.\n\n\nShow the code\n\n\n# Function to clean and build graph\n\nbuild_graph_data &lt;- function(nodes_df, edges_df) {\n  # Clean nodes\n  nodes_cleaned &lt;- nodes_df %&gt;%\n    mutate(id = as.character(id)) %&gt;%\n    filter(!is.na(id)) %&gt;%\n    distinct(id, .keep_all = TRUE) %&gt;%\n    select(id, type, label)\n  \n  # Clean edges\n  edges_cleaned &lt;- edges_df %&gt;%\n    rename(from = source, to = target) %&gt;%\n    mutate(across(c(from, to), as.character)) %&gt;%\n    filter(from %in% nodes_cleaned$id, to %in% nodes_cleaned$id)\n  \n  # Simplified edge table for graph\n  edges_min &lt;- edges_cleaned %&gt;%\n    select(from, to, role)\n    \n  # Build tidygraph object\n  graph_obj &lt;- tbl_graph(\n    nodes = nodes_cleaned,\n    edges = edges_min,\n    directed = TRUE\n  )\n  \n  # Return all elements for reuse\n  return(list(\n    nodes_cleaned = nodes_cleaned,\n    edges_cleaned = edges_cleaned,\n    edges_min = edges_min,\n    graph = graph_obj\n  ))\n}\n\n\nAs several of ggraph layouts involve randomization, in order to ensure reproducibility, it is necessary to set the seed value before plotting. We will use the following function to visualize the knowledge graph:\n\n\nShow the code\n\n\nplot_graph_overview &lt;- function(graph_obj, title = \"Graph Overview\") {\n  set.seed(1234)  # Ensure reproducibility\n  \n  ggraph(graph_obj, layout = \"fr\") +\n    geom_edge_link(alpha = 0.3, colour = \"gray\") +\n    geom_node_point(aes(color = type), size = 4) +\n    geom_node_text(aes(label = type), repel = TRUE, size = 2.5) +\n    ggtitle(title) +\n    theme_void()\n}"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#visualizing-the-knowledge-graph",
    "href": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#visualizing-the-knowledge-graph",
    "title": "Take-home Exercise 2",
    "section": "5.2 Visualizing the Knowledge Graph",
    "text": "5.2 Visualizing the Knowledge Graph\nwe will use the functions set up earlier for each dataset to visualize the knowledge graph as follows:\n\nFILAHTROUTJOURNALIST\n\n\n\nfilah_data &lt;- build_graph_data(filah_nodes, filah_edges)\nplot_graph_overview(filah_data$graph, \"FILAH Knowledge Graph\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights — FILAH Knowledge Graph\n\n\n\nThe FILAH Knowledge Graph is strongly travel-oriented, with trip and place nodes dominating the structure. These nodes form tight clusters, particularly on the graph’s periphery, highlighting a dataset constructed heavily around physical movements and site visits. The central region connects a smaller set of plan, discussion, and topic nodes, indicating some internal deliberation, though not as extensively represented. The prevalence of pink (trip) and cyan (place) suggests a bias toward documenting activities tied to fishing zones and physical visits, aligning with FILAH’s focus. There are relatively few missing (NA) nodes, and entity.person nodes appear less central—implying limited metadata on individual involvement.\n\n\n\n\n\n\ntrout_data &lt;- build_graph_data(trout_nodes, trout_edges)\nplot_graph_overview(trout_data$graph, \"TROUT Knowledge Graph\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights — TROUT Knowledge Graph\n\n\n\nThe TROUT Knowledge Graph presents a denser, more balanced core structure, emphasizing discussion, plan, and meeting nodes. These indicate a dataset centered on dialogue and strategy rather than physical movement. Trip and place nodes are present but largely positioned in the outer cluster, suggesting they play a supporting rather than dominant role in this narrative. The central cluster tightly links topics to discussions and plans, reflecting TROUT’s focus on tourism policy, planning, and civic engagement. Entity.person nodes are more visible here than in FILAH, showing clearer attribution of roles within planning processes. Overall, the structure reflects a policy-driven lens with a centralized documentation approach.\n\n\n\n\n\n\njournalist_data &lt;- build_graph_data(journalist_nodes, journalist_edges)\nplot_graph_overview(journalist_data$graph, \"JOURNALIST Knowledge Graph\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights — Journalist Knowledge Graph\n\n\n\nThe Journalist Knowledge Graph is the most comprehensive and integrated among the three. It merges the narrative strengths of both FILAH and TROUT datasets—featuring a rich, interconnected core of plans, discussions, topics, and meetings, while also showing extensive clusters of trips and places. The graph structure indicates a full lifecycle of committee activity: from discourse to field visits. Entity.person and entity.organization nodes are more evenly distributed and active, suggesting better attribution and data coverage. While there are more NA-labeled nodes compared to other graphs, the overall density and connectivity reveal a holistic portrayal of COOTEFOO’s activities, enabling clearer bias assessment when cross-referenced with the other two graphs."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#cootefoo-member-involvement",
    "href": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#cootefoo-member-involvement",
    "title": "Take-home Exercise 02",
    "section": "5.3 COOTEFOO Member Involvement",
    "text": "5.3 COOTEFOO Member Involvement\nNext, we will examine the Name and Role variables from each dataset to see the involvement of the COOTEFOO members, with the code below:\n\n\nShow the code\n\n\n# Set role hierarchy for custom ordering\nrole_order &lt;- c(\"Committee Chair\", \"Vice Chair\", \"Treasurer\", \"Member\")\n\n# Combine and pivot the dataset\ncootefoo_members_all &lt;- bind_rows(\n  filah_nodes %&gt;% filter(!is.na(role)) %&gt;% mutate(source = \"FILAH\"),\n  trout_nodes %&gt;% filter(!is.na(role)) %&gt;% mutate(source = \"TROUT\"),\n  journalist_nodes %&gt;% filter(!is.na(role)) %&gt;% mutate(source = \"JOURNALIST\")\n) %&gt;%\n  distinct(name, role, source) %&gt;%\n  mutate(present = TRUE) %&gt;%\n  pivot_wider(\n    names_from = source,\n    values_from = present,\n    values_fill = FALSE\n  ) %&gt;%\n  mutate(role = factor(role, levels = role_order)) %&gt;%\n  arrange(role, name)\n\n# Optional: convert TRUE/FALSE to \"✔\" / \"\"\ncootefoo_members_all_display &lt;- cootefoo_members_all %&gt;%\n  mutate(across(FILAH:JOURNALIST, ~ ifelse(.x, \"✔\", \"\")))\n\n# View arranged table\ncootefoo_members_all_display\n\n# A tibble: 6 × 5\n  name            role            FILAH TROUT JOURNALIST\n  &lt;chr&gt;           &lt;fct&gt;           &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     \n1 Seal            Committee Chair \"✔\"   ✔     ✔         \n2 Ed Helpsford    Vice Chair      \"\"    ✔     ✔         \n3 Teddy Goldstein Treasurer       \"\"    ✔     ✔         \n4 Carol Limpet    Member          \"✔\"   ✔     ✔         \n5 Simone Kat      Member          \"✔\"   ✔     ✔         \n6 Tante Titan     Member          \"\"    ✔     ✔         \n\n\n\n\n\n\n\n\n\nInsights — COOTEFOO Member Dataset Representation\n\n\n\nAll six COOTEFOO members are present in the JOURNALIST dataset, confirming its completeness. The TROUT dataset also includes all members, while FILAH is missing Ed Helpsford, Teddy Goldstein, and Tante Titan — all of whom hold senior or relevant committee positions. We should explore and investigate further\n\n\n\n# Step 1: Get full list of COOTEFOO members from journalist\nall_members &lt;- journalist_nodes %&gt;%\n  filter(!is.na(role)) %&gt;%\n  pull(name)\n\n# Step 2: Analyze participation and sentiment\nmember_stats &lt;- journalist_edges %&gt;%\n  filter(role == \"participant\", target %in% all_members | source %in% all_members) %&gt;%\n  mutate(member = if_else(target %in% all_members, target, source)) %&gt;%\n  group_by(member) %&gt;%\n  summarise(\n    n = n(),  # participation count\n    avg_sentiment = mean(sentiment, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(n))\n\n# View result\nmember_stats\n\n# A tibble: 6 × 3\n  member              n avg_sentiment\n  &lt;chr&gt;           &lt;int&gt;         &lt;dbl&gt;\n1 Tante Titan        54         0.819\n2 Simone Kat         45         0.469\n3 Teddy Goldstein    22         0.344\n4 Carol Limpet       21         0.655\n5 Ed Helpsford       20         0.7  \n6 Seal               12         0.108\n\n\n\nggplot(member_stats, aes(x = reorder(member, avg_sentiment), y = avg_sentiment, fill = n)) +\n  geom_col() +\n  coord_flip() +\n  labs(\n    title = \"COOTEFOO Members: Sentiment vs. Activity\",\n    x = \"Member\",\n    y = \"Average Sentiment\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Step 1: Get full member list from JOURNALIST\ncootefoo_members &lt;- journalist_nodes %&gt;%\n  filter(!is.na(role)) %&gt;%\n  pull(name) %&gt;%\n  unique()\n\n# Step 2: Function to extract member activity from any dataset\nget_member_stats &lt;- function(edges, dataset_name) {\n  edges %&gt;%\n    filter(role == \"participant\", source %in% cootefoo_members | target %in% cootefoo_members) %&gt;%\n    mutate(member = if_else(source %in% cootefoo_members, source, target)) %&gt;%\n    group_by(member) %&gt;%\n    summarise(\n      n = n(),\n      avg_sentiment = mean(sentiment, na.rm = TRUE),\n      dataset = dataset_name\n    )\n}\n\n# Step 3: Apply to all three datasets\nfilah_stats &lt;- get_member_stats(filah_edges, \"FILAH\")\ntrout_stats &lt;- get_member_stats(trout_edges, \"TROUT\")\njournalist_stats &lt;- get_member_stats(journalist_edges, \"JOURNALIST\")\n\n# Combine results\nall_member_stats &lt;- bind_rows(filah_stats, trout_stats, journalist_stats)\n\n# Step 4: Plotting\nlibrary(ggplot2)\n\nggplot(all_member_stats, aes(x = reorder(member, avg_sentiment), y = avg_sentiment, fill = dataset)) +\n  geom_col(position = \"dodge\") +\n  coord_flip() +\n  labs(\n    title = \"COOTEFOO Member Sentiment by Dataset\",\n    x = \"Member\",\n    y = \"Average Sentiment\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#cootefoo-member-list",
    "href": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#cootefoo-member-list",
    "title": "Take-home Exercise 2",
    "section": "5.4 COOTEFOO Member List",
    "text": "5.4 COOTEFOO Member List\nNext, we will examine the list of Name and Role variables that are present in each dataset to see the involvement of the COOTEFOO members, with the code below:\n\n\nShow the code\n\n\n# Set role hierarchy for custom ordering\nrole_order &lt;- c(\"Committee Chair\", \"Vice Chair\", \"Treasurer\", \"Member\")\n\n# Combine and pivot the dataset\ncootefoo_members_all &lt;- bind_rows(\n  filah_nodes %&gt;% filter(!is.na(role)) %&gt;% mutate(source = \"FILAH\"),\n  trout_nodes %&gt;% filter(!is.na(role)) %&gt;% mutate(source = \"TROUT\"),\n  journalist_nodes %&gt;% filter(!is.na(role)) %&gt;% mutate(source = \"JOURNALIST\")\n) %&gt;%\n  distinct(name, role, source) %&gt;%\n  mutate(present = TRUE) %&gt;%\n  pivot_wider(\n    names_from = source,\n    values_from = present,\n    values_fill = FALSE\n  ) %&gt;%\n  mutate(role = factor(role, levels = role_order)) %&gt;%\n  arrange(role, name)\n\n# convert TRUE/FALSE to \"✔\" / \"\"\ncootefoo_members_all_display &lt;- cootefoo_members_all %&gt;%\n  mutate(across(FILAH:JOURNALIST, ~ ifelse(.x, \"✔\", \"\")))\n\n# View arranged table\ncootefoo_members_all_display\n\n# A tibble: 6 × 5\n  name            role            FILAH TROUT JOURNALIST\n  &lt;chr&gt;           &lt;fct&gt;           &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     \n1 Seal            Committee Chair \"✔\"   ✔     ✔         \n2 Ed Helpsford    Vice Chair      \"\"    ✔     ✔         \n3 Teddy Goldstein Treasurer       \"\"    ✔     ✔         \n4 Carol Limpet    Member          \"✔\"   ✔     ✔         \n5 Simone Kat      Member          \"✔\"   ✔     ✔         \n6 Tante Titan     Member          \"\"    ✔     ✔         \n\n\n\n\n\n\n\n\n\nInsights — COOTEFOO Member Dataset Representation\n\n\n\nAll six COOTEFOO members are present in the JOURNALIST dataset, confirming its completeness. The TROUT dataset also includes all members, while FILAH is missing Ed Helpsford, Teddy Goldstein, and Tante Titan — all of whom hold senior or relevant committee positions. We should explore and investigate further."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Draft2.html",
    "href": "Take-home_Ex/Take-home_Ex_2/Draft2.html",
    "title": "Draft2",
    "section": "",
    "text": "Show the code\npacman::p_load(\n  tidyverse,     \n  jsonlite,       \n  tidygraph,      \n  ggraph,         \n  visNetwork,     \n  SmartEDA,      \n  lubridate,   \n  ggforce,        \n  plotly,\n  skimr,\n  ggrepel\n)\n\n\n#Import JSON\n\ntrout     &lt;- fromJSON(\"data/TROUT.json\")\nfilah     &lt;- fromJSON(\"data/FILAH.json\")\njournalist&lt;- fromJSON(\"data/journalist.json\")\n\n#Extract Node\n\ntrout_nodes  &lt;- as_tibble(trout$nodes)\ntrout_edges  &lt;- as_tibble(trout$links)\nfilah_nodes  &lt;- as_tibble(filah$nodes)\nfilah_edges  &lt;- as_tibble(filah$links)\njour_nodes   &lt;- as_tibble(journalist$nodes)\njour_edges   &lt;- as_tibble(journalist$links)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Draft2.html#bias-computation",
    "href": "Take-home_Ex/Take-home_Ex_2/Draft2.html#bias-computation",
    "title": "Draft2",
    "section": "Bias Computation",
    "text": "Bias Computation\n\n# Step 10: Helper to compute average sentiment (“bias”) per person\ncompute_bias &lt;- function(nodes, edges) {\n  # 1. Identify person nodes\n  people &lt;- nodes %&gt;%\n    filter(type == \"entity.person\") %&gt;%\n    select(person = id, name)\n\n  # 2. Keep only edges with valid numeric sentiment\n  edges2 &lt;- edges %&gt;%\n    filter(!is.na(sentiment)) %&gt;%\n    mutate(sentiment = as.numeric(sentiment))\n\n  # 3. Pivot both endpoints into one column\n  sent_long &lt;- edges2 %&gt;%\n    pivot_longer(\n      cols      = c(from, to),\n      names_to  = \"endpoint\",\n      values_to = \"person\"\n    ) %&gt;%\n    filter(person %in% people$person)\n\n  # 4. Compute mean sentiment and count per person\n  sent_long %&gt;%\n    group_by(person) %&gt;%\n    summarise(\n      n_edges  = n(),\n      bias_avg = mean(sentiment),\n      .groups  = \"drop\"\n    ) %&gt;%\n    left_join(people, by = \"person\")\n}"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Draft2.html#compute-bias-scores-for-trout-and-filah",
    "href": "Take-home_Ex/Take-home_Ex_2/Draft2.html#compute-bias-scores-for-trout-and-filah",
    "title": "Draft2",
    "section": "Compute bias scores for TROUT and FILAH",
    "text": "Compute bias scores for TROUT and FILAH\n\n# Step 11a: Prepare per‐source edges with from/to naming\ntrout_edges_bias &lt;- trout_edges_clean  %&gt;% rename(from = source, to = target)\nfilah_edges_bias &lt;- filah_edges_clean  %&gt;% rename(from = source, to = target)\n\n# Step 11: Compute per-member bias in each advocacy dataset\ntrout_bias &lt;- compute_bias(trout_nodes, trout_edges_bias)\nfilah_bias &lt;- compute_bias(filah_nodes, filah_edges_bias)\n\n# Quick peek at the results\ntrout_bias %&gt;% slice_max(order_by = bias_avg, n = 5)\n\n# A tibble: 3 × 4\n  person          n_edges bias_avg name           \n  &lt;chr&gt;             &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;          \n1 Ed Helpsford         20    0.7   Ed Helpsford   \n2 Teddy Goldstein      16    0.344 Teddy Goldstein\n3 Seal                 12    0.108 Seal           \n\nfilah_bias %&gt;% slice_min(order_by = bias_avg, n = 5)\n\n# A tibble: 3 × 4\n  person       n_edges bias_avg name        \n  &lt;chr&gt;          &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;       \n1 Seal              12    0.108 Seal        \n2 Simone Kat        42    0.469 Simone Kat  \n3 Carol Limpet      21    0.655 Carol Limpet"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Draft2.html#visualise-the-distribution-of-bias-scores",
    "href": "Take-home_Ex/Take-home_Ex_2/Draft2.html#visualise-the-distribution-of-bias-scores",
    "title": "Draft2",
    "section": "Visualise the distribution of bias scores",
    "text": "Visualise the distribution of bias scores\n\ndensity plot\n\n# Step 12: Distribution of member bias in each dataset\nlibrary(ggplot2)\n\n# Combine for plotting\nbias_all &lt;- bind_rows(\n  trout_bias %&gt;% mutate(dataset = \"TROUT\"),\n  filah_bias %&gt;% mutate(dataset = \"FILAH\")\n)\n\nggplot(bias_all, aes(x = bias_avg, fill = dataset)) +\n  geom_density(alpha = 0.6) +\n  scale_x_continuous(limits = c(-1, 1)) +\n  scale_fill_manual(values = c(TROUT = \"#1f78b4\", FILAH = \"#33a02c\")) +\n  labs(\n    title = \"Distribution of COOTEFOO Member Bias in TROUT vs FILAH\",\n    x     = \"Average sentiment (–1 = fishing bias, +1 = tourism bias)\",\n    y     = \"Density\",\n    fill  = \"Dataset\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nHighlight the Top ±5 Most Extreme Members\n\n# Step 13: Bar charts of top ±5 extremes\n\nlibrary(forcats)\n\n# Helper to extract extremes\ntop_extremes &lt;- function(df, n = 5) {\n  df %&gt;%\n    arrange(desc(bias_avg)) %&gt;% slice_head(n = n) %&gt;%\n    bind_rows(df %&gt;% arrange(bias_avg) %&gt;% slice_head(n = n)) %&gt;%\n    mutate(direction = if_else(bias_avg &gt; 0, \"Tourism\", \"Fishing\"))\n}\n\n# Plot function\nplot_extremes &lt;- function(df, title, fill_vals) {\n  df %&gt;%\n    mutate(name = fct_reorder(name, bias_avg)) %&gt;%\n    ggplot(aes(x = name, y = bias_avg, fill = direction)) +\n    geom_col() +\n    coord_flip() +\n    scale_y_continuous(limits = c(-1, 1)) +\n    scale_fill_manual(values = fill_vals) +\n    labs(title = title, x = \"Member\", y = \"Average sentiment\") +\n    theme_light() +\n    theme(legend.position = \"none\")\n}\n\n\n\nTROUT Extreme\n\nplot_extremes(\n  top_extremes(trout_bias),\n  title     = \"TROUT: Top ±5 Member Biases\",\n  fill_vals = c(Tourism = \"#e31a1c\", Fishing = \"#348ABD\")\n)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_col()`).\n\n\n\n\n\n\n\n\n\n\n\nFILAH Extreme\n\nplot_extremes(\n  top_extremes(filah_bias),\n  title     = \"FILAH: Top ±5 Member Biases\",\n  fill_vals = c(Tourism = \"#e31a1c\", Fishing = \"#348ABD\")\n)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_col()`).\n\n\n\n\n\n\n\n\n\nBoth advocacy datasets skew positive\n\nThe density plots show TROUT’s bias scores clustered around +0.3 – +0.8 and FILAH’s around +0.4 – +0.9.\nNeither set contains a sizeable fishing-leaning (negative) mass.\n\nTop ±5 extremes are almost all tourism\n\nTROUT’s strongest voices—Ed Helpsford and Teddy Goldstein—both score near +0.9; only Seal dips modestly below zero.\nFILAH’s top advocates—Simone Kat (+0.95) and Carol Limpet (+0.65)—are likewise almost purely tourism-focused; again only Seal shows any slight fishing bias.\n\nAccusations unsupported by their own data\n\nTROUT’s claim of fishing-industry favoritism is contradicted: their own records are overwhelmingly pro-tourism.\nFILAH’s charge of anti-fishing bias similarly finds no support: their dataset also tilts strongly toward tourism."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Draft2.html#compute-centrality-community",
    "href": "Take-home_Ex/Take-home_Ex_2/Draft2.html#compute-centrality-community",
    "title": "Draft2",
    "section": "Compute centrality & community",
    "text": "Compute centrality & community\n\n#── Q2.1 Build combined nodes & edges ────────────────────────────────────────────\ncombined_nodes &lt;- bind_rows(\n  trout_nodes  %&gt;% mutate(source_dataset = \"TROUT\"),\n  filah_nodes  %&gt;% mutate(source_dataset = \"FILAH\"),\n  jour_nodes   %&gt;% mutate(source_dataset = \"JOURNALIST\")\n) %&gt;% distinct(id, .keep_all = TRUE)\n\ncombined_edges &lt;- bind_rows(\n  trout_edges_clean  %&gt;% mutate(source_dataset = \"TROUT\"),\n  filah_edges_clean  %&gt;% mutate(source_dataset = \"FILAH\"),\n  jour_edges_clean   %&gt;% mutate(source_dataset = \"JOURNALIST\")\n) %&gt;%\n  rename(from = source, to = target) %&gt;%\n  filter(from %in% combined_nodes$id, to %in% combined_nodes$id)\n\n\n#── Q2.2 Build graph & compute metrics (incl. bias) ─────────────────────────────\nlibrary(tidygraph)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Helper to compute bias_avg per node\nbias_lookup &lt;- compute_bias(combined_nodes, combined_edges)\n\ncombined_graph &lt;- tbl_graph(\n  nodes    = combined_nodes,\n  edges    = combined_edges,\n  directed = TRUE,\n  node_key = \"id\"\n) %&gt;%\n  activate(nodes) %&gt;%\n  left_join(bias_lookup %&gt;% select(person, bias_avg), by = c(\"id\" = \"person\")) %&gt;%\n  mutate(\n    degree   = centrality_degree(mode = \"all\"),\n    cluster  = group_infomap(),               \n    bias_cat = if_else(bias_avg &gt;= 0, \"tourism\", \"fishing\")\n  )\n\n\n#── Q2.3 Plot distribution of combined bias ──────────────────────────────────────\nlibrary(ggplot2)\n\nggplot(bias_lookup, aes(x = bias_avg)) +\n  geom_density(fill = \"#66c2a5\", alpha = 0.6) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  labs(\n    title = \"Bias Scores Across the Full COOTEFOO Dataset\",\n    x     = \"Average sentiment (–1=fishing, +1=tourism)\",\n    y     = \"Density\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nset.seed(42)\nggraph(combined_graph, layout = \"fr\") +\n  geom_edge_link(aes(colour = source_dataset), alpha = 0.3) +\n  geom_node_point(aes(size = degree, colour = bias_cat), alpha = 0.8) +\n  geom_node_text(aes(label = name), repel = TRUE, size = 2) +\n  scale_edge_colour_manual(values = c(\n    TROUT      = \"#1f78b4\",\n    FILAH      = \"#33a02c\",\n    JOURNALIST = \"#e31a1c\"\n  )) +\n  scale_colour_manual(\n    values      = c(fishing = \"#348ABD\", tourism = \"#E24A33\"),\n    na.value    = \"grey80\",\n    na.translate= FALSE\n  ) +\n  scale_size(range = c(2, 8)) +\n  labs(\n    title       = \"Combined COOTEFOO Network\",\n    edge_colour = \"Record Source\",\n    colour      = \"Member Bias\",\n    size        = \"Node Degree\"\n  ) +\n  theme_void()\n\nWarning: Removed 734 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 692 rows containing missing values or values outside the scale range\n(`geom_text_repel()`).\n\n\nWarning: ggrepel: 27 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\n\n\n\n\nWhen we merge TROUT, FILAH and the journalist’s extra records into a single knowledge graph and examine both the bias‐score density and the force-directed network, we see:\nStrong Pro-Tourism Bias Across the Board\n\nThe density of combined bias scores lies almost entirely above zero (−1 = fishing, +1 = tourism), peaking around +0.6–+0.8.\nThis confirms that, in the full dataset, the committee as a whole spends far more time on tourism-related activities than on fishing.\n\nCentralisation of Tourism-Leaning Members\n\nIn the network layout, the largest nodes (highest degree) are predominantly tourism-leaning (red).\nThese core actors drive the majority of meetings, site visits and plans, indicating they set the agenda.\n\nPeripheral Fishing Voices\n\nFishing-leaning members (blue) appear only on the fringes, with low degree and few connections.\nSeal remains the principal fishing advocate but is marginalised in the overall network.\n\nUnified Record Sources\n\nTROUT and FILAH edges (blue and green) overlap heavily, and the journalist’s supplemental edges (red) fill in gaps but still tie into the same tourism-centred core.\nThere are no distinct sub-communities of fishing activity; all sources reinforce the tourism cluster."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Draft2.html#comparison-table-of-bias-score",
    "href": "Take-home_Ex/Take-home_Ex_2/Draft2.html#comparison-table-of-bias-score",
    "title": "Draft2",
    "section": "Comparison Table of Bias Score",
    "text": "Comparison Table of Bias Score\n\nlibrary(dplyr)\n\nbias_comparison &lt;- trout_bias %&gt;% \n  select(person, bias_trout = bias_avg) %&gt;%\n  full_join(\n    filah_bias %&gt;% select(person, bias_filah = bias_avg),\n    by = \"person\"\n  ) %&gt;%\n  full_join(\n    bias_lookup %&gt;% select(person, bias_combined = bias_avg),\n    by = \"person\"\n  ) %&gt;%\n  # attach human‐readable names\n  left_join(\n    combined_nodes %&gt;% \n      filter(type == \"entity.person\") %&gt;% \n      select(person = id, name),\n    by = \"person\"\n  ) %&gt;%\n  filter(!is.na(name))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Draft2.html#visualise-shifts-with-a-dumbbell-chart",
    "href": "Take-home_Ex/Take-home_Ex_2/Draft2.html#visualise-shifts-with-a-dumbbell-chart",
    "title": "Draft2",
    "section": "Visualise shifts with a “dumbbell” chart",
    "text": "Visualise shifts with a “dumbbell” chart\n\nlibrary(ggplot2)\nlibrary(forcats)\n\n# Prepare for TROUT comparison\ndumbbell_trout &lt;- bias_comparison %&gt;%\n  filter(!is.na(bias_trout)) %&gt;%\n  select(name, Original = bias_trout, Combined = bias_combined) %&gt;%\n  arrange(Original) %&gt;%\n  mutate(name = factor(name, levels = name)) %&gt;%\n  pivot_longer(c(Original, Combined), \n               names_to = \"Dataset\", values_to = \"Bias\")\n\n# Plot\nggplot(dumbbell_trout, aes(y = name, x = Bias, colour = Dataset)) +\n  geom_line(aes(group = name), colour = \"grey70\") +\n  geom_point(size = 3) +\n  scale_colour_manual(values = c(Original = \"#1f78b4\", Combined = \"#33a02c\")) +\n  labs(\n    title = \"TROUT Bias: Original vs Combined\",\n    x     = \"Average sentiment (–1 = fishing, +1 = tourism)\",\n    y     = \"Member\",\n    colour= \"\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Draft2.html#summary",
    "href": "Take-home_Ex/Take-home_Ex_2/Draft2.html#summary",
    "title": "Draft2",
    "section": "Summary",
    "text": "Summary\n\nbias_comparison %&gt;%\n  filter(!is.na(bias_trout)) %&gt;%\n  transmute(\n    Member      = name,\n    `Original (TROUT)`  = round(bias_trout, 3),\n    `Combined`          = round(bias_combined, 3),\n    `Delta`             = round(bias_combined - bias_trout, 3)\n  ) %&gt;%\n  arrange(desc(Delta)) %&gt;%\n  knitr::kable(digits = 3)\n\n\n\n\nMember\nOriginal (TROUT)\nCombined\nDelta\n\n\n\n\nEd Helpsford\n0.700\n0.700\n0\n\n\nSeal\n0.108\n0.108\n0\n\n\nTeddy Goldstein\n0.344\n0.344\n0\n\n\n\n\n\n\nInsights\nWhen we compare each member’s original bias in TROUT (and likewise in FILAH) to their bias in the fully combined dataset, we find no change:\n\nEvery member’s “Combined” score is exactly the same as their “Original” score—∆ = 0.\n\nConclusion:\nAdding the journalist’s extra records neither strengthens nor weakens TROUT’s (or FILAH’s) original accusations. In all cases the fuller dataset leaves each member’s measured bias unchanged, so those initial claims are unchanged when viewed in context of the complete data."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Draft2.html#prepare-the-nodes-edges-for-visnetwork",
    "href": "Take-home_Ex/Take-home_Ex_2/Draft2.html#prepare-the-nodes-edges-for-visnetwork",
    "title": "Draft2",
    "section": "Prepare the nodes & edges for visNetwork",
    "text": "Prepare the nodes & edges for visNetwork\n\n#–– 4.1 Data Prep \n\n# First, join bias_avg into combined_nodes so each row has its own bias\nnodes_vn &lt;- combined_nodes %&gt;%\n  # 1. bring in bias_lookup cleanly\n  left_join(\n    bias_lookup %&gt;% select(person, bias_avg),\n    by = c(\"id\" = \"person\")\n  ) %&gt;%\n  # 2. then transmute\n  transmute(\n    id    = id,\n    label = coalesce(name, id),\n    group = if_else(type == \"entity.person\", \"Member\", \"Event\"),\n    title = if_else(\n      group == \"Member\",\n      paste0(\"&lt;b&gt;\", label, \"&lt;/b&gt;&lt;br&gt;Bias: \", round(bias_avg, 2)),\n      label\n    )\n  )\n\n# And edges_vn as before\nedges_vn &lt;- combined_edges %&gt;%\n  group_by(from, to, source_dataset) %&gt;%\n  summarise(weight = n(), .groups = \"drop\") %&gt;%\n  filter(from != to) %&gt;%\n  mutate(\n    color = case_when(\n      source_dataset == \"TROUT\"      ~ \"#1f78b4\",\n      source_dataset == \"FILAH\"      ~ \"#33a02c\",\n      source_dataset == \"JOURNALIST\" ~ \"#e31a1c\",\n      TRUE                            ~ \"grey80\"\n    ),\n    title = paste0(source_dataset, \" (n=\", weight, \")\")\n  ) %&gt;%\n  select(from, to, weight, color, title)\n\n\n#–– 4.2 Working with layout -------------------------------------------\n\nlibrary(visNetwork)\n\nvisNetwork(nodes_vn, edges_vn, height = \"600px\", width = \"100%\") %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\")\n\n\n\n\n\n\nvisNetwork(nodes_vn, edges_vn) %&gt;%\n  # force‐directed layout via igraph\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  # auto‐colour by `group`, no need to override colour\n  # add legend for those groups\n  visLegend() %&gt;%\n  # freeze the layout\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n#–– 4.4 Working with visual attributes – Edges ------------------------\nvisNetwork(nodes_vn, edges_vn) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  # draw arrowheads and smooth the curves\n  visEdges(\n    arrows = \"to\",\n    smooth = list(enabled = TRUE, type = \"curvedCW\")\n  ) %&gt;%\n  # legend for your edge‐colour mapping (if you manually set edge colors)\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n#–– 4.5 Interactivity --------------------------------------------------\nmember_ids &lt;- nodes_vn %&gt;% \n  filter(group == \"Member\") %&gt;% \n  pull(id)\n\nvisNetwork(nodes_vn, edges_vn) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visOptions(\n    highlightNearest   = list(enabled = TRUE, degree = 1, hover = TRUE),\n    nodesIdSelection   = list(\n      enabled   = TRUE,\n      useLabels = TRUE,\n      values    = member_ids,\n      main      = \"Select a COOTEFOO member\"\n    )\n  ) %&gt;%\n  visLegend(\n    addNodes = data.frame(\n      label = c(\"TROUT\", \"FILAH\", \"JOURNALIST\"),\n      shape = \"square\",\n      color = c(\"#1f78b4\", \"#33a02c\", \"#e31a1c\"),\n      stringsAsFactors = FALSE\n    ),\n    useGroups = FALSE\n  ) %&gt;%\n  visLayout(randomSeed = 42)\n\n\n\n\n\n\n#── P1: Records‐per‐Source Bar Chart ──────────────────────────────────\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\nseal_summary &lt;- combined_edges %&gt;%\n  filter(from == \"Seal\" | to == \"Seal\") %&gt;%\n  count(source_dataset, name = \"n\") %&gt;%\n  complete(source_dataset = c(\"TROUT\",\"FILAH\",\"JOURNALIST\"), fill = list(n=0))\n\nggplot(seal_summary, aes(x = source_dataset, y = n, fill = source_dataset)) +\n  geom_col() +\n  scale_fill_manual(values = c(\n    TROUT      = \"#1f78b4\",\n    FILAH      = \"#33a02c\",\n    JOURNALIST = \"#e31a1c\"\n  )) +\n  labs(\n    title = \"Seal: Records per Source\",\n    x     = \"Dataset\",\n    y     = \"Count\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n#── P2: Seal’s 1‐Step Ego‐Network ────────────────────────────────────\n\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(igraph)\n\n\nAttaching package: 'igraph'\n\n\nThe following object is masked from 'package:plotly':\n\n    groups\n\n\nThe following object is masked from 'package:tidygraph':\n\n    groups\n\n\nThe following objects are masked from 'package:lubridate':\n\n    %--%, union\n\n\nThe following objects are masked from 'package:dplyr':\n\n    as_data_frame, groups, union\n\n\nThe following objects are masked from 'package:purrr':\n\n    compose, simplify\n\n\nThe following object is masked from 'package:tidyr':\n\n    crossing\n\n\nThe following object is masked from 'package:tibble':\n\n    as_data_frame\n\n\nThe following objects are masked from 'package:stats':\n\n    decompose, spectrum\n\n\nThe following object is masked from 'package:base':\n\n    union\n\nlibrary(ggplot2)\n\n# build the ego‐edge table\nseal_edges &lt;- combined_edges %&gt;%\n  filter(from == \"Seal\" | to == \"Seal\") %&gt;%\n  count(from, to, source_dataset, name = \"weight\")\n\n# build the ego‐node table\nseal_nodes &lt;- tibble(id = unique(c(seal_edges$from, seal_edges$to))) %&gt;%\n  left_join(combined_nodes %&gt;% select(id, name), by = \"id\") %&gt;%\n  transmute(\n    id,\n    label = coalesce(name, id),\n    type  = if_else(id == \"Seal\", \"Selected\", \"Neighbour\")\n  )\n\n# create the graph\nseal_graph &lt;- tbl_graph(nodes = seal_nodes, edges = seal_edges, directed = TRUE)\n\n# plot\nset.seed(2025)\nggraph(seal_graph, layout = \"star\") +\n  geom_edge_link(aes(width = weight, colour = source_dataset),\n                 alpha = 0.8,\n                 arrow = arrow(type = \"closed\", length = unit(2, \"mm\"))) +\n  scale_edge_width(range = c(0.2, 1.5), guide = guide_legend(\"Count\")) +\n  scale_edge_colour_manual(name = \"Record Source\", values = c(\n    TROUT      = \"#1f78b4\",\n    FILAH      = \"#33a02c\",\n    JOURNALIST = \"#e31a1c\"\n  )) +\n  geom_node_point(aes(fill = type), shape = 21, size = 6, colour = \"black\") +\n  scale_fill_manual(values = c(Selected = \"firebrick\", Neighbour = \"steelblue\"),\n                    guide = FALSE) +\n  geom_node_text(aes(filter = (id == \"Seal\"), label = label),\n                 repel = TRUE, fontface = \"bold\", size = 4) +\n  theme_void() +\n  labs(title = \"Seal’s Ego-Network\")\n\nWarning: The `guide` argument in `scale_*()` cannot be `FALSE`. This was deprecated in\nggplot2 3.3.4.\nℹ Please use \"none\" instead.\n\n\n\n\n\n\n\n\n\nInsight\nBy examining Seal’s activity through both a count plot and a ego-network, we see that TROUT’s dataset captures only a small fraction of his true engagements. TROUT recorded roughly 13 of Seal’s 79 total events, whereas FILAH and the independent journalist each logged ≈ 66. Visually, Seal’s ego-network fans out almost completely in green (FILAH) and red (journalist) spokes, with only a thin blue wedge for TROUT. Thus, any characterisation of Seal’s “bias” or level of involvement based solely on TROUT’s data is severely misleading; the fuller, combined record reveals a far more active and tourism-focused set of activities than TROUT alone would suggest."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#dataset-overview",
    "href": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#dataset-overview",
    "title": "Take-home Exercise 2",
    "section": "4.4 Dataset Overview",
    "text": "4.4 Dataset Overview\nBefore proceeding, we shall take a look at the edges and nodes data overview using the skim function, with the code below.\n\n\nShow the code\n\n\nFILAHTROUTJOURNALIST\n\n\n\nskim(filah_edges)\n\n\nData summary\n\n\nName\nfilah_edges\n\n\nNumber of rows\n765\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nlist\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nrole\n354\n0.54\n4\n11\n0\n6\n0\n\n\nsource\n0\n1.00\n6\n82\n0\n300\n0\n\n\ntarget\n0\n1.00\n3\n82\n0\n195\n0\n\n\nreason\n656\n0.14\n23\n152\n0\n28\n0\n\n\nstatus\n712\n0.07\n7\n11\n0\n4\n0\n\n\ntime\n600\n0.22\n19\n19\n0\n160\n0\n\n\n\nVariable type: list\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\nmin_length\nmax_length\n\n\n\n\nindustry\n0\n1\n7\n0\n2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nkey\n0\n1.00\n0.00\n0.00\n0\n0.0\n0.0\n0\n0\n▁▁▇▁▁\n\n\nsentiment\n656\n0.14\n0.43\n0.63\n-1\n0.1\n0.5\n1\n1\n▂▁▃▂▇\n\n\n\n\n\n\n\n\nskim(trout_edges)\n\n\nData summary\n\n\nName\ntrout_edges\n\n\nNumber of rows\n378\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nlist\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nrole\n94\n0.75\n4\n11\n0\n6\n0\n\n\nsource\n0\n1.00\n7\n73\n0\n103\n0\n\n\ntarget\n0\n1.00\n3\n73\n0\n133\n0\n\n\nreason\n296\n0.22\n34\n102\n0\n26\n0\n\n\nstatus\n342\n0.10\n7\n11\n0\n5\n0\n\n\ntime\n302\n0.20\n19\n19\n0\n56\n0\n\n\n\nVariable type: list\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\nmin_length\nmax_length\n\n\n\n\nindustry\n0\n1\n6\n0\n2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nkey\n0\n1.00\n0.0\n0.00\n0\n0\n0.0\n0\n0\n▁▁▇▁▁\n\n\nsentiment\n296\n0.22\n0.4\n0.65\n-1\n0\n0.5\n1\n1\n▁▂▅▂▇\n\n\n\n\n\n\n\n\nskim(journalist_edges)\n\n\nData summary\n\n\nName\njournalist_edges\n\n\nNumber of rows\n2436\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nlist\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nrole\n1705\n0.30\n4\n11\n0\n6\n0\n\n\nsource\n0\n1.00\n6\n82\n0\n531\n0\n\n\ntarget\n0\n1.00\n3\n82\n0\n382\n0\n\n\nreason\n2237\n0.08\n23\n152\n0\n42\n0\n\n\nstatus\n2338\n0.04\n7\n11\n0\n5\n0\n\n\ntime\n1073\n0.56\n19\n19\n0\n925\n0\n\n\n\nVariable type: list\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\nmin_length\nmax_length\n\n\n\n\nindustry\n0\n1\n7\n0\n2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nkey\n0\n1.00\n0.00\n0.00\n0\n0.00\n0.00\n0\n0\n▁▁▇▁▁\n\n\nsentiment\n2237\n0.08\n0.56\n0.57\n-1\n0.25\n0.75\n1\n1\n▁▁▂▃▇\n\n\n\n\n\n\n\n\n\nskim(filah_nodes)\n\n\nData summary\n\n\nName\nfilah_nodes\n\n\nNumber of rows\n396\n\n\nNumber of columns\n17\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n15\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntype\n12\n0.97\n4\n19\n0\n8\n0\n\n\ndate\n195\n0.51\n9\n10\n0\n130\n0\n\n\nlabel\n257\n0.35\n9\n82\n0\n139\n0\n\n\nid\n0\n1.00\n3\n82\n0\n396\n0\n\n\nname\n385\n0.03\n4\n24\n0\n11\n0\n\n\nrole\n393\n0.01\n6\n15\n0\n2\n0\n\n\nshort_topic\n382\n0.04\n11\n23\n0\n14\n0\n\n\nlong_topic\n382\n0.04\n11\n73\n0\n14\n0\n\n\nshort_title\n297\n0.25\n33\n82\n0\n99\n0\n\n\nlong_title\n297\n0.25\n25\n80\n0\n71\n0\n\n\nplan_type\n355\n0.10\n6\n12\n0\n9\n0\n\n\nzone\n325\n0.18\n7\n11\n0\n5\n0\n\n\nzone_detail\n388\n0.02\n4\n10\n0\n3\n0\n\n\nstart\n207\n0.48\n8\n8\n0\n71\n0\n\n\nend\n207\n0.48\n8\n8\n0\n86\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nlat\n325\n0.18\n-165.19\n0.63\n-165.96\n-165.68\n-165.59\n-164.53\n-164.34\n▅▆▁▁▇\n\n\nlon\n325\n0.18\n39.38\n0.21\n38.99\n39.26\n39.43\n39.54\n39.67\n▆▁▃▆▇\n\n\n\n\nskim(trout_nodes)\n\n\nData summary\n\n\nName\ntrout_nodes\n\n\nNumber of rows\n164\n\n\nNumber of columns\n17\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n15\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntype\n3\n0.98\n4\n19\n0\n8\n0\n\n\nshort_title\n92\n0.44\n28\n73\n0\n72\n0\n\n\nlong_title\n92\n0.44\n25\n84\n0\n43\n0\n\n\nplan_type\n131\n0.20\n6\n12\n0\n8\n0\n\n\nlabel\n61\n0.63\n7\n73\n0\n103\n0\n\n\nid\n0\n1.00\n3\n73\n0\n164\n0\n\n\ndate\n133\n0.19\n9\n10\n0\n31\n0\n\n\nshort_topic\n150\n0.09\n7\n23\n0\n14\n0\n\n\nlong_topic\n150\n0.09\n11\n73\n0\n14\n0\n\n\nzone\n131\n0.20\n7\n11\n0\n6\n0\n\n\nzone_detail\n146\n0.11\n0\n13\n1\n7\n0\n\n\nname\n133\n0.19\n4\n30\n0\n31\n0\n\n\nrole\n158\n0.04\n6\n15\n0\n4\n0\n\n\nstart\n146\n0.11\n8\n8\n0\n15\n0\n\n\nend\n146\n0.11\n8\n8\n0\n13\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nlat\n131\n0.2\n-165.39\n0.59\n-165.96\n-165.88\n-165.60\n-164.58\n-164.34\n▇▅▁▁▅\n\n\nlon\n131\n0.2\n39.36\n0.24\n38.99\n39.10\n39.43\n39.55\n39.67\n▇▁▃▆▇\n\n\n\n\nskim(journalist_nodes)\n\n\nData summary\n\n\nName\njournalist_nodes\n\n\nNumber of rows\n740\n\n\nNumber of columns\n17\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n15\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntype\n19\n0.97\n4\n19\n0\n8\n0\n\n\ndate\n382\n0.48\n8\n10\n0\n159\n0\n\n\nlabel\n515\n0.30\n7\n82\n0\n225\n0\n\n\nid\n0\n1.00\n3\n82\n0\n740\n0\n\n\nname\n692\n0.06\n4\n30\n0\n48\n0\n\n\nrole\n734\n0.01\n6\n15\n0\n4\n0\n\n\nshort_topic\n725\n0.02\n7\n23\n0\n15\n0\n\n\nlong_topic\n725\n0.02\n11\n73\n0\n15\n0\n\n\nshort_title\n565\n0.24\n27\n82\n0\n175\n0\n\n\nlong_title\n565\n0.24\n25\n97\n0\n115\n0\n\n\nplan_type\n666\n0.10\n6\n12\n0\n10\n0\n\n\nzone\n568\n0.23\n7\n11\n0\n6\n0\n\n\nzone_detail\n705\n0.05\n0\n21\n1\n13\n0\n\n\nstart\n398\n0.46\n8\n8\n0\n122\n0\n\n\nend\n398\n0.46\n8\n8\n0\n164\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nlat\n568\n0.23\n-165.13\n0.63\n-165.96\n-165.61\n-165.57\n-164.53\n-164.33\n▅▆▁▁▇\n\n\nlon\n568\n0.23\n39.38\n0.20\n38.99\n39.26\n39.42\n39.54\n39.67\n▅▁▃▇▆"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#edges-and-nodes-overview",
    "href": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#edges-and-nodes-overview",
    "title": "Take-home Exercise 2",
    "section": "4.4 Edges and Nodes Overview",
    "text": "4.4 Edges and Nodes Overview\nBefore proceeding, we shall take a look at the edges and nodes data overview using the skim function, with the code below.\n\n\nShow the code\n\n\nFILAHTROUTJOURNALIST\n\n\n\nskim(filah_edges)\n\n\nData summary\n\n\nName\nfilah_edges\n\n\nNumber of rows\n765\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nlist\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nrole\n354\n0.54\n4\n11\n0\n6\n0\n\n\nsource\n0\n1.00\n6\n82\n0\n300\n0\n\n\ntarget\n0\n1.00\n3\n82\n0\n195\n0\n\n\nreason\n656\n0.14\n23\n152\n0\n28\n0\n\n\nstatus\n712\n0.07\n7\n11\n0\n4\n0\n\n\ntime\n600\n0.22\n19\n19\n0\n160\n0\n\n\n\nVariable type: list\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\nmin_length\nmax_length\n\n\n\n\nindustry\n0\n1\n7\n0\n2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nkey\n0\n1.00\n0.00\n0.00\n0\n0.0\n0.0\n0\n0\n▁▁▇▁▁\n\n\nsentiment\n656\n0.14\n0.43\n0.63\n-1\n0.1\n0.5\n1\n1\n▂▁▃▂▇\n\n\n\n\n\n\n\n\nskim(trout_edges)\n\n\nData summary\n\n\nName\ntrout_edges\n\n\nNumber of rows\n378\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nlist\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nrole\n94\n0.75\n4\n11\n0\n6\n0\n\n\nsource\n0\n1.00\n7\n73\n0\n103\n0\n\n\ntarget\n0\n1.00\n3\n73\n0\n133\n0\n\n\nreason\n296\n0.22\n34\n102\n0\n26\n0\n\n\nstatus\n342\n0.10\n7\n11\n0\n5\n0\n\n\ntime\n302\n0.20\n19\n19\n0\n56\n0\n\n\n\nVariable type: list\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\nmin_length\nmax_length\n\n\n\n\nindustry\n0\n1\n6\n0\n2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nkey\n0\n1.00\n0.0\n0.00\n0\n0\n0.0\n0\n0\n▁▁▇▁▁\n\n\nsentiment\n296\n0.22\n0.4\n0.65\n-1\n0\n0.5\n1\n1\n▁▂▅▂▇\n\n\n\n\n\n\n\n\nskim(journalist_edges)\n\n\nData summary\n\n\nName\njournalist_edges\n\n\nNumber of rows\n2436\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nlist\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nrole\n1705\n0.30\n4\n11\n0\n6\n0\n\n\nsource\n0\n1.00\n6\n82\n0\n531\n0\n\n\ntarget\n0\n1.00\n3\n82\n0\n382\n0\n\n\nreason\n2237\n0.08\n23\n152\n0\n42\n0\n\n\nstatus\n2338\n0.04\n7\n11\n0\n5\n0\n\n\ntime\n1073\n0.56\n19\n19\n0\n925\n0\n\n\n\nVariable type: list\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\nmin_length\nmax_length\n\n\n\n\nindustry\n0\n1\n7\n0\n2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nkey\n0\n1.00\n0.00\n0.00\n0\n0.00\n0.00\n0\n0\n▁▁▇▁▁\n\n\nsentiment\n2237\n0.08\n0.56\n0.57\n-1\n0.25\n0.75\n1\n1\n▁▁▂▃▇\n\n\n\n\n\n\n\n\n\nskim(filah_nodes)\n\n\nData summary\n\n\nName\nfilah_nodes\n\n\nNumber of rows\n396\n\n\nNumber of columns\n17\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n15\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntype\n12\n0.97\n4\n19\n0\n8\n0\n\n\ndate\n195\n0.51\n9\n10\n0\n130\n0\n\n\nlabel\n257\n0.35\n9\n82\n0\n139\n0\n\n\nid\n0\n1.00\n3\n82\n0\n396\n0\n\n\nname\n385\n0.03\n4\n24\n0\n11\n0\n\n\nrole\n393\n0.01\n6\n15\n0\n2\n0\n\n\nshort_topic\n382\n0.04\n11\n23\n0\n14\n0\n\n\nlong_topic\n382\n0.04\n11\n73\n0\n14\n0\n\n\nshort_title\n297\n0.25\n33\n82\n0\n99\n0\n\n\nlong_title\n297\n0.25\n25\n80\n0\n71\n0\n\n\nplan_type\n355\n0.10\n6\n12\n0\n9\n0\n\n\nzone\n325\n0.18\n7\n11\n0\n5\n0\n\n\nzone_detail\n388\n0.02\n4\n10\n0\n3\n0\n\n\nstart\n207\n0.48\n8\n8\n0\n71\n0\n\n\nend\n207\n0.48\n8\n8\n0\n86\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nlat\n325\n0.18\n-165.19\n0.63\n-165.96\n-165.68\n-165.59\n-164.53\n-164.34\n▅▆▁▁▇\n\n\nlon\n325\n0.18\n39.38\n0.21\n38.99\n39.26\n39.43\n39.54\n39.67\n▆▁▃▆▇\n\n\n\n\nskim(trout_nodes)\n\n\nData summary\n\n\nName\ntrout_nodes\n\n\nNumber of rows\n164\n\n\nNumber of columns\n17\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n15\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntype\n3\n0.98\n4\n19\n0\n8\n0\n\n\nshort_title\n92\n0.44\n28\n73\n0\n72\n0\n\n\nlong_title\n92\n0.44\n25\n84\n0\n43\n0\n\n\nplan_type\n131\n0.20\n6\n12\n0\n8\n0\n\n\nlabel\n61\n0.63\n7\n73\n0\n103\n0\n\n\nid\n0\n1.00\n3\n73\n0\n164\n0\n\n\ndate\n133\n0.19\n9\n10\n0\n31\n0\n\n\nshort_topic\n150\n0.09\n7\n23\n0\n14\n0\n\n\nlong_topic\n150\n0.09\n11\n73\n0\n14\n0\n\n\nzone\n131\n0.20\n7\n11\n0\n6\n0\n\n\nzone_detail\n146\n0.11\n0\n13\n1\n7\n0\n\n\nname\n133\n0.19\n4\n30\n0\n31\n0\n\n\nrole\n158\n0.04\n6\n15\n0\n4\n0\n\n\nstart\n146\n0.11\n8\n8\n0\n15\n0\n\n\nend\n146\n0.11\n8\n8\n0\n13\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nlat\n131\n0.2\n-165.39\n0.59\n-165.96\n-165.88\n-165.60\n-164.58\n-164.34\n▇▅▁▁▅\n\n\nlon\n131\n0.2\n39.36\n0.24\n38.99\n39.10\n39.43\n39.55\n39.67\n▇▁▃▆▇\n\n\n\n\nskim(journalist_nodes)\n\n\nData summary\n\n\nName\njournalist_nodes\n\n\nNumber of rows\n740\n\n\nNumber of columns\n17\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n15\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntype\n19\n0.97\n4\n19\n0\n8\n0\n\n\ndate\n382\n0.48\n8\n10\n0\n159\n0\n\n\nlabel\n515\n0.30\n7\n82\n0\n225\n0\n\n\nid\n0\n1.00\n3\n82\n0\n740\n0\n\n\nname\n692\n0.06\n4\n30\n0\n48\n0\n\n\nrole\n734\n0.01\n6\n15\n0\n4\n0\n\n\nshort_topic\n725\n0.02\n7\n23\n0\n15\n0\n\n\nlong_topic\n725\n0.02\n11\n73\n0\n15\n0\n\n\nshort_title\n565\n0.24\n27\n82\n0\n175\n0\n\n\nlong_title\n565\n0.24\n25\n97\n0\n115\n0\n\n\nplan_type\n666\n0.10\n6\n12\n0\n10\n0\n\n\nzone\n568\n0.23\n7\n11\n0\n6\n0\n\n\nzone_detail\n705\n0.05\n0\n21\n1\n13\n0\n\n\nstart\n398\n0.46\n8\n8\n0\n122\n0\n\n\nend\n398\n0.46\n8\n8\n0\n164\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nlat\n568\n0.23\n-165.13\n0.63\n-165.96\n-165.61\n-165.57\n-164.53\n-164.33\n▅▆▁▁▇\n\n\nlon\n568\n0.23\n39.38\n0.20\n38.99\n39.26\n39.42\n39.54\n39.67\n▅▁▃▇▆"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#question-1",
    "href": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#question-1",
    "title": "Take-home Exercise 2",
    "section": "6.1 Question 1",
    "text": "6.1 Question 1\n\n\n\n\n\n\nThe Question\n\n\n\nBased on the datasets that TROUT & FILAH have provided, use visual analytics to determine if each group’s accusations are supported by their own record set. In other words, develop a visualization to highlight bias (if present) in TROUT & FILAH’s datasets. Is there evidence of bias in the COOTEFOO member actions in either dataset?\n\n\n\n6.1.1 🧭 How Sentiment Is Analyzed\nTo analyze potential bias in each dataset, we extract sentiment scores linked to COOTEFOO members using the participant edges. These scores capture how positively or negatively each member responded to topics associated with Fishing or Tourism.\nHowever, sentiment alone is not enough—context matters. That’s why we also examine the associated industry and reason columns to classify the sentiment into high-level categories:\n\nFishing\nTourism\nOther\n\nThis approach ensures that when a member expresses a positive sentiment, we understand toward what. For example, a +1 score could mean “pro-fishing” in FILAH or “pro-tourism” in TROUT, depending on the topic context. We classify topics accordingly and compute average sentiment scores per member and per dataset.\n\n\n6.1.2 Overall Sentiment - Bias Score Ridgeline\nThis ridgeline chart shows how the aggregate sentiment scores (bias scores) are distributed across the TROUT and FILAH datasets. A score near +1 suggests strong support for tourism, while a score near –1 suggests alignment with fishing interests. We will use blue colour to represent FILAH and yellow colour for TROUT.\n\n\nShow the code\n\n\n# Define COOTEFOO member names\ncootefoo_members &lt;- c(\"Seal\", \"Simone Kat\", \"Carol Limpet\", \n                      \"Teddy Goldstein\", \"Ed Helpsford\", \"Tante Titan\")\n\n# Function to classify topic by industry\nassign_topic_category &lt;- function(industry, reason) {\n  industry &lt;- tolower(industry)\n  reason   &lt;- tolower(reason)\n\n  case_when(\n    str_detect(reason, \"housing\") ~ \"Other\",\n    str_detect(industry, \"tourism|tourist|wharf|travel|harbor|marina|port\") |\n      str_detect(reason, \"tourism|tourist|wharf|travel|harbor|marina|port\") ~ \"Tourism\",\n    str_detect(industry, \"fishing|vessel|dock|crane|fish\") |\n      str_detect(reason, \"fishing|vessel|dock|crane|fish\") ~ \"Fishing\",\n    TRUE ~ \"Other\"\n  )\n}\n\n# Ensure edge tables exist\nfilah_edges &lt;- as_tibble(filah$links)\ntrout_edges &lt;- as_tibble(trout$links)\n\n# Compute FILAH sentiment by member\nfilah_sentiment_by_member &lt;- filah_edges %&gt;%\n  filter(role == \"participant\",\n         !is.na(sentiment),\n         target %in% cootefoo_members,\n         !is.na(industry),\n         industry != \"character(0)\",\n         industry != \"\") %&gt;%\n  mutate(\n    industry_clean = str_replace_all(industry, \"c\\\\(|\\\\)|\\\"\", \"\"),\n    industry_group = assign_topic_category(industry_clean, reason)\n  ) %&gt;%\n  group_by(target) %&gt;%\n  summarise(avg_sentiment = mean(sentiment, na.rm = TRUE), .groups = \"drop\")\n\n# Compute TROUT sentiment by member\ntrout_sentiment_by_member &lt;- trout_edges %&gt;%\n  filter(role == \"participant\",\n         !is.na(sentiment),\n         target %in% cootefoo_members,\n         !is.na(industry),\n         industry != \"character(0)\",\n         industry != \"\") %&gt;%\n  mutate(\n    industry_clean = str_replace_all(industry, \"c\\\\(|\\\\)|\\\"\", \"\"),\n    industry_group = assign_topic_category(industry_clean, reason)\n  ) %&gt;%\n  group_by(target) %&gt;%\n  summarise(avg_sentiment = mean(sentiment, na.rm = TRUE), .groups = \"drop\")\n\n# Combine both datasets\nbias_all &lt;- bind_rows(\n  filah_sentiment_by_member %&gt;%\n    rename(name = target, bias_avg = avg_sentiment) %&gt;%\n    mutate(dataset = \"FILAH\"),\n  \n  trout_sentiment_by_member %&gt;%\n    rename(name = target, bias_avg = avg_sentiment) %&gt;%\n    mutate(dataset = \"TROUT\")\n)\n\n# Filter to only COOTEFOO members\nbias_all_filtered &lt;- bias_all %&gt;%\n  filter(name %in% cootefoo_members)\n\n# Plot ridgeline chart of overall sentiment\nggplot(bias_all_filtered, aes(x = bias_avg, y = fct_rev(dataset), fill = dataset)) +\n  geom_density_ridges(\n    alpha = 0.7, \n    scale = 0.95, \n    color = \"white\",\n    size = 0.3\n  ) +\n  scale_fill_manual(values = c(\"TROUT\" = \"#ffcc00\", \"FILAH\" = \"#1f78b4\")) +\n  scale_x_continuous(\n    limits = c(-1, 1),\n    breaks = c(-1, 0, 1),\n    labels = c(\"Fishing Bias\", \"Neutral\", \"Tourism Bias\")\n  ) +\n  labs(\n    title = \"Overall COOTEFOO Member Sentiment — FILAH vs TROUT\",\n    subtitle = \"Average sentiment score distribution by dataset\",\n    x = \"Bias Score\",\n    y = NULL\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = \"none\",\n    axis.text.y = element_text(face = \"bold\"),\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(margin = margin(b = 10))\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\n\n\n\nFILAH member sentiments are centered around neutral-to-positive, with a slight lean toward tourism despite the group’s fishing-oriented stance. TROUT shows more polarized sentiment, including a member with strong pro-fishing sentiment (unexpected for a tourism advocacy group). This suggests each dataset presents a selective narrative that may not fully align with their stated agenda.\n\n\nTo better understand the potential bias, it’s important to examine how each individual COOTEFOO member is represented in the FILAH and TROUT datasets. To do this, we generate member-level sentiment visualizations to explore how sentiment scores vary across members and industries in each dataset.\n\n\n6.1.3 Individual Member Sentiment\nThis chart breaks down how individual members in the FILAH and TROUT dataset express sentiment toward Fishing and Tourism.\n\n\nShow the code\n\n\nFILAHTROUT\n\n\n\n# Clean and transform FILAH sentiment data\nfilah_sentiment_by_member &lt;- filah_edges %&gt;%\n  filter(role == \"participant\",\n         !is.na(sentiment),\n         target %in% cootefoo_members,\n         !is.na(industry),\n         industry != \"character(0)\",\n         industry != \"\") %&gt;%\n  mutate(\n    industry_clean = str_replace_all(industry, \"c\\\\(|\\\\)|\\\"\", \"\"),\n    industry_clean = tolower(industry_clean),\n    industry_group = assign_topic_category(industry_clean, reason)\n  ) %&gt;%\n  group_by(target, industry_group) %&gt;%\n  summarise(\n    avg_sentiment = mean(sentiment, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  filter(industry_group %in% c(\"Fishing\", \"Tourism\"))\n\n# Combined dot plot (no facets)\nggplot(filah_sentiment_by_member, aes(x = avg_sentiment, y = fct_rev(target))) +\n  geom_point(aes(color = industry_group), size = 5, position = position_dodge(width = 0.5)) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray60\") +\n  scale_color_manual(values = c(\"Fishing\" = \"#1f78b4\", \"Tourism\" = \"#ffcc00\")) +\n  scale_x_continuous(limits = c(-1, 1), breaks = seq(-1, 1, 0.5)) +\n  labs(\n    title = \"COOTEFOO Member Sentiment — FILAH Dataset\",\n    subtitle = \"Each dot shows average sentiment toward Fishing or Tourism\",\n    x = \"Sentiment Score (–1: Oppose, +1: Support)\",\n    y = NULL,\n    color = \"Industry\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    axis.text.y = element_text(face = \"bold\"),\n    legend.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\n\n\n\nOnly 3 COOTEFOO members appear in FILAH: Seal, Carol Limpet, and Simone Kat. Seal remains neutral, while the other two show surprisingly positive sentiment toward tourism, not fishing. This undermines FILAH’s claimed fishing advocacy and reveals significant sampling bias in member representation.\n\n\n\n\n\n# Clean and transform TROUT sentiment data\ntrout_sentiment_by_member &lt;- trout_edges %&gt;%\n  filter(role == \"participant\",\n         !is.na(sentiment),\n         target %in% cootefoo_members,\n         !is.na(industry),\n         industry != \"character(0)\",\n         industry != \"\") %&gt;%\n  mutate(\n    industry_clean = str_replace_all(industry, \"c\\\\(|\\\\)|\\\"\", \"\"),\n    industry_clean = tolower(industry_clean),\n    industry_group = assign_topic_category(industry_clean, reason)\n  ) %&gt;%\n  group_by(target, industry_group) %&gt;%\n  summarise(\n    avg_sentiment = mean(sentiment, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  filter(industry_group %in% c(\"Fishing\", \"Tourism\"))\n\n\nggplot(trout_sentiment_by_member, aes(x = avg_sentiment, y = fct_rev(target))) +\n  geom_point(aes(color = industry_group), size = 5, position = position_dodge(width = 0.5)) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray60\") +\n  scale_color_manual(values = c(\"Fishing\" = \"#1f78b4\", \"Tourism\" = \"#ffcc00\")) +\n  scale_x_continuous(limits = c(-1, 1), breaks = seq(-1, 1, 0.5)) +\n  labs(\n    title = \"COOTEFOO Member Sentiment — TROUT Dataset\",\n    subtitle = \"Each dot shows average sentiment toward Fishing or Tourism\",\n    x = \"Sentiment Score (–1: Oppose, +1: Support)\",\n    y = NULL,\n    color = \"Industry\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    axis.text.y = element_text(face = \"bold\"),\n    legend.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\n\n\n\nTROUT features Teddy Goldstein, Seal, and Ed Helpsford. Teddy expresses strong pro-fishing sentiment, contradicting TROUT’s tourism alignment. Seal is again neutral. TROUT omits several members (e.g., Simone Kat), and the presence of fishing support in a tourism dataset suggests selective inclusion to make a point or to avoid contradicting narratives.\n\n\n\n\n\n\n\n\n\nQ1 Answer — Evidence of Bias in TROUT & FILAH\n\n\n\nBoth the TROUT and FILAH datasets reveal potential indicators of bias, particularly in the way sentiment is recorded and which COOTEFOO members are included or omitted.\nFILAH includes only a subset of COOTEFOO members and primarily reflects positive sentiment toward tourism, which appears at odds with its stated advocacy for the fishing sector.\nTROUT, while including more members, emphasizes voices that are critical of tourism and supportive of fishing, suggesting a narrative aligned with their stance.\nThese observations suggest that each dataset may reflect the priorities and framing choices of its source organization. However, the patterns of inclusion, omission, and sentiment warrant deeper exploration.\nTo better understand the extent and implications of these biases, the subsequent questions (Q2–Q4) will integrate a more complete dataset and examine structural, geographic, and individual-level behavior in context."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#geographic-data-overview-oceanus-zones-and-road-network",
    "href": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#geographic-data-overview-oceanus-zones-and-road-network",
    "title": "Take-home Exercise 2",
    "section": "4.5 Geographic Data Overview: Oceanus Zones and Road Network",
    "text": "4.5 Geographic Data Overview: Oceanus Zones and Road Network\nTo supplement the knowledge graph analysis with spatial context, we incorporate two geographic datasets:\n\nOceanus Zone Map (oceanus_map.geojson): Defines land use zones across the island (e.g., residential, tourism, industrial).\nRoad Network (road_map.json): Represents the transportation infrastructure using node-link format with GPS coordinates.\n\nThese datasets provide spatial grounding for COOTEFOO committee activities, including meeting venues and travel routes. This helps identify geographic bias or clustering in committee decisions and movement patterns.\n\n\n📂 Load Spatial Data\n\n\n# Load geographic data\noceanus_map &lt;- st_read(\"data/oceanus_map.geojson\")\n\nReading layer `oceanus_map' from data source \n  `D:\\shartiono\\ISSS608-VAA\\Take-home_Ex\\Take-home_Ex_2\\data\\oceanus_map.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 29 features and 6 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: -167.0654 ymin: 38.07452 xmax: -163.2723 ymax: 40.67775\nGeodetic CRS:  WGS 84\n\nroad_json &lt;- fromJSON(\"data/road_map.json\", simplifyDataFrame = TRUE)\n\n\nWe then prepare the road network for inspection and visualize both datasets to confirm spatial integrity.\n\n\nShow the Code\n\n\n# Prepare road network data\nroad_nodes &lt;- road_json$nodes %&gt;%\n  mutate(id = as.character(id), x = longitude, y = latitude)\n\nroad_edges &lt;- road_json$links %&gt;%\n  rename(from = source, to = target) %&gt;%\n  mutate(across(c(from, to), as.character))\n\n# Preview structure of both datasets\nglimpse(oceanus_map)\n\nRows: 29\nColumns: 7\n$ Name                 &lt;chr&gt; \"Suna Island\", \"Thalassa Retreat\", \"Makara Shoal\"…\n$ Description          &lt;chr&gt; \"Large island of Oceanus\", \"Smaller island of Oce…\n$ type                 &lt;chr&gt; \"Entity.Location.Region\", \"Entity.Location.Region…\n$ Kind                 &lt;chr&gt; \"Island\", \"Island\", \"Island\", \"Island\", \"Fishing …\n$ Activities           &lt;list&gt; \"Residential\", \"Residential\", \"Recreation\", &lt;\"To…\n$ fish_species_present &lt;list&gt; &lt;&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, &lt;\"Cod/Gadus n.specificatae\", \"Bi…\n$ geometry             &lt;GEOMETRY [°]&gt; MULTIPOLYGON (((-166.0111 3..., MULTIPOL…\n\nglimpse(road_nodes)\n\nRows: 2,664\nColumns: 8\n$ zone      &lt;chr&gt; \"commercial\", \"commercial\", \"commercial\", \"commercial\", \"com…\n$ city_name &lt;chr&gt; \"Lomark\", \"Lomark\", \"Lomark\", \"Lomark\", \"Lomark\", \"Lomark\", …\n$ longitude &lt;dbl&gt; -165.5773, -165.5769, -165.5768, -165.5780, -165.5787, -165.…\n$ latitude  &lt;dbl&gt; 39.53082, 39.52992, 39.53158, 39.53109, 39.53041, 39.54057, …\n$ id        &lt;chr&gt; \"48344443\", \"48344507\", \"48524007\", \"2135056313\", \"212811835…\n$ name      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Har…\n$ x         &lt;dbl&gt; -165.5773, -165.5769, -165.5768, -165.5780, -165.5787, -165.…\n$ y         &lt;dbl&gt; 39.53082, 39.52992, 39.53158, 39.53109, 39.53041, 39.54057, …\n\n# Visual preview: Oceanus zones and road network\npar(mfrow = c(1, 2))\nplot(oceanus_map$geometry,\n     main = \"Oceanus Zones (GeoJSON)\",\n     col = \"lightblue\", border = \"gray\")\nplot(road_nodes$x, road_nodes$y,\n     main = \"Road Network Nodes\",\n     pch = 19, col = \"darkred\",\n     xlab = \"Longitude\", ylab = \"Latitude\")\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#question-2",
    "href": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#question-2",
    "title": "Take-home Exercise 2",
    "section": "6.2 Question 2",
    "text": "6.2 Question 2\n\n\n\n\n\n\nThe Question\n\n\n\n\nAs a journalist, Ms. Moray would like a more complete picture of the COOTEFOO’s actions and activities. She has arranged to combine the data provided by TROUT and FILAH into a single knowledge graph along with additional records. Design visual analytics approaches for this combined knowledge graph to see how members of COOTEFOO spend their time. Is the committee as a whole biased? Provide visual evidence for your conclusions.\n\n\n\nTo answer this, we integrated the TROUT, FILAH, and JOURNALIST datasets into a single comprehensive knowledge graph. We use both network centrality and geographic activity patterns to determine whether the committee’s actions reflect a collective bias toward either industry.\n\n6.2.1 Network Graph Analysis\nThe combined knowledge graph visualizes:\n\nEdges by source dataset (TROUT = orange, FILAH = brown, JOURNALIST = green)\nNodes by sentiment bias (yellow = tourism, blue = fishing, grey = unknown)\nNode size by degree centrality (activity/influence level)\n\n\n\nShow the code\n\n\n# Compute average sentiment per person\ncompute_bias &lt;- function(nodes, edges) {\n  edges %&gt;%\n    filter(!is.na(sentiment), str_detect(role, \"participant\")) %&gt;%\n    filter(to %in% nodes$id) %&gt;%\n    group_by(person = to) %&gt;%\n    summarise(\n      bias_avg = mean(sentiment, na.rm = TRUE),\n      n_obs    = n(),\n      .groups  = \"drop\"\n    )\n}\nbias_lookup &lt;- compute_bias(combined_nodes, combined_edges)\n\n# Build combined graph with metrics\ncombined_graph &lt;- tbl_graph(\n  nodes = combined_nodes,\n  edges = combined_edges,\n  directed = TRUE,\n  node_key = \"id\"\n) %&gt;%\n  activate(nodes) %&gt;%\n  left_join(bias_lookup, by = c(\"id\" = \"person\")) %&gt;%\n  mutate(\n    degree   = centrality_degree(mode = \"all\"),\n    cluster  = group_infomap(),\n    bias_cat = case_when(\n      is.na(bias_avg) ~ NA_character_,\n      bias_avg &gt;= 0   ~ \"tourism\",\n      TRUE            ~ \"fishing\"\n    )\n  )\n\n# Plot the network graph\nset.seed(42)\n\nggraph(combined_graph, layout = \"fr\") +\n  geom_edge_link(aes(colour = source_dataset), alpha = 0.3) +\n  geom_node_point(aes(\n    size = degree,\n    colour = bias_cat,\n    alpha = if_else(is.na(bias_cat), 0.05, 0.9)\n  )) +\n  geom_node_text(aes(label = label), repel = TRUE, size = 2.5, alpha = 0.8) +\n  scale_edge_colour_manual(values = c(\n    TROUT      = \"#FFA500\",  # Orange\n    FILAH      = \"#8B4513\",  # Brown\n    JOURNALIST = \"#33a02c\"   # Green\n  )) +\n  scale_colour_manual(values = c(\n    fishing = \"#1f78b4\",     # Blue\n    tourism = \"#FFD700\"      # Yellow\n  ), na.value = \"grey80\") +\n  scale_alpha_identity() +\n  scale_size(range = c(2, 8)) +\n  labs(\n    title       = \"Combined COOTEFOO Network (Bias Highlighted, Others Dimmed)\",\n    edge_colour = \"Source Dataset\",\n    colour      = \"Bias Category\",\n    size        = \"Degree Centrality\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation — Network Graph (Bias Highlighted)\n\n\n\nThe visualization shows that nodes with tourism-aligned sentiment (yellow) tend to be larger and more centrally located, while fishing-aligned nodes (blue) appear less connected and more peripheral. The JOURNALIST dataset contributes most of the connections across the network. This view reflects how sentiment categories are structurally embedded within the overall committee activity network.\n\n\n\n\n\n6.2.2 Geographic Activity Analysis\nTo complement the network analysis, we examine the spatial movement of COOTEFOO members using trip records tagged with latitude and longitude. These records help reveal where board members travel across Oceanus and whether their movement patterns correspond to certain zone types (e.g., tourism, fishing, commercial).\nThis analysis leverages:\n\nThe Oceanus Zone Map (oceanus_map.geojson) from Section 4.5, which defines land use classification.\nThe Road Network (road_map.json), which supports spatial orientation but is not directly used in the trip overlay.\n\nBefore extracting trip locations, we enrich the combined node dataset with latitude and longitude fields from the original JOURNALIST dataset to ensure coordinate data is available for spatial mapping.\n\n\nShow the code\n\n\n# Enrich combined_nodes with lat/lon from the journalist dataset\ncombined_nodes &lt;- combined_nodes %&gt;%\n  left_join(\n    journalist_nodes %&gt;% select(id, lat, lon),\n    by = \"id\"\n  )\n\n\nThe plot below shows board member trip locations, color-coded by member name, overlaid on the zoning map.\n\n\nShow the code\n\n\n# Step 1: Identify board members\nboard_members &lt;- combined_nodes %&gt;%\n  filter(type == \"entity.person\") %&gt;%\n  select(person_id = id, member_name = label)\n\n# Step 2: Extract trip-to-person edges\ntrip_participant_edges &lt;- combined_edges %&gt;%\n  filter(role == \"participant\") %&gt;%\n  inner_join(board_members, by = c(\"to\" = \"person_id\")) %&gt;%\n  filter(str_detect(from, \"trip\")) %&gt;%\n  select(trip_id = from, member_name)\n\n# Step 3: Match trip nodes with location coordinates\ncootefoo_trip_locs &lt;- combined_nodes %&gt;%\n  filter(type == \"trip\", !is.na(lat), !is.na(lon)) %&gt;%\n  select(id, label, lat, lon) %&gt;%\n  inner_join(trip_participant_edges, by = c(\"id\" = \"trip_id\")) %&gt;%\n  mutate(\n    latitude = as.numeric(lat),\n    longitude = as.numeric(lon)\n  )\n\n# Step 4: Plot board member trips on Oceanus zone map\nggplot() +\n  geom_sf(data = oceanus_map, aes(fill = Kind), color = \"black\", alpha = 0.4) +\n  geom_point(data = cootefoo_trip_locs, aes(x = longitude, y = latitude, color = member_name),\n             size = 2, alpha = 0.8) +\n  labs(\n    title = \"COOTEFOO Board Member Trips Over Oceanus Zones\",\n    fill = \"Zone Type\",\n    color = \"Board Member\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation — Geographic View of Board Member Trips\n\n\n\nThis map shows COOTEFOO board member trips (black dots) overlaid on the Oceanus zoning map. Most trip locations are clustered near Island, City, and Ecological Preserve zones, while relatively few are located within or near Fishing Ground zones (blue areas). This spatial distribution suggests that board member activity, as captured through recorded trips, is more concentrated in non-fishing zones, offering geographic context to complement the network analysis.\n\n\n\n\n6.2.3 Member Involvement by Dataset and Activity Type\nTo complement the earlier network graph, we insert a bar plot showing how many times each COOTEFOO member appears across different datasets and the type of engagement (e.g., meeting, site visit, etc.). This mirrors the approach seen in the reference link and helps Ms. Moray understand member activity depth and focus areas.\n\n\nShow the code\n\n\n# Filter only COOTEFOO members and participant roles\ncootefoo_members &lt;- c(\"Seal\", \"Simone Kat\", \"Carol Limpet\", \n                      \"Teddy Goldstein\", \"Ed Helpsford\", \"Tante Titan\")\n\nmember_activity &lt;- combined_edges %&gt;%\n  filter(from %in% cootefoo_members | to %in% cootefoo_members) %&gt;%\n  pivot_longer(cols = c(from, to), names_to = \"direction\", values_to = \"person\") %&gt;%\n  filter(person %in% cootefoo_members) %&gt;%\n  count(person, source_dataset, name = \"n\") %&gt;%\n  mutate(source_dataset = factor(source_dataset, levels = c(\"TROUT\", \"FILAH\", \"JOURNALIST\")))\n\n# Bar chart\nggplot(member_activity, aes(x = fct_reorder(person, n), y = n, fill = source_dataset)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"TROUT\" = \"#ffcc00\", \"FILAH\" = \"#1f78b4\", \"JOURNALIST\" = \"#33a02c\")) +\n  labs(\n    title = \"COOTEFOO Member Activity by Dataset\",\n    subtitle = \"Each bar shows the number of activities a member is linked to\",\n    x = \"Member\",\n    y = \"Activity Count\",\n    fill = \"Dataset\"\n  ) +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation — Dataset Participation by Member\n\n\n\nThis bar chart reveals notable disparities in dataset coverage across COOTEFOO members:\nTante Titan, Ed Helpsford, and Teddy Goldstein have substantial engagement logged in the JOURNALIST dataset but are largely missing or underrepresented in FILAH, confirming a sampling bias in that subset.\nSimone Kat and Carol Limpet are prominently featured in FILAH, but absent or minimally recorded in TROUT, suggesting selective emphasis favoring tourism-leaning narratives.\nSeal is the only member with relatively balanced representation across all three datasets, though still with variations in total activity count.\nThe JOURNALIST dataset captures the most complete engagement for every member, underscoring its critical value in correcting incomplete perspectives presented by TROUT and FILAH.\nTogether, this visualization strengthens the argument that no individual dataset tells the full story, and evaluating COOTEFOO’s behavior in context requires triangulating across all sources.\n\n\n\n\n\n\n\n\n\nAnswer — Question 2: Is COOTEFOO as a whole biased?\n\n\n\nThe combined analysis of network structure, geographic activity, and member engagement reveals that COOTEFOO as a whole shows operational bias toward tourism-related initiatives, even if this bias is not overtly declared.\nIn the network graph, nodes aligned with tourism sentiment (yellow) are more central and active than those aligned with fishing (blue), suggesting that discussions and plans about tourism are more influential within the committee. The JOURNALIST dataset, which contributes the majority of connections, provides the clearest evidence of this structural tilt.\nThe geographic trip overlay confirms this finding: COOTEFOO members predominantly travel to zones associated with urban development, islands, and ecological preserves—areas tied to tourism and infrastructure—while fishing grounds receive minimal attention.\nFinally, the member activity bar chart highlights sampling bias in the advocacy datasets. Some members (e.g., Teddy Goldstein, Tante Titan) are active in the full dataset but underrepresented or omitted in TROUT and FILAH. This disparity reinforces the importance of using the full dataset when assessing committee behavior.\nTaken together, the evidence suggests that while COOTEFOO may not be explicitly biased, its actions and priorities in practice show a systematic leaning toward tourism interests, largely underreported in the subset datasets."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#question-3",
    "href": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#question-3",
    "title": "Take-home Exercise 2",
    "section": "6.3 Question 3",
    "text": "6.3 Question 3\n\n\n\n\n\n\nThe Question\n\n\n\n\nThe TROUT and FILAH datasets are incomplete. Use your visualizations to compare and contrast conclusions drawn from the TROUT and FILAH datasets separately with behaviors in the whole dataset. Are the accusations of TROUT strengthened, weakened or unchanged when taken in context of the whole dataset?\n\n\n\nWe compare how sentiment toward Fishing and Tourism is represented across TROUT, FILAH, and the full JOURNALIST dataset. This reveals whether either subset presents a skewed or selective view of COOTEFOO member attitudes. Discrepancies in sentiment distributions highlight potential bias or missing context.\n\n\nShow the code\n\n\n# Helper function to classify topic by industry\nassign_topic_category &lt;- function(industry, reason) {\n  industry &lt;- tolower(industry)\n  reason   &lt;- tolower(reason)\n  \n  case_when(\n    str_detect(reason, \"housing\") ~ \"Other\",\n    str_detect(industry, \"tourism|tourist|wharf|travel|harbor|marina|port\") |\n      str_detect(reason,   \"tourism|tourist|wharf|travel|harbor|marina|port\") ~ \"Tourism\",\n    str_detect(industry, \"fishing|vessel|dock|crane|fish\") |\n      str_detect(reason,   \"fishing|vessel|dock|crane|fish\") ~ \"Fishing\",\n    TRUE ~ \"Other\"\n  )\n}\n\n# Function to extract sentiment records\nextract_sentiments &lt;- function(edge_df, dataset_label) {\n  edge_df %&gt;%\n    filter(role == \"participant\", !is.na(sentiment), !is.na(industry)) %&gt;%\n    mutate(\n      industry_group = assign_topic_category(industry, reason),\n      dataset = dataset_label\n    ) %&gt;%\n    select(target, sentiment, industry_group, dataset)\n}\n\n# Apply to all three datasets\ntrout_sentiments &lt;- extract_sentiments(trout_edges, \"TROUT\")\nfilah_sentiments &lt;- extract_sentiments(filah_edges, \"FILAH\")\njournalist_sentiments &lt;- extract_sentiments(journalist_edges, \"JOURNALIST\")\n\n# Combine into one tidy dataset\ncombined_sentiments &lt;- bind_rows(trout_sentiments, filah_sentiments, journalist_sentiments)\n\n\n\n6.3.1🎻 Sentiment Distribution by Industry and Dataset\nThis violin plot shows how each dataset portrays sentiment toward Fishing, Tourism, and Other topics. Wider shapes reflect more sentiment records. By comparing TROUT and FILAH to the full JOURNALIST dataset, we can identify patterns of omission or emphasis.\n\n\nShow the code\n\n\n### Violin Plot for Sentiment Comparison\n\nggplot(combined_sentiments, aes(x = industry_group, y = sentiment, fill = dataset)) +\n  geom_violin(trim = FALSE, scale = \"width\", alpha = 0.7) +\n  geom_jitter(aes(color = dataset), size = 1.4, width = 0.2, alpha = 0.5, show.legend = FALSE) +\n  facet_wrap(~ industry_group, scales = \"free_x\") +\n  scale_fill_manual(values = c(\n    \"TROUT\" = \"#ffcc00\",\n    \"FILAH\" = \"#1f78b4\",\n    \"JOURNALIST\" = \"#33a02c\"\n  )) +\n  scale_color_manual(values = c(\n    \"TROUT\" = \"#ffcc00\",\n    \"FILAH\" = \"#1f78b4\",\n    \"JOURNALIST\" = \"#33a02c\"\n  )) +\n  labs(\n    title = \"Sentiment Toward Industry by Dataset\",\n    subtitle = \"Violin plots comparing TROUT, FILAH, and JOURNALIST sentiment\",\n    x = \"Industry Category\",\n    y = \"Sentiment Score\",\n    fill = \"Dataset\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\n\n\n\nTeddy Goldstein stands out with conflicting sentiment: strongly positive in TROUT and JOURNALIST on Fishing, but negative in TROUT on Tourism. FILAH omits his sentiment entirely. Tante Titan, absent from TROUT and FILAH, only appears in the JOURNALIST dataset. These gaps and contradictions suggest each subset selectively represents member behavior, underscoring the importance of cross-dataset analysis.\n\n\n\n\n\n6.3.2 📊 Volume of Sentiment Records per Dataset\nThis stacked bar chart complements the violin plot by showing the total number of sentiment-tagged records per dataset, broken down by industry category. This directly exposes which datasets are underreporting certain industries, which helps explain why the sentiment heatmap has missing cells. The codes are shown below:\n\n\nShow the code\n\n\n# Count total sentiment records by dataset and industry\nsentiment_volume &lt;- combined_sentiments %&gt;%\n  filter(industry_group %in% c(\"Fishing\", \"Tourism\", \"Other\")) %&gt;%\n  count(dataset, industry_group)\n\n# Plot: Stacked bar chart of sentiment volume\nggplot(sentiment_volume, aes(x = dataset, y = n, fill = industry_group)) +\n  geom_col(position = \"stack\") +\n  scale_fill_manual(values = c(\n    \"Fishing\" = \"#1f78b4\",\n    \"Tourism\" = \"#ffcc00\",\n    \"Other\" = \"gray70\"\n  )) +\n  labs(\n    title = \"Total Sentiment Records by Dataset and Industry\",\n    subtitle = \"Highlights coverage gaps in FILAH and TROUT\",\n    x = \"Dataset\",\n    y = \"Number of Sentiment Records\",\n    fill = \"Industry Category\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(margin = margin(b = 10)),\n    legend.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation — Sentiment Volume by Dataset\n\n\n\nThe bar chart reveals major coverage disparities:\n\nJOURNALIST provides the most complete sentiment coverage, with nearly equal attention to Tourism, Fishing, and Other topics.\nFILAH heavily emphasizes Tourism, despite being a pro-fishing group, and contributes minimal sentiment records on Fishing.\nTROUT presents a narrower distribution with relatively few records overall, and underrepresents Tourism sentiment compared to expectations.\n\nThese gaps directly affect how COOTEFOO member sentiment appears in analysis—highlighting why sentiment conclusions based solely on TROUT or FILAH may be unreliable.\n\n\n\n\n\n6.3.3🔍 COOTEFOO Member-Level Sentiment Heatmap\nThis heatmap compares average sentiment scores for each COOTEFOO member by dataset and industry category. It highlights inconsistencies across data sources and reveals how individual member portrayals shift depending on which dataset is used. Blank cells represent missing sentiment data.\n\n\nShow the code\n\n\n# Define COOTEFOO members\ncootefoo_members &lt;- c(\"Seal\", \"Simone Kat\", \"Carol Limpet\", \n                      \"Teddy Goldstein\", \"Ed Helpsford\", \"Tante Titan\")\n\n# Clean sentiment data: keep only COOTEFOO member targets\nmember_sentiments &lt;- combined_sentiments %&gt;%\n  filter(target %in% cootefoo_members) %&gt;%\n  group_by(target, dataset, industry_group) %&gt;%\n  summarise(avg_sentiment = mean(sentiment, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  mutate(column_id = paste(dataset, industry_group, sep = \"_\"))\n\n# Reshape to wide format for heatmap-style display\nsentiment_matrix &lt;- member_sentiments %&gt;%\n  select(name = target, column_id, avg_sentiment) %&gt;%\n  pivot_wider(names_from = column_id, values_from = avg_sentiment)\n\n# Create actual heatmap-style plot\nsentiment_matrix_long &lt;- sentiment_matrix %&gt;%\n  pivot_longer(-name, names_to = \"Dataset_Industry\", values_to = \"Sentiment\")\n\nggplot(sentiment_matrix_long, aes(x = Dataset_Industry, y = fct_rev(name), fill = Sentiment)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = round(Sentiment, 2)), size = 3) +\n  scale_fill_gradient2(low = \"#b2182b\", mid = \"white\", high = \"#2166ac\", midpoint = 0, na.value = \"grey90\") +\n  labs(\n    title = \"Sentiment Presence by COOTEFOO Member and Dataset\",\n    subtitle = \"Missing = not scored in that dataset/industry\",\n    x = \"Dataset × Industry\", y = NULL, fill = \"Avg Sentiment\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\n\n\n\n\n\n# Add count\nmember_sentiments &lt;- combined_sentiments %&gt;%\n  filter(target %in% cootefoo_members) %&gt;%\n  group_by(target, dataset, industry_group) %&gt;%\n  summarise(\n    avg_sentiment = mean(sentiment, na.rm = TRUE),\n    n_obs = n(),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(label = paste0(round(avg_sentiment, 2), \" (\", n_obs, \")\"),\n         column_id = paste(dataset, industry_group, sep = \"_\"))\n\n\n\n\n\n\n\nObservation\n\n\n\nThe heatmap shows that while all six COOTEFOO members are structurally present in the TROUT dataset, only Seal, Ed Helpsford, and Teddy Goldstein have sentiment scores recorded. Tante Titan, Simone Kat, and Carol Limpet have no sentiment values in TROUT, despite being scored in the JOURNALIST dataset with moderate-to-positive sentiment—especially toward Tourism. This partial sentiment coverage skews TROUT’s representation, amplifying voices like Goldstein’s (strong pro-fishing) while silencing others who may offer more balanced or tourism-aligned perspectives.\n\n\n\n\n\n\n\n\n\nAnswer — Question 3: Are TROUT’s Accusations Strengthened, Weakened, or Unchanged?\n\n\n\nTROUT asserts that the COOTEFOO board is biased in favor of the fishing industry and resistant to tourism development. However, this claim is weakened when TROUT is analyzed alongside the more complete JOURNALIST dataset.\nThe violin plot reveals that TROUT records include sentiment values toward both Fishing and Tourism, with several members expressing positive views on tourism, contradicting the central accusation. The sentiment distribution in TROUT actually mirrors the broader sentiment balance seen in JOURNALIST, undermining the claim that the board opposes tourism outright.\nThe heatmap further highlights internal inconsistency: while all six COOTEFOO members are structurally present in TROUT, only three (Seal, Ed Helpsford, and Teddy Goldstein) have sentiment data attached. The omission of sentiment values for Simone Kat, Carol Limpet, and Tante Titan—who are documented in the JOURNALIST dataset as expressing tourism-positive or balanced views—suggests that TROUT selectively includes voices that reinforce a fishing-biased narrative.\nFinally, the sentiment volume bar chart shows that TROUT has fewer total sentiment records than JOURNALIST, and contributes only a narrow slice of the available context. FILAH, although more voluminous, also shows an unexpected skew toward tourism sentiment and underrepresents fishing.\nTaken together, these visualizations demonstrate that TROUT’s dataset lacks key sentiment context and omits critical voices. Its accusations appear to be based on incomplete and selectively framed evidence. The more comprehensive JOURNALIST data offers a more nuanced and balanced picture, with COOTEFOO members engaging meaningfully with both industries.\nTROUT’s narrative of systemic fishing bias within COOTEFOO is not substantiated when assessed in the full context. The omission of member sentiment, especially those favoring tourism, diminishes the credibility of the accusation."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#combining-all-graphs-for-section-6-analysis",
    "href": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#combining-all-graphs-for-section-6-analysis",
    "title": "Take-home Exercise 2",
    "section": "5.3 Combining All Graphs for Section 6 Analysis",
    "text": "5.3 Combining All Graphs for Section 6 Analysis\nTo support the comparative analysis in Section 6, we now construct a combined dataset from the three sources. This step ensures that all relevant nodes and edges are merged into a single structure for downstream bias evaluation using sentiment, centrality, and spatial analysis. The code is shown below:\n\n\nShow the code\n\n\n# Combine cleaned nodes and edges for integrated analysis\ncombined_nodes &lt;- bind_rows(\n  trout_data$nodes_cleaned %&gt;% mutate(source_dataset = \"TROUT\"),\n  filah_data$nodes_cleaned %&gt;% mutate(source_dataset = \"FILAH\"),\n  journalist_data$nodes_cleaned %&gt;% mutate(source_dataset = \"JOURNALIST\")\n) %&gt;%\n  distinct(id, .keep_all = TRUE)\n\ncombined_edges &lt;- bind_rows(\n  trout_data$edges_cleaned %&gt;% mutate(source_dataset = \"TROUT\"),\n  filah_data$edges_cleaned %&gt;% mutate(source_dataset = \"FILAH\"),\n  journalist_data$edges_cleaned %&gt;% mutate(source_dataset = \"JOURNALIST\")\n) %&gt;%\n  filter(from %in% combined_nodes$id, to %in% combined_nodes$id)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#question-4",
    "href": "Take-home_Ex/Take-home_Ex_2/Take-home_Ex_2.html#question-4",
    "title": "Take-home Exercise 2",
    "section": "6.4 Question 4",
    "text": "6.4 Question 4\n\n\n\n\n\n\nThe Question\n\n\n\nDesign a visualization that allows Ms. Moray to pick a person and highlight the differences in that person’s behavior as illustrated through the different datasets. Focus on the contrast in the story each dataset tells.\n\nPick at least one COOTEFOO member accused by TROUT.\nIllustrate how your understanding of their activities changed when using the more complete dataset.\nWhat are the key pieces of evidence missing from the original TROUT data that most influenced the change in judgement?\nWhose behaviors are most impacted by sampling bias when looking at the FILAH dataset in context of the other data?\nIllustrate the bias of the FILAH data in the context of the whole dataset.\n\n\n\n\n6.4.1 🗺️ Building on Q1–Q3\nIn Q1–Q3, we uncovered evidence of selective omission and framing across the TROUT and FILAH datasets. TROUT emphasized a pro-fishing stance while omitting planning-related reasoning. FILAH omitted multiple active members entirely. Using the full JOURNALIST dataset clarified that COOTEFOO member sentiment is more nuanced, with participation spanning fishing, tourism, and broader infrastructure concerns.\n\n\n6.4.2👤 Focal Member: Teddy Goldstein\nTeddy was explicitly accused by TROUT of resisting tourism and favoring fishing. His record is:\n\nPresent in TROUT and JOURNALIST datasets\nAbsent from FILAH, indicating sampling bias\nShows contrasting portrayals between datasets\n\n\n\n6.4.3 📊 Teddy’s Activity Record Count\nWe begin with a simple bar chart showing the number of Teddy’s engagements recorded across the three datasets.\n\n\nShow the code\n\n\nteddy_summary &lt;- combined_edges %&gt;%\n  filter(from == \"Teddy Goldstein\" | to == \"Teddy Goldstein\") %&gt;%\n  count(source_dataset, name = \"n\") %&gt;%\n  complete(source_dataset = c(\"TROUT\", \"FILAH\", \"JOURNALIST\"), fill = list(n = 0))\n\nggplot(teddy_summary, aes(x = source_dataset, y = n, fill = source_dataset)) +\n  geom_col() +\n  scale_fill_manual(values = c(\n    \"TROUT\" = \"#ffcc00\",\n    \"FILAH\" = \"#1f78b4\",\n    \"JOURNALIST\" = \"#33a02c\"\n  )) +\n  labs(\n    title = \"Teddy Goldstein: Records per Source\",\n    x = \"Dataset\",\n    y = \"Number of Records\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\n\n\n\nTROUT logs only a fraction of Teddy’s total engagements. He is completely omitted from FILAH. The complete JOURNALIST dataset records the majority of his interactions, showing a more balanced presence across tourism and fishing topics.\n\n\n\n\n\n6.4.4 📊 Ego Network Visualization (static)\nWe now generate a static circular ego network using ggraph to illustrate how Teddy’s interactions differ by dataset.\n\n\nShow the code\n\n\n# Prepare ego network\nteddy_edges &lt;- combined_edges %&gt;%\n  filter(from == \"Teddy Goldstein\" | to == \"Teddy Goldstein\") %&gt;%\n  count(from, to, source_dataset, name = \"weight\")\n\nteddy_nodes &lt;- tibble(id = unique(c(teddy_edges$from, teddy_edges$to))) %&gt;%\n  left_join(combined_nodes %&gt;% select(id, label), by = \"id\") %&gt;%\n  mutate(group = if_else(id == \"Teddy Goldstein\", \"Selected\", \"Neighbour\"))\n\nteddy_graph &lt;- tbl_graph(nodes = teddy_nodes, edges = teddy_edges, directed = TRUE)\n\n# Static circular layout\nset.seed(2025)\nggraph(teddy_graph, layout = \"circle\") +\n  geom_edge_link(aes(width = weight, colour = source_dataset),\n                 alpha = 0.8, arrow = arrow(type = \"closed\", length = unit(2, \"mm\"))) +\n  scale_edge_colour_manual(values = c(\n    \"TROUT\" = \"#ffcc00\",\n    \"FILAH\" = \"#1f78b4\",\n    \"JOURNALIST\" = \"#33a02c\"\n  )) +\n  geom_node_point(aes(fill = group), shape = 21, size = 6, color = \"black\") +\n  geom_node_text(aes(label = label), repel = TRUE, size = 3.5) +\n  scale_fill_manual(values = c(\"Selected\" = \"firebrick\", \"Neighbour\" = \"steelblue\")) +\n  scale_edge_width(range = c(0.4, 1.5)) +\n  theme_void() +\n  labs(title = \"Teddy Goldstein’s Ego Network by Dataset\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\n\n\n\nThis ego network shows a dense range of interactions for Teddy across many nodes. Green links (JOURNALIST) dominate, showing diverse participation, while yellow links (TROUT) are limited and skewed. Blue links (FILAH) are absent, confirming Teddy is completely missing from that dataset.\n\n\n\n\n\n6.4.5 🕸️ Ego Network (Interactive View)\nTo explore interactions in more depth, we include an interactive visNetwork version of the ego network.\n\n\nShow the code\n\n\n# Step 1: Prepare edges for visNetwork\nteddy_vis_edges &lt;- teddy_edges %&gt;%\n  mutate(\n    color = case_when(\n      source_dataset == \"TROUT\"      ~ \"#ffcc00\",       # Yellow\n      source_dataset == \"FILAH\"      ~ \"#1f78b4\",       # Blue\n      source_dataset == \"JOURNALIST\" ~ \"#33a02c\",       # Green\n      TRUE                           ~ \"gray\"\n    ),\n    title = paste0(\"Source: \", source_dataset, \"&lt;br&gt;Weight: \", weight),\n    arrows = \"to\"\n  ) %&gt;%\n  select(from, to, color, title, arrows)\n\n# Step 2: Prepare nodes for visNetwork\nteddy_vis_nodes &lt;- teddy_nodes %&gt;%\n  mutate(\n    label = coalesce(label, id),\n    shape = if_else(group == \"Selected\", \"star\", \"dot\"),\n    color = if_else(group == \"Selected\", \"firebrick\", \"gray\"),\n    title = paste0(\"&lt;b&gt;\", label, \"&lt;/b&gt;\")\n  )\n\n# Step 3: Generate interactive network\nvisNetwork(teddy_vis_nodes, teddy_vis_edges, height = \"700px\", width = \"100%\") %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visOptions(\n    highlightNearest = list(enabled = TRUE, degree = 1, hover = TRUE),\n    nodesIdSelection = list(enabled = TRUE, main = \"Explore the network\")\n  ) %&gt;%\n  visLegend(\n    addNodes = data.frame(\n      label = c(\"TROUT\", \"FILAH\", \"JOURNALIST\", \"Selected\", \"Neighbour\"),\n      shape = c(\"dot\", \"dot\", \"dot\", \"star\", \"dot\"),\n      color = c(\"#ffcc00\", \"#1f78b4\", \"#33a02c\", \"firebrick\", \"gray\"),\n      stringsAsFactors = FALSE\n    ),\n    useGroups = FALSE\n  ) %&gt;%\n  visLayout(randomSeed = 42)\n\n\n\n\n\n\n\n\n\n\n\nObservation\n\n\n\nThis interactive visualization highlights Teddy Goldstein’s personal network across the three datasets.\n\nTotal interactions: 65 unique edge connections.\n\nTROUT edges (yellow): 17 links recorded.\nJOURNALIST edges (green): 48 links — the majority of Teddy’s engagement.\nFILAH edges (blue): 0 — Teddy is completely absent in FILAH.\n\nNode count: The network spans 65 nodes, with Teddy at the center (star shape, red), and all connected nodes shown as blue circles (neighbours).\nVisual pattern:\n\nThe green spokes (JOURNALIST) radiate outward in all directions, connecting Teddy to a wide variety of meetings, trips, and discussions.\nYellow spokes (TROUT) are limited and concentrated to fewer topics, visually representing the constrained narrative captured by TROUT.\nBlue spokes (FILAH) are absent, confirming FILAH omitted Teddy’s contributions entirely.\n\nThis layout offers immediate insight into the disparity in data coverage: the richness of Teddy’s documented activities in JOURNALIST versus his underrepresentation in TROUT and total absence in FILAH. The presence of both pro-tourism and pro-fishing topics in the green connections reveals the nuance in Teddy’s actual role, which is oversimplified or missing in the two advocacy datasets.\n\n\n\n\n\n\n\n\n\n\nAnswer — Q4\n\n\n\nTeddy Goldstein is portrayed in the TROUT dataset as a clear opponent of tourism and a staunch supporter of fishing. While this characterization is partially accurate, it is derived from a limited subset of records. The full JOURNALIST dataset reveals a more nuanced profile: Teddy supports fishing-related infrastructure for its economic and operational efficiency, but also expresses concern over environmental impacts and participates in planning-related discussions. His critiques of tourism appear conditional—focused on instances where tourism development may jeopardize fishing livelihoods or environmental stability. By contrast, the FILAH dataset omits Teddy entirely, excluding his perspective from the narrative altogether.\nThis case highlights how bias emerges not only from framing, but from omission. The lack of context in TROUT, particularly missing reasoning behind sentiments, obscures Teddy’s planning and sustainability concerns. FILAH’s total exclusion removes any possibility of evaluating his role. When viewed in full, Teddy is best described as strategically pro-fishing, rather than blindly opposed to tourism. His story exemplifies why data completeness and contextual richness are critical for fair interpretation, and why relying on partial datasets can lead to misleading or polarized conclusions."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08A/Hands-on_Ex08A.html",
    "href": "Hands-on_Ex/Hands-on_Ex08A/Hands-on_Ex08A.html",
    "title": "Hands-on Exercise 08A",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nIn this chapter, you will learn how to plot functional and truthful choropleth maps by using an R package called tmap package.\n\n\n\n\n\n\nTip\n\n\n\nIt is advisable for you to read the functional description of each function before using them.\n\n\n\n\n\nIn this hands-on exercise, the key R package use is tmap package in R. Beside tmap package, four other R packages will be used. They are:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package.\nThe code chunk below will be used to install and load these packages in RStudio.\n\npacman::p_load(sf, tmap, tidyverse)\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that, we only need to install tidyverse instead of readr, tidyr and dplyr individually.\n\n\n\n\n\n\n\nTwo data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\n\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `D:\\shartiono\\ISSS608-VAA\\Hands-on_Ex\\Hands-on_Ex08A\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nYou can examine the content of mpsz by using the code chunk below.\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n\nNext, we will import respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popagsex.\nThe task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\n\n\nBefore a thematic map can be prepared, you are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\n\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nThings to learn from the code chunk above:\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")\n\n\n\n\n\n\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\n\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons(\"DEPENDENCY\", \n              style = \"quantile\", \n              n = 5,\n              palette = \"Blues\",\n              title = \"Dependency Ratio\") +\n  tm_layout(\n    title = \"Distribution of Dependency Ratio by Planning Subzone\",\n    title.size = 1.2,  # reduce if too large\n    title.position = c(\"center\", \"top\"),\n    outer.margins = c(0.08, 0.02, 0.02, 0.02),  # add margin at top\n    frame = TRUE\n  ) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", size = 2) +\n  tm_grid(alpha = 0.2) +\n  tm_credits(\n    \"Source: Planning Sub-zone boundary from Urban Redevelopment Authority (URA)\\n and Population data from Department of Statistics (DOS)\", \n    position = c(\"left\", \"bottom\")\n  )\n\n\n\n\n\n\n\n\nIn the following sub-section, we will share with you tmap functions that used to plot these elements.\n\n\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\n\n\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey.\n\n\n\n\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons(\"DEPENDENCY\",\n              style = \"fixed\",\n              breaks = c(0, 5, 10, 15, 20),\n              palette = \"Blues\",\n              colorNA = \"grey70\",\n              textNA = \"Missing\",\n              title = \"DEPENDENCY\") +\n  tm_borders(lwd = 0.01) +\n  tm_layout(\n    frame = TRUE,\n    outer.margins = c(0.02, 0.02, 0.02, 0.02),\n    legend.title.size = 1.0,\n    legend.text.size = 0.8,\n    title = NULL  # Ensures no title is rendered\n  )\n\n\n\n\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.\n\n\n\n\n\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n\nThe code chunk below shows a quantile data classification that used 5 classes.\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons(\"DEPENDENCY\",\n              style = \"jenks\",\n              n = 5,\n              palette = \"Blues\",\n              title = \"Dependency Ratio\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons(\"DEPENDENCY\",\n              style = \"equal\",  # Equal interval bins\n              n = 5,\n              palette = \"Blues\",\n              title = \"Dependency Ratio\") +\n  tm_borders(alpha = 0.5)  # Border transparency\n\n\n\n\n\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\nWarning: Maps Lie!\nDIY: Using what you had learned, prepare choropleth maps by using different classification methods supported by tmap and compare their differences.\nDIY: Preparing choropleth maps by using similar classification method but with different numbers of classes (i.e. 2, 6, 10, 20). Compare the output maps, what observation can you draw?\nUsing the KMeans classification method, the dependency ratios are grouped based on cluster similarity rather than even ranges or frequencies. This highlights distinct regional clusters but may produce uneven class widths, which can obscure subtle local variations.\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons(\"DEPENDENCY\",\n              style = \"kmeans\",  # try \"equal\", \"jenks\", etc.\n              n = 5,\n              palette = \"Blues\",\n              title = \"KMeans Classification\") +\n  tm_layout(title = \"KMeans Method\", legend.outside = TRUE)\n\n\n\n\n\n\n\n\nUsing the Jenks classification method with 10 classes, the choropleth map reveals finer distinctions in dependency ratios, highlighting several subzones with distinctly higher ratios (e.g., in the north and east). This method effectively emphasizes natural groupings while maintaining readability. The code chunks are below:\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons(\"DEPENDENCY\",\n              style = \"jenks\",\n              n = 10,\n              palette = \"Blues\",\n              title = \"Jenks, 10 classes\") +\n  tm_layout(title = \"Jenks Method, 10 Classes\", legend.outside = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code chunk below.\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons(\"DEPENDENCY\",\n              style = \"fixed\",\n              breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00),\n              palette = \"Blues\",\n              alpha = 0.8,\n              title = \"Dependency\") +\n  tm_borders(alpha = 0.5)  # correct argument\n\n\n\n\n\n\n\n\n\n\n\n\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons(\"DEPENDENCY\",\n              style = \"quantile\",\n              n = 5,\n              palette = \"Greens\",\n              title = \"Dependency\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nNotice that the choropleth map is shaded in green.\nTo reverse the colour shading, add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nNotice that the colour scheme has been reversed.\n\n\n\n\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe code chunk below shows the classic style is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n\n\n\n\n\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons(\"DEPENDENCY\", \n              style = \"quantile\",\n              n = 5,\n              palette = \"Blues\",\n              title = \"Dependency Ratio\") +\n  tm_layout(\n    title = \"Distribution of Dependency Ratio by Planning Subzone\",\n    frame = TRUE\n  ) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", size = 2) +\n  tm_grid(alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authority (URA)\\n and Population data from Department of Statistics (DOS)\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nTo reset the default style, refer to the code chunk below.\n\ntmap_style(\"white\")\n\n\n\n\n\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n\nIn this example, multiple small choropleth maps are created by using tm_facets().\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\n\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\n\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08A/Hands-on_Ex08A.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex08A/Hands-on_Ex08A.html#overview",
    "title": "Hands-on Exercise 08A",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nIn this chapter, you will learn how to plot functional and truthful choropleth maps by using an R package called tmap package.\n\n\n\n\n\n\nTip\n\n\n\nIt is advisable for you to read the functional description of each function before using them."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08A/Hands-on_Ex08A.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex08A/Hands-on_Ex08A.html#getting-started",
    "title": "Hands-on Exercise 08A",
    "section": "",
    "text": "In this hands-on exercise, the key R package use is tmap package in R. Beside tmap package, four other R packages will be used. They are:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package.\nThe code chunk below will be used to install and load these packages in RStudio.\n\npacman::p_load(sf, tmap, tidyverse)\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that, we only need to install tidyverse instead of readr, tidyr and dplyr individually."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08A/Hands-on_Ex08A.html#importing-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex08A/Hands-on_Ex08A.html#importing-data-into-r",
    "title": "Hands-on Exercise 08A",
    "section": "",
    "text": "Two data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\n\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `D:\\shartiono\\ISSS608-VAA\\Hands-on_Ex\\Hands-on_Ex08A\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nYou can examine the content of mpsz by using the code chunk below.\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n\nNext, we will import respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popagsex.\nThe task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\n\n\nBefore a thematic map can be prepared, you are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\n\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nThings to learn from the code chunk above:\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08A/Hands-on_Ex08A.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex08A/Hands-on_Ex08A.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-on Exercise 08A",
    "section": "",
    "text": "Two approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\n\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons(\"DEPENDENCY\", \n              style = \"quantile\", \n              n = 5,\n              palette = \"Blues\",\n              title = \"Dependency Ratio\") +\n  tm_layout(\n    title = \"Distribution of Dependency Ratio by Planning Subzone\",\n    title.size = 1.2,  # reduce if too large\n    title.position = c(\"center\", \"top\"),\n    outer.margins = c(0.08, 0.02, 0.02, 0.02),  # add margin at top\n    frame = TRUE\n  ) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", size = 2) +\n  tm_grid(alpha = 0.2) +\n  tm_credits(\n    \"Source: Planning Sub-zone boundary from Urban Redevelopment Authority (URA)\\n and Population data from Department of Statistics (DOS)\", \n    position = c(\"left\", \"bottom\")\n  )\n\n\n\n\n\n\n\n\nIn the following sub-section, we will share with you tmap functions that used to plot these elements.\n\n\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\n\n\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey.\n\n\n\n\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons(\"DEPENDENCY\",\n              style = \"fixed\",\n              breaks = c(0, 5, 10, 15, 20),\n              palette = \"Blues\",\n              colorNA = \"grey70\",\n              textNA = \"Missing\",\n              title = \"DEPENDENCY\") +\n  tm_borders(lwd = 0.01) +\n  tm_layout(\n    frame = TRUE,\n    outer.margins = c(0.02, 0.02, 0.02, 0.02),\n    legend.title.size = 1.0,\n    legend.text.size = 0.8,\n    title = NULL  # Ensures no title is rendered\n  )\n\n\n\n\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.\n\n\n\n\n\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n\nThe code chunk below shows a quantile data classification that used 5 classes.\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons(\"DEPENDENCY\",\n              style = \"jenks\",\n              n = 5,\n              palette = \"Blues\",\n              title = \"Dependency Ratio\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons(\"DEPENDENCY\",\n              style = \"equal\",  # Equal interval bins\n              n = 5,\n              palette = \"Blues\",\n              title = \"Dependency Ratio\") +\n  tm_borders(alpha = 0.5)  # Border transparency\n\n\n\n\n\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\nWarning: Maps Lie!\nDIY: Using what you had learned, prepare choropleth maps by using different classification methods supported by tmap and compare their differences.\nDIY: Preparing choropleth maps by using similar classification method but with different numbers of classes (i.e. 2, 6, 10, 20). Compare the output maps, what observation can you draw?\nUsing the KMeans classification method, the dependency ratios are grouped based on cluster similarity rather than even ranges or frequencies. This highlights distinct regional clusters but may produce uneven class widths, which can obscure subtle local variations.\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons(\"DEPENDENCY\",\n              style = \"kmeans\",  # try \"equal\", \"jenks\", etc.\n              n = 5,\n              palette = \"Blues\",\n              title = \"KMeans Classification\") +\n  tm_layout(title = \"KMeans Method\", legend.outside = TRUE)\n\n\n\n\n\n\n\n\nUsing the Jenks classification method with 10 classes, the choropleth map reveals finer distinctions in dependency ratios, highlighting several subzones with distinctly higher ratios (e.g., in the north and east). This method effectively emphasizes natural groupings while maintaining readability. The code chunks are below:\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons(\"DEPENDENCY\",\n              style = \"jenks\",\n              n = 10,\n              palette = \"Blues\",\n              title = \"Jenks, 10 classes\") +\n  tm_layout(title = \"Jenks Method, 10 Classes\", legend.outside = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code chunk below.\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons(\"DEPENDENCY\",\n              style = \"fixed\",\n              breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00),\n              palette = \"Blues\",\n              alpha = 0.8,\n              title = \"Dependency\") +\n  tm_borders(alpha = 0.5)  # correct argument\n\n\n\n\n\n\n\n\n\n\n\n\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons(\"DEPENDENCY\",\n              style = \"quantile\",\n              n = 5,\n              palette = \"Greens\",\n              title = \"Dependency\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nNotice that the choropleth map is shaded in green.\nTo reverse the colour shading, add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nNotice that the colour scheme has been reversed.\n\n\n\n\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe code chunk below shows the classic style is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n\n\n\n\n\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons(\"DEPENDENCY\", \n              style = \"quantile\",\n              n = 5,\n              palette = \"Blues\",\n              title = \"Dependency Ratio\") +\n  tm_layout(\n    title = \"Distribution of Dependency Ratio by Planning Subzone\",\n    frame = TRUE\n  ) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type = \"8star\", size = 2) +\n  tm_grid(alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authority (URA)\\n and Population data from Department of Statistics (DOS)\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nTo reset the default style, refer to the code chunk below.\n\ntmap_style(\"white\")\n\n\n\n\n\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n\nIn this example, multiple small choropleth maps are created by using tm_facets().\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08A/Hands-on_Ex08A.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex08A/Hands-on_Ex08A.html#reference",
    "title": "Hands-on Exercise 08A",
    "section": "",
    "text": "tmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\n\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\n\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08A/data/geospatial/MPSZ-2019.html",
    "href": "Hands-on_Ex/Hands-on_Ex08A/data/geospatial/MPSZ-2019.html",
    "title": "ISSS608-VAA",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08B/Hands-on_Ex08B.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex08B/Hands-on_Ex08B.html#learning-outcome",
    "title": "Hands-on Exercise 08B",
    "section": "1.1 Learning outcome",
    "text": "1.1 Learning outcome\nBy the end of this hands-on exercise, you will acquire the following skills by using appropriate R packages:\n\nTo import an aspatial data file into R.\nTo convert it into simple point feature data frame and at the same time, to assign an appropriate projection reference to the newly create simple point feature data frame.\nTo plot interactive proportional symbol maps."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08B/data/geospatial/MPSZ-2019.html",
    "href": "Hands-on_Ex/Hands-on_Ex08B/data/geospatial/MPSZ-2019.html",
    "title": "ISSS608-VAA",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08B/Hands-on_Ex08B.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex08B/Hands-on_Ex08B.html#the-data",
    "title": "Hands-on Exercise 08B",
    "section": "3.1 The data",
    "text": "3.1 The data\nThe data set use for this hands-on exercise is called SGPools_svy21. The data is in csv file format.\nFigure below shows the first 15 records of SGPools_svy21.csv. It consists of seven columns. The XCOORD and YCOORD columns are the x-coordinates and y-coordinates of SingPools outlets and branches. They are in Singapore SVY21 Projected Coordinates System."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08B/Hands-on_Ex08B.html#data-import-and-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex08B/Hands-on_Ex08B.html#data-import-and-preparation",
    "title": "Hands-on Exercise 08B",
    "section": "3.2 Data Import and Preparation",
    "text": "3.2 Data Import and Preparation\nThe code chunk below uses read_csv() function of readr package to import SGPools_svy21.csv into R as a tibble data frame called sgpools.\n\nsgpools &lt;- read_csv(\"data/aspatial/SGPools_svy21.csv\")\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe code chunk below shows list() is used to do the job.\n\nlist(sgpools) \n\n[[1]]\n# A tibble: 306 × 7\n   NAME           ADDRESS POSTCODE XCOORD YCOORD `OUTLET TYPE` `Gp1Gp2 Winnings`\n   &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Mar… 2 Bayf…    18972 30842. 29599. Branch                        5\n 2 Livewire (Res… 26 Sen…    98138 26704. 26526. Branch                       11\n 3 SportsBuzz (K… Lotus …   738078 20118. 44888. Branch                        0\n 4 SportsBuzz (P… 1 Sele…   188306 29777. 31382. Branch                       44\n 5 Prime Serango… Blk 54…   552542 32239. 39519. Branch                        0\n 6 Singapore Poo… 1A Woo…   731001 21012. 46987. Branch                        3\n 7 Singapore Poo… Blk 64…   370064 33990. 34356. Branch                       17\n 8 Singapore Poo… Blk 88…   370088 33847. 33976. Branch                       16\n 9 Singapore Poo… Blk 30…   540308 33910. 41275. Branch                       21\n10 Singapore Poo… Blk 20…   560202 29246. 38943. Branch                       25\n# ℹ 296 more rows\n\n\nNotice that the sgpools data in tibble data frame and not the common R data frame."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08B/Hands-on_Ex08B.html#creating-a-sf-data-frame-from-an-aspatial-data-frame",
    "href": "Hands-on_Ex/Hands-on_Ex08B/Hands-on_Ex08B.html#creating-a-sf-data-frame-from-an-aspatial-data-frame",
    "title": "Hands-on Exercise 08B",
    "section": "3.3 Creating a sf data frame from an aspatial data frame",
    "text": "3.3 Creating a sf data frame from an aspatial data frame\nThe code chunk below converts sgpools data frame into a simple feature data frame by using st_as_sf() of sf packages\n\nsgpools_sf &lt;- st_as_sf(sgpools, \n                       coords = c(\"XCOORD\", \"YCOORD\"),\n                       crs= 3414)\n\nThings to learn from the arguments above:\n\nThe coords argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\nThe crs argument required you to provide the coordinates system in epsg format. EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by refering to epsg.io.\n\nFigure below shows the data table of sgpools_sf. Notice that a new column called geometry has been added into the data frame.\n\nYou can display the basic information of the newly created sgpools_sf by using the code chunk below.\n\nlist(sgpools_sf)\n\n[[1]]\nSimple feature collection with 306 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 7844.194 ymin: 26525.7 xmax: 45176.57 ymax: 47987.13\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 306 × 6\n   NAME                         ADDRESS POSTCODE `OUTLET TYPE` `Gp1Gp2 Winnings`\n * &lt;chr&gt;                        &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Marina Bay Sands)  2 Bayf…    18972 Branch                        5\n 2 Livewire (Resorts World Sen… 26 Sen…    98138 Branch                       11\n 3 SportsBuzz (Kranji)          Lotus …   738078 Branch                        0\n 4 SportsBuzz (PoMo)            1 Sele…   188306 Branch                       44\n 5 Prime Serangoon North        Blk 54…   552542 Branch                        0\n 6 Singapore Pools Woodlands C… 1A Woo…   731001 Branch                        3\n 7 Singapore Pools 64 Circuit … Blk 64…   370064 Branch                       17\n 8 Singapore Pools 88 Circuit … Blk 88…   370088 Branch                       16\n 9 Singapore Pools Anchorvale … Blk 30…   540308 Branch                       21\n10 Singapore Pools Ang Mo Kio … Blk 20…   560202 Branch                       25\n# ℹ 296 more rows\n# ℹ 1 more variable: geometry &lt;POINT [m]&gt;\n\n\nThe output shows that sgppols_sf is in point feature class. It’s epsg ID is 3414. The bbox provides information of the extend of the geospatial data."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08B/Hands-on_Ex08B.html#it-all-started-with-an-interactive-point-symbol-map",
    "href": "Hands-on_Ex/Hands-on_Ex08B/Hands-on_Ex08B.html#it-all-started-with-an-interactive-point-symbol-map",
    "title": "Hands-on Exercise 08B",
    "section": "4.1 It all started with an interactive point symbol map",
    "text": "4.1 It all started with an interactive point symbol map\nThe code chunks below are used to create an interactive point symbol map.\n\ntm_shape(sgpools_sf) + \n  tm_bubbles(\n    size = 1,            \n    col = \"red\",         \n    border.col = \"black\",\n    border.lwd = 1       \n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08B/Hands-on_Ex08B.html#lets-make-it-proportional",
    "href": "Hands-on_Ex/Hands-on_Ex08B/Hands-on_Ex08B.html#lets-make-it-proportional",
    "title": "Hands-on Exercise 08B",
    "section": "4.2 Lets make it proportional",
    "text": "4.2 Lets make it proportional\nTo draw a proportional symbol map, we need to assign a numerical variable to the size visual attribute. The code chunks below show that the variable Gp1Gp2Winnings is assigned to size visual attribute.\n\ntm_shape(sgpools_sf) + \n  tm_bubbles(\n    size = \"Gp1Gp2 Winnings\", \n    col = \"red\",              \n    border.col = \"black\",     \n    border.lwd = 1             \n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08B/Hands-on_Ex08B.html#lets-give-it-a-different-colour",
    "href": "Hands-on_Ex/Hands-on_Ex08B/Hands-on_Ex08B.html#lets-give-it-a-different-colour",
    "title": "Hands-on Exercise 08B",
    "section": "4.3 Lets give it a different colour",
    "text": "4.3 Lets give it a different colour\nThe proportional symbol map can be further improved by using the colour visual attribute. In the code chunks below, OUTLET_TYPE variable is used as the colour attribute variable.\n\ntm_shape(sgpools_sf) + \n  tm_bubbles(\n    size = \"Gp1Gp2 Winnings\",  \n    col = \"OUTLET TYPE\",       \n    border.col = \"black\",      \n    border.lwd = 1             \n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08B/Hands-on_Ex08B.html#i-have-a-twin-brothers",
    "href": "Hands-on_Ex/Hands-on_Ex08B/Hands-on_Ex08B.html#i-have-a-twin-brothers",
    "title": "Hands-on Exercise 08B",
    "section": "4.4 I have a twin brothers :)",
    "text": "4.4 I have a twin brothers :)\nAn impressive and little-know feature of tmap’s view mode is that it also works with faceted plots. The argument sync in tm_facets() can be used in this case to produce multiple maps with synchronised zoom and pan settings.\n\ntm_shape(sgpools_sf) + \n  tm_bubbles(\n    size = \"Gp1Gp2 Winnings\",  \n    col = \"OUTLET TYPE\",       \n    border.col = \"black\",      \n    border.lwd = 1             \n  ) +\n  tm_facets(\n    by = \"OUTLET TYPE\",\n    nrow = 1,\n    sync = TRUE\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nBefore you end the session, it is wiser to switch tmap’s Viewer back to plot mode by using the code chunk below.\n\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08B/Hands-on_Ex08B.html#all-about-tmap-package",
    "href": "Hands-on_Ex/Hands-on_Ex08B/Hands-on_Ex08B.html#all-about-tmap-package",
    "title": "Hands-on Exercise 08B",
    "section": "5.1 All about tmap package",
    "text": "5.1 All about tmap package\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08B/Hands-on_Ex08B.html#geospatial-data-wrangling-1",
    "href": "Hands-on_Ex/Hands-on_Ex08B/Hands-on_Ex08B.html#geospatial-data-wrangling-1",
    "title": "Hands-on Exercise 08B",
    "section": "5.2 Geospatial data wrangling",
    "text": "5.2 Geospatial data wrangling\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08B/Hands-on_Ex08B.html#data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex08B/Hands-on_Ex08B.html#data-wrangling",
    "title": "Hands-on Exercise 08B",
    "section": "5.3 Data wrangling",
    "text": "5.3 Data wrangling\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08B/Hands-on_Ex08B.html#i-have-a-twin",
    "href": "Hands-on_Ex/Hands-on_Ex08B/Hands-on_Ex08B.html#i-have-a-twin",
    "title": "Hands-on Exercise 08B",
    "section": "4.4 I have a twin :)",
    "text": "4.4 I have a twin :)\nAn impressive and little-know feature of tmap’s view mode is that it also works with faceted plots. The argument sync in tm_facets() can be used in this case to produce multiple maps with synchronised zoom and pan settings.\n\ntm_shape(sgpools_sf) + \n  tm_bubbles(\n    size = \"Gp1Gp2 Winnings\",  \n    col = \"OUTLET TYPE\",       \n    border.col = \"black\",      \n    border.lwd = 1             \n  ) +\n  tm_facets(\n    by = \"OUTLET TYPE\",\n    nrow = 1,\n    sync = TRUE\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nBefore you end the session, it is wiser to switch tmap’s Viewer back to plot mode by using the code chunk below.\n\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08C/Hands-on_Ex08C.html",
    "href": "Hands-on_Ex/Hands-on_Ex08C/Hands-on_Ex08C.html",
    "title": "Hands-on Exercise 08C",
    "section": "",
    "text": "In this in-class exercise, you will gain hands-on experience on using appropriate R methods to plot analytical maps.\n\n\n\nBy the end of this in-class exercise, you will be able to use appropriate functions of tmap and tidyverse to perform the following tasks:\n\nImporting geospatial data in rds format into R environment.\nCreating cartographic quality choropleth maps by using appropriate tmap functions.\nCreating rate map\nCreating percentile map\nCreating boxmap\n\n\n\n\n\n\n\nBefore we get started, we need to ensure that tmap package of R and other related R packages have been installed and loaded into R.\n\npacman::p_load(tmap, tidyverse, sf)\n\n\n\n\nFor the purpose of this hands-on exercise, a prepared data set called NGA_wp.rds will be used. The data set is a polygon feature data.frame providing information on water point of Nigeria at the LGA level.\n\nNGA_wp &lt;- read_rds(\"data/rds/NGA_wp.rds\")\n\n\n\n\n\n\n\nPlot a choropleth map showing the distribution of non-function water point by LGA\n\n\nShow the code\n\n\np1 &lt;- tm_shape(NGA_wp) +\n  tm_polygons(\n    col = \"wp_functional\",\n    style = \"equal\",\n    n = 10,\n    palette = \"Blues\",\n    title = \"Functional Water Points\"\n  ) +\n  tm_borders(lwd = 0.1) +\n  tm_layout(\n    title = \"Distribution of functional water points by LGAs\",\n    legend.position = c(\"right\", \"bottom\")\n  )\n\n\n\n\nShow the code\n\n\np2 &lt;- tm_shape(NGA_wp) + \n  tm_polygons(\n    col = \"total_wp\", \n    style = \"equal\", \n    n = 10, \n    palette = \"Blues\", \n    title = \"Total Water Points\"\n  ) + \n  tm_borders(lwd = 0.1) + \n  tm_layout(\n    title = \"Distribution of total water points by LGAs\",\n    legend.position = c(\"right\", \"bottom\")\n  )\n\n\n\ntmap_arrange(p2, p1, nrow = 1)\n\n\n\n\n\n\n\n\n\n\n\n\nIn much of our readings we have now seen the importance to map rates rather than counts of things, and that is for the simple reason that water points are not equally distributed in space. That means that if we do not account for how many water points are somewhere, we end up mapping total water point size rather than our topic of interest.\n\n\nWe will tabulate the proportion of functional water points and the proportion of non-functional water points in each LGA. In the following code chunk, mutate() from dplyr package is used to derive two fields, namely pct_functional and pct_nonfunctional.\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  mutate(pct_functional = wp_functional/total_wp) %&gt;%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\n\n\nPlot a choropleth map showing the distribution of percentage functional water point by LGA\n\ntm_shape(NGA_wp) +\n  tm_polygons(\n    col = \"pct_functional\",\n    style = \"equal\",\n    n = 10,\n    palette = \"Blues\",\n    title = \"pct_functional\"\n  ) +\n  tm_borders(lwd = 0.1) +\n  tm_layout(\n    main.title = \"Rate map of functional water point by LGAs\",  # Places title above map box\n    main.title.position = \"center\",  \n    main.title.size = 1.2,           \n    legend.position = c(\"right\", \"bottom\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nExtreme value maps are variations of common choropleth maps where the classification is designed to highlight extreme values at the lower and upper end of the scale, with the goal of identifying outliers. These maps were developed in the spirit of spatializing EDA, i.e., adding spatial features to commonly used approaches in non-spatial EDA (Anselin 1994).\n\n\nThe percentile map is a special type of quantile map with six specific categories: 0-1%,1-10%, 10-50%,50-90%,90-99%, and 99-100%. The corresponding breakpoints can be derived by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note that the begin and endpoint need to be included.\n\n\nStep 1: Exclude records with NA by using the code chunk below.\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  drop_na()\n\nStep 2: Creating customised classification and extracting values\n\npercent &lt;- c(0,.01,.1,.5,.9,.99,1)\nvar &lt;- NGA_wp[\"pct_functional\"] %&gt;%\n  st_set_geometry(NULL)\nquantile(var[,1], percent)\n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen variables are extracted from an sf data.frame, the geometry is extracted as well. For mapping and spatial manipulation, this is the expected behavior, but many base R functions cannot deal with the geometry. Specifically, the quantile() gives an error. As a result st_set_geomtry(NULL) is used to drop geomtry field.\n\n\n\n\n\nWriting a function has three big advantages over using copy-and-paste:\n\nYou can give a function an evocative name that makes your code easier to understand.\nAs requirements change, you only need to update code in one place, instead of many.\nYou eliminate the chance of making incidental mistakes when you copy and paste (i.e. updating a variable name in one place, but not in another).\n\nSource: Chapter 19: Functions of R for Data Science.\n\n\n\nFirstly, we will write an R function as shown below to extract a variable (i.e. wp_nonfunctional) as a vector out of an sf data.frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% \n    st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\n\nNext, we will write a percentile mapping function by using the code chunk below.\n\npercentmap &lt;- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent &lt;- c(0,.01,.1,.5,.9,.99,1)\n  var &lt;- get.var(vnam, df)\n  bperc &lt;- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_polygons(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"&lt; 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"&gt; 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\n\n\nTo run the function, type the code chunk as shown below.\n\npercentmap(\"total_wp\", NGA_wp)\n\n\n\n\n\n\n\n\nNote that this is just a bare bones implementation. Additional arguments such as the title, legend positioning just to name a few of them, could be passed to customise various features of the map.\n\n\n\n\nIn essence, a box map is an augmented quartile map, with an additional lower and upper category. When there are lower outliers, then the starting point for the breaks is the minimum value, and the second break is the lower fence. In contrast, when there are no lower outliers, then the starting point for the breaks will be the lower fence, and the second break is the minimum value (there will be no observations that fall in the interval between the lower fence and the minimum value).\n\nggplot(data = NGA_wp,\n       aes(x = \"\",\n           y = wp_nonfunctional)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nDisplaying summary statistics on a choropleth map by using the basic principles of boxplot.\nTo create a box map, a custom breaks specification will be used. However, there is a complication. The break points for the box map vary depending on whether lower or upper outliers are present.\n\n\n\nThe code chunk below is an R function that creating break points for a box map.\n\narguments:\n\nv: vector with observations\nmult: multiplier for IQR (default 1.5)\n\nreturns:\n\nbb: vector with 7 break points compute quartile and fences\n\n\n\nboxbreaks &lt;- function(v,mult=1.5) {\n  qv &lt;- unname(quantile(v))\n  iqr &lt;- qv[4] - qv[2]\n  upfence &lt;- qv[4] + mult * iqr\n  lofence &lt;- qv[2] - mult * iqr\n  # initialize break points vector\n  bb &lt;- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence &lt; qv[1]) {  # no lower outliers\n    bb[1] &lt;- lofence\n    bb[2] &lt;- floor(qv[1])\n  } else {\n    bb[2] &lt;- lofence\n    bb[1] &lt;- qv[1]\n  }\n  if (upfence &gt; qv[5]) { # no upper outliers\n    bb[7] &lt;- upfence\n    bb[6] &lt;- ceiling(qv[5])\n  } else {\n    bb[6] &lt;- upfence\n    bb[7] &lt;- qv[5]\n  }\n  bb[3:5] &lt;- qv[2:4]\n  return(bb)\n}\n\n\n\n\nThe code chunk below is an R function to extract a variable as a vector out of an sf data frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\n\nLet’s test the newly created function\n\nvar &lt;- get.var(\"wp_nonfunctional\", NGA_wp) \nboxbreaks(var)\n\n[1] -56.5   0.0  14.0  34.0  61.0 131.5 278.0\n\n\n\n\n\nThe code chunk below is an R function to create a box map. - arguments: - vnam: variable name (as character, in quotes) - df: simple features polygon layer - legtitle: legend title - mtitle: map title - mult: multiplier for IQR - returns: - a tmap-element (plots a map)\n\nboxmap &lt;- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var &lt;- get.var(vnam,df)\n  bb &lt;- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"&lt; 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"&gt; 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}\n\n\ntmap_mode(\"plot\")\nboxmap(\"wp_nonfunctional\", NGA_wp)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08C/Hands-on_Ex08C.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex08C/Hands-on_Ex08C.html#overview",
    "title": "Hands-on Exercise 08C",
    "section": "",
    "text": "In this in-class exercise, you will gain hands-on experience on using appropriate R methods to plot analytical maps.\n\n\n\nBy the end of this in-class exercise, you will be able to use appropriate functions of tmap and tidyverse to perform the following tasks:\n\nImporting geospatial data in rds format into R environment.\nCreating cartographic quality choropleth maps by using appropriate tmap functions.\nCreating rate map\nCreating percentile map\nCreating boxmap"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08C/Hands-on_Ex08C.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex08C/Hands-on_Ex08C.html#getting-started",
    "title": "Hands-on Exercise 08C",
    "section": "",
    "text": "Before we get started, we need to ensure that tmap package of R and other related R packages have been installed and loaded into R.\n\npacman::p_load(tmap, tidyverse, sf)\n\n\n\n\nFor the purpose of this hands-on exercise, a prepared data set called NGA_wp.rds will be used. The data set is a polygon feature data.frame providing information on water point of Nigeria at the LGA level.\n\nNGA_wp &lt;- read_rds(\"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08C/Hands-on_Ex08C.html#basic-choropleth-mapping",
    "href": "Hands-on_Ex/Hands-on_Ex08C/Hands-on_Ex08C.html#basic-choropleth-mapping",
    "title": "Hands-on Exercise 08C",
    "section": "",
    "text": "Plot a choropleth map showing the distribution of non-function water point by LGA\n\n\nShow the code\n\n\np1 &lt;- tm_shape(NGA_wp) +\n  tm_polygons(\n    col = \"wp_functional\",\n    style = \"equal\",\n    n = 10,\n    palette = \"Blues\",\n    title = \"Functional Water Points\"\n  ) +\n  tm_borders(lwd = 0.1) +\n  tm_layout(\n    title = \"Distribution of functional water points by LGAs\",\n    legend.position = c(\"right\", \"bottom\")\n  )\n\n\n\n\nShow the code\n\n\np2 &lt;- tm_shape(NGA_wp) + \n  tm_polygons(\n    col = \"total_wp\", \n    style = \"equal\", \n    n = 10, \n    palette = \"Blues\", \n    title = \"Total Water Points\"\n  ) + \n  tm_borders(lwd = 0.1) + \n  tm_layout(\n    title = \"Distribution of total water points by LGAs\",\n    legend.position = c(\"right\", \"bottom\")\n  )\n\n\n\ntmap_arrange(p2, p1, nrow = 1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08C/Hands-on_Ex08C.html#choropleth-map-for-rates",
    "href": "Hands-on_Ex/Hands-on_Ex08C/Hands-on_Ex08C.html#choropleth-map-for-rates",
    "title": "Hands-on Exercise 08C",
    "section": "",
    "text": "In much of our readings we have now seen the importance to map rates rather than counts of things, and that is for the simple reason that water points are not equally distributed in space. That means that if we do not account for how many water points are somewhere, we end up mapping total water point size rather than our topic of interest.\n\n\nWe will tabulate the proportion of functional water points and the proportion of non-functional water points in each LGA. In the following code chunk, mutate() from dplyr package is used to derive two fields, namely pct_functional and pct_nonfunctional.\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  mutate(pct_functional = wp_functional/total_wp) %&gt;%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\n\n\nPlot a choropleth map showing the distribution of percentage functional water point by LGA\n\ntm_shape(NGA_wp) +\n  tm_polygons(\n    col = \"pct_functional\",\n    style = \"equal\",\n    n = 10,\n    palette = \"Blues\",\n    title = \"pct_functional\"\n  ) +\n  tm_borders(lwd = 0.1) +\n  tm_layout(\n    main.title = \"Rate map of functional water point by LGAs\",  # Places title above map box\n    main.title.position = \"center\",  \n    main.title.size = 1.2,           \n    legend.position = c(\"right\", \"bottom\")\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08C/data/geospatial/MPSZ-2019.html",
    "href": "Hands-on_Ex/Hands-on_Ex08C/data/geospatial/MPSZ-2019.html",
    "title": "ISSS608-VAA",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08C/Hands-on_Ex08C.html#extreme-value-maps",
    "href": "Hands-on_Ex/Hands-on_Ex08C/Hands-on_Ex08C.html#extreme-value-maps",
    "title": "Hands-on Exercise 08C",
    "section": "",
    "text": "Extreme value maps are variations of common choropleth maps where the classification is designed to highlight extreme values at the lower and upper end of the scale, with the goal of identifying outliers. These maps were developed in the spirit of spatializing EDA, i.e., adding spatial features to commonly used approaches in non-spatial EDA (Anselin 1994).\n\n\nThe percentile map is a special type of quantile map with six specific categories: 0-1%,1-10%, 10-50%,50-90%,90-99%, and 99-100%. The corresponding breakpoints can be derived by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note that the begin and endpoint need to be included.\n\n\nStep 1: Exclude records with NA by using the code chunk below.\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  drop_na()\n\nStep 2: Creating customised classification and extracting values\n\npercent &lt;- c(0,.01,.1,.5,.9,.99,1)\nvar &lt;- NGA_wp[\"pct_functional\"] %&gt;%\n  st_set_geometry(NULL)\nquantile(var[,1], percent)\n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen variables are extracted from an sf data.frame, the geometry is extracted as well. For mapping and spatial manipulation, this is the expected behavior, but many base R functions cannot deal with the geometry. Specifically, the quantile() gives an error. As a result st_set_geomtry(NULL) is used to drop geomtry field.\n\n\n\n\n\nWriting a function has three big advantages over using copy-and-paste:\n\nYou can give a function an evocative name that makes your code easier to understand.\nAs requirements change, you only need to update code in one place, instead of many.\nYou eliminate the chance of making incidental mistakes when you copy and paste (i.e. updating a variable name in one place, but not in another).\n\nSource: Chapter 19: Functions of R for Data Science.\n\n\n\nFirstly, we will write an R function as shown below to extract a variable (i.e. wp_nonfunctional) as a vector out of an sf data.frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% \n    st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\n\nNext, we will write a percentile mapping function by using the code chunk below.\n\npercentmap &lt;- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent &lt;- c(0,.01,.1,.5,.9,.99,1)\n  var &lt;- get.var(vnam, df)\n  bperc &lt;- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_polygons(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"&lt; 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"&gt; 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\n\n\nTo run the function, type the code chunk as shown below.\n\npercentmap(\"total_wp\", NGA_wp)\n\n\n\n\n\n\n\n\nNote that this is just a bare bones implementation. Additional arguments such as the title, legend positioning just to name a few of them, could be passed to customise various features of the map.\n\n\n\n\nIn essence, a box map is an augmented quartile map, with an additional lower and upper category. When there are lower outliers, then the starting point for the breaks is the minimum value, and the second break is the lower fence. In contrast, when there are no lower outliers, then the starting point for the breaks will be the lower fence, and the second break is the minimum value (there will be no observations that fall in the interval between the lower fence and the minimum value).\n\nggplot(data = NGA_wp,\n       aes(x = \"\",\n           y = wp_nonfunctional)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nDisplaying summary statistics on a choropleth map by using the basic principles of boxplot.\nTo create a box map, a custom breaks specification will be used. However, there is a complication. The break points for the box map vary depending on whether lower or upper outliers are present.\n\n\n\nThe code chunk below is an R function that creating break points for a box map.\n\narguments:\n\nv: vector with observations\nmult: multiplier for IQR (default 1.5)\n\nreturns:\n\nbb: vector with 7 break points compute quartile and fences\n\n\n\nboxbreaks &lt;- function(v,mult=1.5) {\n  qv &lt;- unname(quantile(v))\n  iqr &lt;- qv[4] - qv[2]\n  upfence &lt;- qv[4] + mult * iqr\n  lofence &lt;- qv[2] - mult * iqr\n  # initialize break points vector\n  bb &lt;- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence &lt; qv[1]) {  # no lower outliers\n    bb[1] &lt;- lofence\n    bb[2] &lt;- floor(qv[1])\n  } else {\n    bb[2] &lt;- lofence\n    bb[1] &lt;- qv[1]\n  }\n  if (upfence &gt; qv[5]) { # no upper outliers\n    bb[7] &lt;- upfence\n    bb[6] &lt;- ceiling(qv[5])\n  } else {\n    bb[6] &lt;- upfence\n    bb[7] &lt;- qv[5]\n  }\n  bb[3:5] &lt;- qv[2:4]\n  return(bb)\n}\n\n\n\n\nThe code chunk below is an R function to extract a variable as a vector out of an sf data frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\n\nLet’s test the newly created function\n\nvar &lt;- get.var(\"wp_nonfunctional\", NGA_wp) \nboxbreaks(var)\n\n[1] -56.5   0.0  14.0  34.0  61.0 131.5 278.0\n\n\n\n\n\nThe code chunk below is an R function to create a box map. - arguments: - vnam: variable name (as character, in quotes) - df: simple features polygon layer - legtitle: legend title - mtitle: map title - mult: multiplier for IQR - returns: - a tmap-element (plots a map)\n\nboxmap &lt;- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var &lt;- get.var(vnam,df)\n  bb &lt;- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"&lt; 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"&gt; 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}\n\n\ntmap_mode(\"plot\")\nboxmap(\"wp_nonfunctional\", NGA_wp)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to ISSS608 Visual Analytics and Applications.\nIn this website, you will find my coursework prepared for this course."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html",
    "href": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "“Prototyping first may help keep you from investing far too much time for marginal gains.”\n— The Art of UNIX Programming (Raymond, 2003)\n\n\n\n\nThis prototype continues from Take-home Exercise 2, based on Mini-Challenge 2 (MC2) from the VAST Challenge 2025, which investigates claims of bias within the fictional government committee COOTEFOO, amid tensions between economic development and cultural preservation in Oceanus.\nOceanus faces competing visions from:\n\nFishing is Living and Heritage (FILAH) – preserving traditional fishing zones.\nTourism Raises OceanUs Together (TROUT) – promoting tourism-led growth.\n\nBoth groups have accused COOTEFOO of favoring the other. To investigate, we analyze:\n\n🟦 FILAH.json – records submitted by the pro-fishing group\n🟨 TROUT.json – records from the pro-tourism group\n🟩 journalist.json – independent reports to provide a neutral perspective\n🗺️ oceanus_map.geojson – official zoning map of Oceanus\n🛣️ road_map.json – road network infrastructure\n\n📝 All datasets were provided through the official VAST Challenge MC2 instruction page.\nWe aim to build a modular Shiny app prototype that allows stakeholders to:\n1. View which COOTEFOO members are most frequently discussed\n2. Analyze the topics and sentiment associated with each\n3. Compare bias across datasets\n4. Explore geographic movement patterns\nThis report outlines our Quarto-based storyboard prototype, which will be integrated into a future Shiny app.\n\n\n\nThis section lists all the required packages for building the prototype. They include packages for data wrangling (tidyverse), network and geospatial analysis (igraph, sf), text analytics (tidytext), interactive plots (DT, shiny, leaflet, visNetwork), and utility support for managing JSON data.\n\n\nShow the code\n\n\npacman::p_load(\n  tidyverse, jsonlite, sf, lubridate, DT, shiny,\n  visNetwork, igraph, stringr, tidygraph, tmap, tidytext\n)\n\n\n\n\n\nThe following datasets are used in this prototype. These were provided as part of the VAST Challenge 2025 Mini-Challenge 2.\n\nFILAH.json: Records provided by the fishing advocacy group\nTROUT.json: Records from the tourism advocacy group\njournalist.json: Independent data contributed by journalists\noceanus_map.geojson: Oceanus spatial zoning map\nroad_map.json: Road network of Oceanus\n\n\n\n📥 Load JSON and GeoJSON Files\n\n\n# Load JSON-based knowledge graphs\nfilah &lt;- jsonlite::fromJSON(\"data/FILAH.json\")\ntrout &lt;- jsonlite::fromJSON(\"data/TROUT.json\")\njournalist &lt;- jsonlite::fromJSON(\"data/journalist.json\")\n\n# Load spatial files\noceanus_map &lt;- sf::st_read(\"data/oceanus_map.geojson\")\n\nReading layer `oceanus_map' from data source \n  `D:\\shartiono\\ISSS608-VAA\\Take-home_Ex\\Take-home_Ex_3\\data\\oceanus_map.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 29 features and 6 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: -167.0654 ymin: 38.07452 xmax: -163.2723 ymax: 40.67775\nGeodetic CRS:  WGS 84\n\nroad_map &lt;- jsonlite::fromJSON(\"data/road_map.json\", simplifyDataFrame = TRUE)\n\n\n\n\n\nTo support our storyboard modules, we begin with cleaning and exploring key aspects of the datasets. This includes understanding the structure, extracting relevant geospatial information, identifying role and plan type distributions, and performing basic text analysis.\n\n\nWe start by extracting geotagged nodes from each dataset (FILAH, TROUT, journalist). Only entries with valid latitude and longitude are included for mapping.\n\n# Define helper function to extract geotagged place nodes\nextract_places &lt;- function(data, source_name) {\n  data$nodes %&gt;%\n    filter(!is.na(lat) & !is.na(lon)) %&gt;%\n    mutate(source = source_name)\n}\n\n# Extract and combine geotagged nodes across all datasets\nplaces_filah &lt;- extract_places(filah, \"FILAH\")\nplaces_trout &lt;- extract_places(trout, \"TROUT\")\nplaces_journalist &lt;- extract_places(journalist, \"journalist\")\n\nall_places &lt;- bind_rows(places_filah, places_trout, places_journalist)\n\n# Extract all nodes (not just places with lat/lon)\nnodes_filah &lt;- filah$nodes %&gt;% mutate(source = \"FILAH\")\nnodes_trout &lt;- trout$nodes %&gt;% mutate(source = \"TROUT\")\nnodes_journalist &lt;- journalist$nodes %&gt;% mutate(source = \"journalist\")\n\n# Combine all nodes into a single dataframe\nall_nodes &lt;- bind_rows(nodes_filah, nodes_trout, nodes_journalist)\n\nWe briefly inspect the spatial distribution to ensure values fall within expected boundaries.\n\nsummary(all_places$lat)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -166.0  -165.7  -165.6  -165.2  -164.5  -164.3 \n\nsummary(all_places$lon)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  38.99   39.26   39.42   39.38   39.54   39.67 \n\n\nWe also check whether zone values align with the official zoning names provided in the Oceanus map.\n\nunique(all_nodes$zone)\n\n[1] NA            \"industrial\"  \"tourism\"     \"residential\" \"government\" \n[6] \"commercial\"  \"connector\"  \n\nunique(oceanus_map$Name)\n\n [1] \"Suna Island\"         \"Thalassa Retreat\"    \"Makara Shoal\"       \n [4] \"Silent Sanctuary\"    \"Cod Table\"           \"Ghoti Preserve\"     \n [7] \"Wrasse Beds\"         \"Nemo Reef\"           \"Don Limpet Preserve\"\n[10] \"Tuna Shelf\"          \"Haacklee\"            \"Port Grove\"         \n[13] \"Lomark\"              \"Himark\"              \"Paackland\"          \n[16] \"Centralia\"           \"South Paackland\"     \"Exit West\"          \n[19] \"Nav 3\"               \"Nav D\"               \"Nav B\"              \n[22] \"Nav A\"               \"Nav C\"               \"Nav 2\"              \n[25] \"Nav 1\"               \"Exit East\"           \"Exit South\"         \n[28] \"Exit North\"          \"Nav E\"              \n\n\n\n\n\nTo better understand spatial focus across the datasets, we first count how often each zone is mentioned:\n\nall_nodes %&gt;%\n  filter(!is.na(zone)) %&gt;%\n  count(zone, name = \"place_count\") %&gt;%\n  ggplot(aes(x = fct_reorder(zone, place_count), y = place_count, fill = zone)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  labs(title = \"Place Mentions by Zone Type\",\n       x = \"Zone Type\", y = \"Count\")\n\n\n\n\n\n\n\n\n\n\n\nWe then compare how often each dataset (FILAH, TROUT, journalist) mentions different zones.\n\n# Stacked bar chart: mentions by zone type and dataset source\nall_nodes %&gt;%\n  filter(!is.na(zone)) %&gt;%\n  count(source, zone) %&gt;%\n  ggplot(aes(x = fct_reorder(zone, n, sum), y = n, fill = source)) +\n  geom_col(position = \"stack\") +\n  coord_flip() +\n  labs(title = \"Zone Mentions by Source\",\n       x = \"Zone Type\", y = \"Mentions\", fill = \"Source\")\n\n\n\n\n\n\n\n\n\n\n\nWe examine the distribution of plan types proposed by each source.\n\nall_nodes %&gt;%\n  filter(!is.na(plan_type)) %&gt;%\n  count(source, plan_type) %&gt;%\n  ggplot(aes(x = fct_reorder(plan_type, n, sum), y = n, fill = source)) +\n  geom_col(position = \"stack\") +\n  coord_flip() +\n  labs(title = \"Plan Type Mentions by Source\",\n       x = \"Plan Type\", y = \"Mentions\")\n\n\n\n\n\n\n\n\n\n\n\nWe prepare and tokenize text fields to uncover common language used across sources.\n\nlibrary(tidytext)\nlibrary(stringr)\n\n# Clean plan_type formatting for consistency\nall_nodes_clean &lt;- all_nodes %&gt;%\n  mutate(plan_type = str_to_title(plan_type))\n\n# Combine text fields into one string\nall_text &lt;- all_nodes_clean %&gt;%\n  mutate(text = paste(label, short_title, long_title, short_topic, long_topic, sep = \" \")) %&gt;%\n  select(source, text) %&gt;%\n  filter(!is.na(text))\n\n# Tokenize and remove common stop words and numbers\ntokens &lt;- all_text %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  anti_join(stop_words, by = \"word\") %&gt;%\n  filter(!str_detect(word, \"^\\\\d+$\"))\n\n# Visualize most frequent words by source\ntokens %&gt;%\n  filter(word != \"na\") %&gt;%\n  count(source, word, sort = TRUE) %&gt;%\n  group_by(source) %&gt;%\n  slice_max(n, n = 10) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(x = reorder_within(word, n, source), y = n, fill = source)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~source, scales = \"free_y\") +\n  scale_x_reordered() +\n  coord_flip() +\n  labs(title = \"Top Words in Meeting Records by Source\",\n       x = \"Word\", y = \"Frequency\")\n\n\n\n\n\n\n\n\n\n\n\n\nTo address the investigative goals in our project proposal, we present five interactive modules in a Shiny dashboard. These modules follow Shneiderman’s mantra: “overview first, zoom and filter, then details on demand” and are structured using a consistent member-centric interaction model.\nEach module is built with Shiny-ready functions and coordinated using reactiveValues() to support cross-filtering, seamless user experience, and visual storytelling.\n\n\n\nGoal: Help users understand the structure, scale, and characteristics of the datasets before deeper analysis\nUI Components:\n\n-    Summary tables (e.g., total nodes, node types, source counts) using `DT`\n\n-    Bar chart: Frequency of zone types mentioned\n\n-    Violin or bar plots: Distribution of plan types by source\n\n-    Top word frequencies across datasets (via `tidytext`)\nInsight: Offers transparency and builds user confidence by surfacing base-level descriptive insights.\n\n\n\n\n\nGoal: Reveal structural relationships between COOTEFOO members, their discussion topics, and proposed plans\nUI Components:\n\n-   Interactive `visNetwork` graph\n\n-    Member selector (ego filter)\n\n-    Dataset toggle: FILAH, TROUT, journalist\n\n-    Tooltip: Node type, role, or plan type\n\nInsight: Visualizes influence patterns, overlapping interest areas, and collaborative networks across stakeholders.\n\n\n\n\n\n\nGoal: Understand where members are engaging geographically through travel and planning actions\nUI Components:\n\n-    Interactive `leaflet` map overlaid with Oceanus zones\n\n-   Year/meeting slider\n\nMarker toggles for travel nodes and planned site visits\nInsight: Maps the physical footprint of member activity, highlighting areas of concentrated or neglected engagement.\n\n\n\n\n\n\nGoal: Uncover sentiment trends across members, grouped by topic or industry focus\nUI Components:\n\n-    Violin/bar plots faceted by source\n\n-    Filters: Member, topic, plan type\n\n-    Tooltip: Sentiment score, role, context\nInsight: Highlights potential pro/anti-industry bias and comparative sentiment levels between advocacy groups.\n\n\n\n\n\nGoal: Track member participation and influence over time in a chronological view\nUI Components:\n\n-    Lollipop or line chart by meeting index or date\n\n-    Color-coded by role (e.g., Chair, Member)\n\n-    Filters: Topic, source, plan type\n\n-    Hover: Meeting label, topic discussed, plan proposed\n\nInsight: Shows evolution in member engagement, surfacing key turning points or role transitions over the course of discussions.\n\n\n\n\nAll modules are developed using modular R functions inside a Quarto document and are structured for direct transition to Shiny. Core design considerations include:\n\nFramework: tabsetPanel() with each module in its own tabPanel()\nShared State: Member/topic/source filters coordinated via reactiveValues()\nKey Packages:\n\n-    `shiny`, `shinydashboard`: layout & interaction\n\n-    `ggplot2`, `plotly`: dynamic plot\n\n-    `leaflet`, `sf`, `tmap`: spatial mapping\n\n-    `visNetwork`: network graphs\n\n-    `DT`: tabular summaries\n\n-    `tidytext`: text tokenization and frequency analysis"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#the-task",
    "href": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#the-task",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "In this take-home exercise, we will select one of the module of the proposed Shiny application and complete the following tasks:\n\nTo evaluate and determine the necessary R packages needed for your Shiny application are supported in R CRAN,\nTo prepare and test the specific R codes can be run and returned the correct output as expected,\nTo determine the parameters and outputs that will be exposed on the Shiny applications, and\nTo select the appropriate Shiny UI components for exposing the parameters determine above."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#project-overview",
    "href": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#project-overview",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "This prototype continues from Take-home Exercise 2, based on Mini-Challenge 2 (MC2) from the VAST Challenge 2025, which investigates claims of bias within the fictional government committee COOTEFOO, amid tensions between economic development and cultural preservation in Oceanus.\nOceanus faces competing visions from:\n\nFishing is Living and Heritage (FILAH) – preserving traditional fishing zones.\nTourism Raises OceanUs Together (TROUT) – promoting tourism-led growth.\n\nBoth groups have accused COOTEFOO of favoring the other. To investigate, we analyze:\n\n🟦 FILAH.json – records submitted by the pro-fishing group\n🟨 TROUT.json – records from the pro-tourism group\n🟩 journalist.json – independent reports to provide a neutral perspective\n🗺️ oceanus_map.geojson – official zoning map of Oceanus\n🛣️ road_map.json – road network infrastructure\n\n📝 All datasets were provided through the official VAST Challenge MC2 instruction page.\nWe aim to build a modular Shiny app prototype that allows stakeholders to:\n1. View which COOTEFOO members are most frequently discussed\n2. Analyze the topics and sentiment associated with each\n3. Compare bias across datasets\n4. Explore geographic movement patterns\nThis report outlines our Quarto-based storyboard prototype, which will be integrated into a future Shiny app."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#required-r-packages",
    "href": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#required-r-packages",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "This section lists all the required packages for building the prototype. They include packages for data wrangling (tidyverse), network and geospatial analysis (igraph, sf), text analytics (tidytext), interactive plots (DT, shiny, leaflet, visNetwork), and utility support for managing JSON data.\n\n\nShow the code\n\n\npacman::p_load(\n  tidyverse, jsonlite, sf, lubridate, DT, shiny,\n  visNetwork, igraph, stringr, tidygraph, tmap, tidytext\n)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#data-import",
    "href": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#data-import",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "The following datasets are used in this prototype. These were provided as part of the VAST Challenge 2025 Mini-Challenge 2.\n\nFILAH.json: Records provided by the fishing advocacy group\nTROUT.json: Records from the tourism advocacy group\njournalist.json: Independent data contributed by journalists\noceanus_map.geojson: Oceanus spatial zoning map\nroad_map.json: Road network of Oceanus\n\n\n\n📥 Load JSON and GeoJSON Files\n\n\n# Load JSON-based knowledge graphs\nfilah &lt;- jsonlite::fromJSON(\"data/FILAH.json\")\ntrout &lt;- jsonlite::fromJSON(\"data/TROUT.json\")\njournalist &lt;- jsonlite::fromJSON(\"data/journalist.json\")\n\n# Load spatial files\noceanus_map &lt;- sf::st_read(\"data/oceanus_map.geojson\")\n\nReading layer `oceanus_map' from data source \n  `D:\\shartiono\\ISSS608-VAA\\Take-home_Ex\\Take-home_Ex_3\\data\\oceanus_map.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 29 features and 6 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: -167.0654 ymin: 38.07452 xmax: -163.2723 ymax: 40.67775\nGeodetic CRS:  WGS 84\n\nroad_map &lt;- jsonlite::fromJSON(\"data/road_map.json\", simplifyDataFrame = TRUE)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#data-wrangling",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "Each dataset (FILAH, TROUT, JOURNALIST) is structured as a knowledge graph with two core components:\n\nNodes: Entities such as people, locations, events, or topics.\nLinks: Relationships between nodes (e.g., participation, planning, visits).\n\nWe extract and tidy these components to support downstream analysis.\n\n\n\nWe begin by converting the nested JSON structures into flat tibble data frames.\n\n\n📦 Expand to view code\n\n\n# Extract node and edge tibbles from each dataset\nfilah_nodes &lt;- as_tibble(filah$nodes)\nfilah_edges &lt;- as_tibble(filah$links)\n\ntrout_nodes &lt;- as_tibble(trout$nodes)\ntrout_edges &lt;- as_tibble(trout$links)\n\njournalist_nodes &lt;- as_tibble(journalist$nodes)\njournalist_edges &lt;- as_tibble(journalist$links)\n\n\n\n\n\nWe merge node and edge data across all sources and standardize column naming for clarity.\n\n# Combine node data and tag each with source\nall_nodes &lt;- bind_rows(\n  filah_nodes %&gt;% mutate(source = \"FILAH\"),\n  trout_nodes %&gt;% mutate(source = \"TROUT\"),\n  journalist_nodes %&gt;% mutate(source = \"JOURNALIST\")\n)\n\n# Combine edge data and label dataset of origin\ncombined_edges &lt;- bind_rows(\n  filah_edges %&gt;% mutate(source_dataset = \"FILAH\"),\n  trout_edges %&gt;% mutate(source_dataset = \"TROUT\"),\n  journalist_edges %&gt;% mutate(source_dataset = \"JOURNALIST\")\n) %&gt;%\n  rename(from = source, to = target)\n\n\n\n\nBefore building any network visualizations, we perform a cleanup to ensure consistency across node and edge identifiers. This includes standardizing IDs, and selecting only relevant variables.\n\n# Step 1: Standardize and deduplicate node IDs\nall_nodes &lt;- all_nodes %&gt;%\n  mutate(id = coalesce(id, label, name)) %&gt;%\n  distinct(id, .keep_all = TRUE) %&gt;%\n  select(id, name, label, type, role, source)\n\n# Step 2: Clean edge table\ncombined_edges &lt;- combined_edges %&gt;%\n  filter(!is.na(from), !is.na(to)) %&gt;%\n  distinct(from, to, role, source_dataset, sentiment, industry, reason, .keep_all = TRUE)\n\n# Step 3: (Optional) Filter to COOTEFOO board-related links only\nboard_ids &lt;- all_nodes %&gt;%\n  filter(role %in% c(\"Committee Chair\", \"Vice Chair\", \"Treasurer\", \"Member\")) %&gt;%\n  pull(id)\n\ncombined_edges &lt;- combined_edges %&gt;%\n  filter(from %in% board_ids | to %in% board_ids)\n\n\n\n\nTo track presence and sentiment bias, we extract the list of COOTEFOO board members and check which datasets they appear in.\n\n# Get named COOTEFOO board members\ncootefoo_raw &lt;- all_nodes %&gt;%\n  filter(role %in% c(\"Committee Chair\", \"Vice Chair\", \"Treasurer\", \"Member\")) %&gt;%\n  mutate(name = coalesce(label, name, id)) %&gt;%\n  distinct(name, role, source)\n\n# Pivot wide to show dataset presence\ncootefoo_wide &lt;- cootefoo_raw %&gt;%\n  mutate(present = TRUE) %&gt;%\n  pivot_wider(names_from = source, values_from = present, values_fill = FALSE)\n\n# Ensure all 3 dataset columns exist\nfor (col in c(\"FILAH\", \"TROUT\", \"JOURNALIST\")) {\n  if (!(col %in% names(cootefoo_wide))) {\n    cootefoo_wide[[col]] &lt;- FALSE\n  }\n}\n\n# Display as checkmarks\ncootefoo_wide &lt;- cootefoo_wide %&gt;%\n  mutate(across(c(FILAH, TROUT, JOURNALIST), ~ ifelse(.x, \"✔\", \"\"))) %&gt;%\n  arrange(factor(role, levels = c(\"Committee Chair\", \"Vice Chair\", \"Treasurer\", \"Member\")))\n\n# Preview table\nDT::datatable(cootefoo_wide, rownames = FALSE,\n              options = list(pageLength = 10),\n              caption = \"COOTEFOO Members and Dataset Presence\")\n\n\n\n\n\n\n\n\nAlthough all COOTEFOO members may be structurally present in the dataset (i.e., they appear in the nodes table), not all of them are associated with sentiment-scored interactions. This section highlights which members have actual sentiment data in each dataset.\n\n# Define full COOTEFOO member list from earlier section\ncootefoo_members &lt;- cootefoo_wide$name\n\n# Filter edge records with sentiment + role participant/planner\nsentiment_edges &lt;- combined_edges %&gt;%\n  filter(!is.na(sentiment), role %in% c(\"participant\", \"planner\", \"author\"))\n\n# Extract presence of sentiment for each member and dataset\nsentiment_coverage &lt;- sentiment_edges %&gt;%\n  filter(to %in% cootefoo_members | from %in% cootefoo_members) %&gt;%\n  pivot_longer(cols = c(from, to), names_to = \"direction\", values_to = \"person\") %&gt;%\n  filter(person %in% cootefoo_members) %&gt;%\n  distinct(person, source_dataset) %&gt;%\n  mutate(sentiment_present = TRUE) %&gt;%\n  pivot_wider(\n    names_from = source_dataset,\n    values_from = sentiment_present,\n    values_fill = FALSE\n  ) %&gt;%\n  mutate(across(FILAH:JOURNALIST, ~ ifelse(.x, \"✔\", \"\"))) %&gt;%\n  rename(`Board Member` = person)\n\n# Merge with role data for context\nsentiment_coverage &lt;- left_join(cootefoo_wide %&gt;% select(name, role), sentiment_coverage,\n                                by = c(\"name\" = \"Board Member\"))\n\n# Preview final table\nDT::datatable(sentiment_coverage, rownames = FALSE,\n              options = list(pageLength = 10),\n              caption = \"Sentiment Coverage for COOTEFOO Members by Dataset\")\n\n\n\n\n\n\n\n\n\n\n\nObservation — Member Presence vs Sentiment Coverage\n\n\n\n\nWhile all six COOTEFOO members appear structurally in the dataset (nodes), not all are associated with sentiment-scored interactions (edges). For example:\n\nFILAH omits sentiment for Teddy, Tante, and Ed\nTROUT lacks sentiment for Simone and Carol\nJOURNALIST provides complete sentiment coverage\n\n🧭 This distinction is crucial when analyzing perceived bias vs factual engagement."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#module-2-sentiment-attribution-by-topic-and-member",
    "href": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#module-2-sentiment-attribution-by-topic-and-member",
    "title": "Take-home Exercise 3",
    "section": "6. Module 2 — Sentiment Attribution by Topic and Member",
    "text": "6. Module 2 — Sentiment Attribution by Topic and Member\nWhile all three datasets (FILAH, TROUT, and JOURNALIST) contain sentiment values, a positive or negative score alone is not enough to infer bias — we must understand what topic that sentiment is about.\nFor example: - A +1 score toward a fishing topic may indicate pro-fishing bias - A –1 score toward a tourism project may suggest anti-tourism sentiment - A neutral score may refer to non-industry topics such as housing or education\nTo account for this, we classify sentiment records by industry category using the industry and reason fields. This module provides a breakdown of average sentiment values by COOTEFOO member, by topic category, and by dataset — enabling us to detect framing bias and narrative skew.\n\n\n6.1 Classify Topic Type\n\n# Function to assign industry category\nassign_topic_category &lt;- function(industry, reason) {\n  industry &lt;- tolower(industry)\n  reason   &lt;- tolower(reason)\n  \n  case_when(\n    str_detect(reason, \"housing\") ~ \"Other\",\n    str_detect(industry, \"tourism|tourist|wharf|travel|harbor|marina|port\") |\n      str_detect(reason, \"tourism|tourist|wharf|travel|harbor|marina|port\") ~ \"Tourism\",\n    str_detect(industry, \"fishing|vessel|dock|crane|fish\") |\n      str_detect(reason, \"fishing|vessel|dock|crane|fish\") ~ \"Fishing\",\n    TRUE ~ \"Other\"\n  )\n}\n\n\n\n6.2 Summarize Member Sentiment by Topic and Dataset\n\n# Clean and tag sentiment-related edges\nsentiment_tagged &lt;- combined_edges %&gt;%\n  filter(role == \"participant\", !is.na(sentiment)) %&gt;%\n  mutate(industry_group = assign_topic_category(industry, reason)) %&gt;%\n  filter(to %in% cootefoo_members)\n\n# Calculate mean sentiment by member, topic, and dataset\nsentiment_summary &lt;- sentiment_tagged %&gt;%\n  group_by(member = to, source_dataset, industry_group) %&gt;%\n  summarise(mean_sentiment = mean(sentiment, na.rm = TRUE), .groups = \"drop\")\n\n\n\n6.3 Visualize Sentiment Bias by Topic\n\n# Sentiment bar chart: Member × Topic × Dataset\nggplot(sentiment_summary, aes(x = mean_sentiment, y = fct_rev(member), fill = industry_group)) +\n  geom_col(position = \"dodge\") +\n  facet_wrap(~ source_dataset) +\n  scale_fill_manual(values = c(\"Fishing\" = \"#1f78b4\", \"Tourism\" = \"#FFD700\", \"Other\" = \"gray70\")) +\n  scale_x_continuous(limits = c(-1, 1)) +\n  labs(\n    title = \"Topic-Based Sentiment for COOTEFOO Members Across Datasets\",\n    subtitle = \"Positive scores suggest support; negative scores suggest opposition\",\n    x = \"Mean Sentiment Score (–1 to +1)\",\n    y = \"Board Member\",\n    fill = \"Topic Category\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(strip.text = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation — Topic-Specific Sentiment Patterns\n\n\n\nThis chart reveals important distinctions in how sentiment is framed across datasets:\nFILAH highlights only a few members (e.g., Carol Limpet, Simone Kat), and surprisingly shows positive sentiment toward Tourism, contradicting its pro-fishing stance.\nTROUT includes Teddy Goldstein and Ed Helpsford, both showing pro-fishing or anti-tourism sentiment, but omits others like Simone Kat and Carol Limpet — indicating selective framing.\nJOURNALIST provides the most complete view, with all members represented and sentiment spread across all three topic categories, reinforcing its role as a balanced dataset.\nThese gaps and differences highlight how advocacy datasets may selectively include or exclude sentiment context, underscoring the need to use the full dataset to fairly assess COOTEFOO members’ behaviors."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#module-3-interactive-ego-network-explorer",
    "href": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#module-3-interactive-ego-network-explorer",
    "title": "Take-home Exercise 3",
    "section": "7. Module 3 — Interactive Ego-Network Explorer",
    "text": "7. Module 3 — Interactive Ego-Network Explorer\nUnderstanding who connects to whom — and how — offers valuable insight into each COOTEFOO board member’s influence, relationships, and presence across different narratives. This module allows users to select any board member and explore their ego network, dynamically rendered from the full knowledge graph.\n\n7.1 Objective\nThis module supports analysis of:\n\nWhich entities (e.g., projects, people, places) a board member is most connected to\nWhether those connections are reported by FILAH, TROUT, or JOURNALIST\nWhether the member serves as a bridge, is peripheral, or centrally involved in planning or public engagement\n\n\n\n7.2 Prototype Output — Interactive Ego Network\nThe following interface allows users to explore a board member’s ego network interactively. Edges represent directional relationships (e.g., “participant in”, “planner of”) and are color-coded by dataset source.\n\n# Load required libraries\nlibrary(visNetwork)\nlibrary(dplyr)\n\n# Prepare COOTEFOO board member list\ncootefoo_members &lt;- all_nodes %&gt;%\n  filter(role %in% c(\"Committee Chair\", \"Vice Chair\", \"Treasurer\", \"Member\")) %&gt;%\n  mutate(name = coalesce(label, name, id)) %&gt;%\n  distinct(id, name) %&gt;%\n  arrange(name)\n\n# Dropdown UI for board member selection\nselectInput(\n  inputId = \"selected_member\",\n  label = \"Select a COOTEFOO Board Member:\",\n  choices = setNames(cootefoo_members$id, cootefoo_members$name),\n  selected = cootefoo_members$id[1]\n)\n\n\nSelect a COOTEFOO Board Member:\n\nCarol Limpet\nEd Helpsford\nSeal\nSimone Kat\nTante Titan\nTeddy Goldstein\n\n\n\n\n# Render ego network for selected board member\nrenderVisNetwork({\n  selected_id &lt;- input$selected_member\n\n  # Extract related edges\n  ego_edges &lt;- vis_edges %&gt;%\n    filter(from == selected_id | to == selected_id)\n\n  # Handle case with no connections\n  if (nrow(ego_edges) == 0) return(NULL)\n\n  # Get connected node IDs\n  connected_ids &lt;- unique(c(ego_edges$from, ego_edges$to))\n\n  # Filter nodes involved in the ego network\n  ego_nodes &lt;- vis_nodes %&gt;%\n    filter(id %in% connected_ids) %&gt;%\n    distinct(id, .keep_all = TRUE)\n\n  # Render network\n  visNetwork(ego_nodes, ego_edges, height = \"500px\", width = \"100%\") %&gt;%\n    visOptions(highlightNearest = TRUE, nodesIdSelection = FALSE) %&gt;%\n    visLegend() %&gt;%\n    visLayout(randomSeed = 42)\n})"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09A/Hands-on_Ex09A.html",
    "href": "Hands-on_Ex/Hands-on_Ex09A/Hands-on_Ex09A.html",
    "title": "Hands-on Exercise 09A",
    "section": "",
    "text": "Ternary plots are a way of displaying the distribution and variability of three-part compositional data. (For example, the proportion of aged, economy active and young population or sand, silt, and clay in soil.) It’s display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\nIn this hands-on, you will learn how to build ternary plot programmatically using R for visualising and analysing population structure of Singapore.\nThe hands-on exercise consists of four steps:\n\nInstall and launch tidyverse and ggtern packages.\nDerive three new measures using mutate() function of dplyr package.\nBuild a static ternary plot using ggtern() function of ggtern package.\nBuild an interactive ternary plot using plot-ly() function of Plotly R package.\n\n\n\n\nFor this exercise, two main R packages will be used in this hands-on exercise, they are:\n\nggtern, a ggplot extension specially designed to plot ternary diagrams. The package will be used to plot static ternary plots.\nPlotly R, an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js . The plotly R libary contains the ggplotly function, which will convert ggplot2 figures into a Plotly object.\n\nWe will also need to ensure that selected tidyverse family packages namely: readr, dplyr and tidyr are also installed and loaded.\nThe code chunks below will accomplish the task.\n\npacman::p_load(plotly, ggtern, tidyverse)\n\n\n\n\n\n\nFor the purpose of this hands-on exercise, the Singapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used. The data set has been downloaded and included in the data sub-folder of the hands-on exercise folder. It is called respopagsex2000to2018_tidy.csv and is in csv file format.\n\n\n\nTo important respopagsex2000to2018_tidy.csv into R, read_csv() function of readr package will be used.\n\npop_data &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n\n\n\nNext, use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\n#Deriving the young, economy active and old measures\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)\n\n\n\n\n\n\n\nUse ggtern() function of ggtern package to create a simple ternary plot.\n\n#Building the static ternary plot\nggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n#Building the static ternary plot\nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population structure, 2015\") +\n  theme_rgbw()\n\n\n\n\n\n\n\n\n\n\n\nThe code below create an interactive ternary plot using plot_ly() function of Plotly R.\n\n# reusable function for creating annotation object\nlabel &lt;- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis &lt;- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes &lt;- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\"\n) %&gt;%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09A/Hands-on_Ex09A.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex09A/Hands-on_Ex09A.html#overview",
    "title": "Hands-on Exercise 09A",
    "section": "",
    "text": "Ternary plots are a way of displaying the distribution and variability of three-part compositional data. (For example, the proportion of aged, economy active and young population or sand, silt, and clay in soil.) It’s display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\nIn this hands-on, you will learn how to build ternary plot programmatically using R for visualising and analysing population structure of Singapore.\nThe hands-on exercise consists of four steps:\n\nInstall and launch tidyverse and ggtern packages.\nDerive three new measures using mutate() function of dplyr package.\nBuild a static ternary plot using ggtern() function of ggtern package.\nBuild an interactive ternary plot using plot-ly() function of Plotly R package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09A/Hands-on_Ex09A.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex09A/Hands-on_Ex09A.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 09A",
    "section": "",
    "text": "For this exercise, two main R packages will be used in this hands-on exercise, they are:\n\nggtern, a ggplot extension specially designed to plot ternary diagrams. The package will be used to plot static ternary plots.\nPlotly R, an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js . The plotly R libary contains the ggplotly function, which will convert ggplot2 figures into a Plotly object.\n\nWe will also need to ensure that selected tidyverse family packages namely: readr, dplyr and tidyr are also installed and loaded.\nThe code chunks below will accomplish the task.\n\npacman::p_load(plotly, ggtern, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09A/Hands-on_Ex09A.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex09A/Hands-on_Ex09A.html#data-preparation",
    "title": "Hands-on Exercise 09A",
    "section": "",
    "text": "For the purpose of this hands-on exercise, the Singapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used. The data set has been downloaded and included in the data sub-folder of the hands-on exercise folder. It is called respopagsex2000to2018_tidy.csv and is in csv file format.\n\n\n\nTo important respopagsex2000to2018_tidy.csv into R, read_csv() function of readr package will be used.\n\npop_data &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n\n\n\nNext, use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\n#Deriving the young, economy active and old measures\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09A/Hands-on_Ex09A.html#plotting-ternary-diagram-with-r",
    "href": "Hands-on_Ex/Hands-on_Ex09A/Hands-on_Ex09A.html#plotting-ternary-diagram-with-r",
    "title": "Hands-on Exercise 09A",
    "section": "",
    "text": "Use ggtern() function of ggtern package to create a simple ternary plot.\n\n#Building the static ternary plot\nggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n#Building the static ternary plot\nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population structure, 2015\") +\n  theme_rgbw()\n\n\n\n\n\n\n\n\n\n\n\nThe code below create an interactive ternary plot using plot_ly() function of Plotly R.\n\n# reusable function for creating annotation object\nlabel &lt;- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis &lt;- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes &lt;- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\"\n) %&gt;%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09B/Hands-on_Ex09B.html",
    "href": "Hands-on_Ex/Hands-on_Ex09B/Hands-on_Ex09B.html",
    "title": "Hands-on Exercise 09B",
    "section": "",
    "text": "Correlation coefficient is a popular statistic that use to measure the type and strength of the relationship between two variables. The values of a correlation coefficient ranges between -1.0 and 1.0. A correlation coefficient of 1 shows a perfect linear relationship between the two variables, while a -1.0 shows a perfect inverse relationship between the two variables. A correlation coefficient of 0.0 shows no linear relationship between the two variables.\nWhen multivariate data are used, the correlation coefficeints of the pair comparisons are displayed in a table form known as correlation matrix or scatterplot matrix.\nThere are three broad reasons for computing a correlation matrix.\n\nTo reveal the relationship between high-dimensional variables pair-wisely.\nTo input into other analyses. For example, people commonly use correlation matrices as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise.\nAs a diagnostic when checking other analyses. For example, with linear regression a high amount of correlations suggests that the linear regression’s estimates will be unreliable.\n\nWhen the data is large, both in terms of the number of observations and the number of variables, Corrgram tend to be used to visually explore and analyse the structure and the patterns of relations among variables. It is designed based on two main schemes:\n\nRendering the value of a correlation to depict its sign and magnitude, and\nReordering the variables in a correlation matrix so that “similar” variables are positioned adjacently, facilitating perception.\n\nIn this hands-on exercise, you will learn how to plot data visualisation for visualising correlation matrix with R. It consists of three main sections. First, you will learn how to create correlation matrix using pairs() of R Graphics. Next, you will learn how to plot corrgram using corrplot package of R. Lastly, you will learn how to create an interactive correlation matrix using plotly R.\n\n\n\nBefore you get started, you are required to open a new Quarto document. Keep the default html authoring format.\nNext, you will use the code chunk below to install and launch corrplot, ggpubr, plotly and tidyverse in RStudio.\n\npacman::p_load(corrplot, ggstatsplot, tidyverse)\n\n\n\n\nIn this hands-on exercise, the Wine Quality Data Set of UCI Machine Learning Repository will be used. The data set consists of 13 variables and 6497 observations. For the purpose of this exercise, we have combined the red wine and white wine data into one data file. It is called wine_quality and is in csv file format.\n\n\nFirst, let us import the data into R by using read_csv() of readr package.\n\nwine &lt;- read_csv(\"data/wine_quality.csv\")\n\nNotice that beside quality and type, the rest of the variables are numerical and continuous data type.\n\n\n\n\nThere are more than one way to build scatterplot matrix with R. In this section, you will learn how to create a scatterplot matrix by using the pairs function of R Graphics.\nBefore you continue to the next step, you should read the syntax description of pairsfunction.\n\n\nFigure below shows the scatter plot matrix of Wine Quality Data. It is a 11 by 11 matrix.\n\npairs(wine[,1:11])\n\n\n\n\n\n\n\n\nThe required input of pairs() can be a matrix or data frame. The code chunk used to create the scatterplot matrix is relatively simple. It uses the default pairs function. Columns 2 to 12 of wine dataframe is used to build the scatterplot matrix. The variables are: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates and alcohol.\n\npairs(wine[,2:12])\n\n\n\n\n\n\n\n\n\n\n\npairs function of R Graphics provided many customisation arguments. For example, it is a common practice to show either the upper half or lower half of the correlation matrix instead of both. This is because a correlation matrix is symmetric.\nTo show the lower half of the correlation matrix, the upper.panel argument will be used as shown in the code chunk below.\n\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\n\n\n\n\nSimilarly, you can display the upper half of the correlation matrix by using the code chun below.\n\npairs(wine[,2:12], lower.panel = NULL)\n\n\n\n\n\n\n\n\n\n\n\nTo show the correlation coefficient of each pair of variables instead of a scatter plot, panel.cor function will be used. This will also show higher correlations in a larger font.\nDon’t worry about the details for now-just type this code into your R session or script. Let’s have more fun way to display the correlation matrix.\n\npanel.cor &lt;- function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\nusr &lt;- par(\"usr\")\non.exit(par(usr))\npar(usr = c(0, 1, 0, 1))\nr &lt;- abs(cor(x, y, use=\"complete.obs\"))\ntxt &lt;- format(c(r, 0.123456789), digits=digits)[1]\ntxt &lt;- paste(prefix, txt, sep=\"\")\nif(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)\n\n\n\n\n\n\n\n\n\n\n\n\nOne of the major limitation of the correlation matrix is that the scatter plots appear very cluttered when the number of observations is relatively large (i.e. more than 500 observations). To over come this problem, Corrgram data visualisation technique suggested by D. J. Murdoch and E. D. Chow (1996) and Friendly, M (2002) and will be used.\nThe are at least three R packages provide function to plot corrgram, they are:\n\ncorrgram\nellipse\ncorrplot\n\nOn top that, some R package like ggstatsplot package also provides functions for building corrgram.\nIn this section, you will learn how to visualising correlation matrix by using ggcorrmat() of ggstatsplot package.\n\n\nOn of the advantage of using ggcorrmat() over many other methods to visualise a correlation matrix is it’s ability to provide a comprehensive and yet professional statistical report as shown in the figure below.\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11)\n\n\n\n\n\n\n\n\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\"\n)\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\ncor.vars argument is used to compute the correlation matrix needed to build the corrgram. ggcorrplot.args argument provide additional (mostly aesthetic) arguments that will be passed to ggcorrplot::ggcorrplot function. The list should avoid any of the following arguments since they are already internally being used: corr, method, p.mat, sig.level, ggtheme, colors, lab, pch, legend.title, digits. The sample sub-code chunk can be used to control specific component of the plot such as the font size of the x-axis, y-axis, and the statistical report.\n\nggplot.component = list(\n    theme(text=element_text(size=5),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))\n\n\n\n\n\nSince ggstasplot is an extension of ggplot2, it also supports faceting. However the feature is not available in ggcorrmat() but in the grouped_ggcorrmat() of ggstatsplot.\n\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\nto build a facet plot, the only argument needed is grouping.var.\nBehind group_ggcorrmat(), patchwork package is used to create the multiplot. plotgrid.args argument provides a list of additional arguments passed to patchwork::wrap_plots, except for guides argument which is already separately specified earlier.\nLikewise, annotation.args argument is calling plot annotation arguments of patchwork package.\n\n\n\n\nIn this hands-on exercise, we will focus on corrplot. However, you are encouraged to explore the other two packages too.\nBefore getting started, you are required to read An Introduction to corrplot Package in order to gain basic understanding of corrplot package.\n\n\nBefore we can plot a corrgram using corrplot(), we need to compute the correlation matrix of wine data frame.\nIn the code chunk below, cor() of R Stats is used to compute the correlation matrix of wine data frame.\n\nwine.cor &lt;- cor(wine[, 1:11])\n\nNext, corrplot() is used to plot the corrgram by using all the default setting as shown in the code chunk below.\n\ncorrplot(wine.cor)\n\n\n\n\n\n\n\n\nNotice that the default visual object used to plot the corrgram is circle. The default layout of the corrgram is a symmetric matrix. The default colour scheme is diverging blue-red. Blue colours are used to represent pair variables with positive correlation coefficients and red colours are used to represent pair variables with negative correlation coefficients. The intensity of the colour or also know as saturation is used to represent the strength of the correlation coefficient. Darker colours indicate relatively stronger linear relationship between the paired variables. On the other hand, lighter colours indicates relatively weaker linear relationship.\n\n\n\nIn corrplot package, there are seven visual geometrics (parameter method) can be used to encode the attribute values. They are: circle, square, ellipse, number, shade, color and pie. The default is circle. As shown in the previous section, the default visual geometric of corrplot matrix is circle. However, this default setting can be changed by using the method argument as shown in the code chunk below.\n\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\n\n\n\n\nFeel free to change the method argument to other supported visual geometrics.\n\n\n\ncorrplor() supports three layout types, namely: “full”, “upper” or “lower”. The default is “full” which display full matrix. The default setting can be changed by using the type argument of corrplot().\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\")\n\n\n\n\n\n\n\n\nThe default layout of the corrgram can be further customised. For example, arguments diag and tl.col are used to turn off the diagonal cells and to change the axis text label colour to black colour respectively as shown in the code chunk and figure below.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\n\n\nPlease feel free to experiment with other layout design argument such as tl.pos, tl.cex, tl.offset, cl.pos, cl.cex and cl.offset, just to mention a few of them.\n\n\n\nWith corrplot package, it is possible to design corrgram with mixed visual matrix of one half and numerical matrix on the other half. In order to create a coorgram with mixed layout, the corrplot.mixed(), a wrapped function for mixed visualisation style will be used.\nFigure below shows a mixed layout corrgram plotted using wine quality data.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\nThe code chunk used to plot the corrgram are shown below.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\nNotice that argument lower and upper are used to define the visualisation method used. In this case ellipse is used to map the lower half of the corrgram and numerical matrix (i.e. number) is used to map the upper half of the corrgram. The argument tl.pos, on the other, is used to specify the placement of the axis label. Lastly, the diag argument is used to specify the glyph on the principal diagonal of the corrgram.\n\n\n\nIn statistical analysis, we are also interested to know which pair of variables their correlation coefficients are statistically significant.\nFigure below shows a corrgram combined with the significant test. The corrgram reveals that not all correlation pairs are statistically significant. For example the correlation between total sulfur dioxide and free surfur dioxide is statistically significant at significant level of 0.1 but not the pair between total sulfur dioxide and citric acid.\n\nWith corrplot package, we can use the cor.mtest() to compute the p-values and confidence interval for each pair of variables.\n\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\nWe can then use the p.mat argument of corrplot function as shown in the code chunk below.\n\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\n\n\n\n\n\n\n\nMatrix reorder is very important for mining the hiden structure and pattern in a corrgram. By default, the order of attributes of a corrgram is sorted according to the correlation matrix (i.e. “original”). The default setting can be over-write by using the order argument of corrplot(). Currently, corrplot package support four sorting methods, they are:\n\n“AOE” is for the angular order of the eigenvectors. See Michael Friendly (2002) for details.\n“FPC” for the first principal component order.\n“hclust” for hierarchical clustering order, and “hclust.method” for the agglomeration method to be used.\n\n“hclust.method” should be one of “ward”, “single”, “complete”, “average”, “mcquitty”, “median” or “centroid”.\n\n“alphabet” for alphabetical order.\n\n“AOE”, “FPC”, “hclust”, “alphabet”. More algorithms can be found in seriation package.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\nIf using hclust, corrplot() can draw rectangles around the corrgram based on the results of hierarchical clustering.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)\n\n\n\n\n\n\n\n\n\n\n\n\nMichael Friendly (2002). “Corrgrams: Exploratory displays for correlation matrices”. The American Statistician, 56, 316–324.\nD.J. Murdoch, E.D. Chow (1996). “A graphical display of large correlation matrices”. The American Statistician, 50, 178–180.\n\n\n\nggcormat() of ggstatsplot package\nggscatmat and ggpairs of GGally.\ncorrplot. A graphical display of a correlation matrix or general matrix. It also contains some algorithms to do matrix reordering. In addition, corrplot is good at details, including choosing color, text labels, color labels, layout, etc.\ncorrgram calculates correlation of variables and displays the results graphically. Included panel functions can display points, shading, ellipses, and correlation values with confidence intervals."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09B/Hands-on_Ex09B.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex09B/Hands-on_Ex09B.html#overview",
    "title": "Hands-on Exercise 09B",
    "section": "",
    "text": "Correlation coefficient is a popular statistic that use to measure the type and strength of the relationship between two variables. The values of a correlation coefficient ranges between -1.0 and 1.0. A correlation coefficient of 1 shows a perfect linear relationship between the two variables, while a -1.0 shows a perfect inverse relationship between the two variables. A correlation coefficient of 0.0 shows no linear relationship between the two variables.\nWhen multivariate data are used, the correlation coefficeints of the pair comparisons are displayed in a table form known as correlation matrix or scatterplot matrix.\nThere are three broad reasons for computing a correlation matrix.\n\nTo reveal the relationship between high-dimensional variables pair-wisely.\nTo input into other analyses. For example, people commonly use correlation matrices as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise.\nAs a diagnostic when checking other analyses. For example, with linear regression a high amount of correlations suggests that the linear regression’s estimates will be unreliable.\n\nWhen the data is large, both in terms of the number of observations and the number of variables, Corrgram tend to be used to visually explore and analyse the structure and the patterns of relations among variables. It is designed based on two main schemes:\n\nRendering the value of a correlation to depict its sign and magnitude, and\nReordering the variables in a correlation matrix so that “similar” variables are positioned adjacently, facilitating perception.\n\nIn this hands-on exercise, you will learn how to plot data visualisation for visualising correlation matrix with R. It consists of three main sections. First, you will learn how to create correlation matrix using pairs() of R Graphics. Next, you will learn how to plot corrgram using corrplot package of R. Lastly, you will learn how to create an interactive correlation matrix using plotly R."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09B/Hands-on_Ex09B.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex09B/Hands-on_Ex09B.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 09B",
    "section": "",
    "text": "Before you get started, you are required to open a new Quarto document. Keep the default html authoring format.\nNext, you will use the code chunk below to install and launch corrplot, ggpubr, plotly and tidyverse in RStudio.\n\npacman::p_load(corrplot, ggstatsplot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09B/Hands-on_Ex09B.html#importing-and-preparing-the-data-set",
    "href": "Hands-on_Ex/Hands-on_Ex09B/Hands-on_Ex09B.html#importing-and-preparing-the-data-set",
    "title": "Hands-on Exercise 09B",
    "section": "",
    "text": "In this hands-on exercise, the Wine Quality Data Set of UCI Machine Learning Repository will be used. The data set consists of 13 variables and 6497 observations. For the purpose of this exercise, we have combined the red wine and white wine data into one data file. It is called wine_quality and is in csv file format.\n\n\nFirst, let us import the data into R by using read_csv() of readr package.\n\nwine &lt;- read_csv(\"data/wine_quality.csv\")\n\nNotice that beside quality and type, the rest of the variables are numerical and continuous data type."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09B/Hands-on_Ex09B.html#building-correlation-matrix-pairs-method",
    "href": "Hands-on_Ex/Hands-on_Ex09B/Hands-on_Ex09B.html#building-correlation-matrix-pairs-method",
    "title": "Hands-on Exercise 09B",
    "section": "",
    "text": "There are more than one way to build scatterplot matrix with R. In this section, you will learn how to create a scatterplot matrix by using the pairs function of R Graphics.\nBefore you continue to the next step, you should read the syntax description of pairsfunction.\n\n\nFigure below shows the scatter plot matrix of Wine Quality Data. It is a 11 by 11 matrix.\n\npairs(wine[,1:11])\n\n\n\n\n\n\n\n\nThe required input of pairs() can be a matrix or data frame. The code chunk used to create the scatterplot matrix is relatively simple. It uses the default pairs function. Columns 2 to 12 of wine dataframe is used to build the scatterplot matrix. The variables are: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates and alcohol.\n\npairs(wine[,2:12])\n\n\n\n\n\n\n\n\n\n\n\npairs function of R Graphics provided many customisation arguments. For example, it is a common practice to show either the upper half or lower half of the correlation matrix instead of both. This is because a correlation matrix is symmetric.\nTo show the lower half of the correlation matrix, the upper.panel argument will be used as shown in the code chunk below.\n\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\n\n\n\n\nSimilarly, you can display the upper half of the correlation matrix by using the code chun below.\n\npairs(wine[,2:12], lower.panel = NULL)\n\n\n\n\n\n\n\n\n\n\n\nTo show the correlation coefficient of each pair of variables instead of a scatter plot, panel.cor function will be used. This will also show higher correlations in a larger font.\nDon’t worry about the details for now-just type this code into your R session or script. Let’s have more fun way to display the correlation matrix.\n\npanel.cor &lt;- function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\nusr &lt;- par(\"usr\")\non.exit(par(usr))\npar(usr = c(0, 1, 0, 1))\nr &lt;- abs(cor(x, y, use=\"complete.obs\"))\ntxt &lt;- format(c(r, 0.123456789), digits=digits)[1]\ntxt &lt;- paste(prefix, txt, sep=\"\")\nif(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09B/Hands-on_Ex09B.html#visualising-correlation-matrix-ggcormat",
    "href": "Hands-on_Ex/Hands-on_Ex09B/Hands-on_Ex09B.html#visualising-correlation-matrix-ggcormat",
    "title": "Hands-on Exercise 09B",
    "section": "",
    "text": "One of the major limitation of the correlation matrix is that the scatter plots appear very cluttered when the number of observations is relatively large (i.e. more than 500 observations). To over come this problem, Corrgram data visualisation technique suggested by D. J. Murdoch and E. D. Chow (1996) and Friendly, M (2002) and will be used.\nThe are at least three R packages provide function to plot corrgram, they are:\n\ncorrgram\nellipse\ncorrplot\n\nOn top that, some R package like ggstatsplot package also provides functions for building corrgram.\nIn this section, you will learn how to visualising correlation matrix by using ggcorrmat() of ggstatsplot package.\n\n\nOn of the advantage of using ggcorrmat() over many other methods to visualise a correlation matrix is it’s ability to provide a comprehensive and yet professional statistical report as shown in the figure below.\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11)\n\n\n\n\n\n\n\n\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\"\n)\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\ncor.vars argument is used to compute the correlation matrix needed to build the corrgram. ggcorrplot.args argument provide additional (mostly aesthetic) arguments that will be passed to ggcorrplot::ggcorrplot function. The list should avoid any of the following arguments since they are already internally being used: corr, method, p.mat, sig.level, ggtheme, colors, lab, pch, legend.title, digits. The sample sub-code chunk can be used to control specific component of the plot such as the font size of the x-axis, y-axis, and the statistical report.\n\nggplot.component = list(\n    theme(text=element_text(size=5),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09B/Hands-on_Ex09B.html#building-multiple-plots",
    "href": "Hands-on_Ex/Hands-on_Ex09B/Hands-on_Ex09B.html#building-multiple-plots",
    "title": "Hands-on Exercise 09B",
    "section": "",
    "text": "Since ggstasplot is an extension of ggplot2, it also supports faceting. However the feature is not available in ggcorrmat() but in the grouped_ggcorrmat() of ggstatsplot.\n\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\nto build a facet plot, the only argument needed is grouping.var.\nBehind group_ggcorrmat(), patchwork package is used to create the multiplot. plotgrid.args argument provides a list of additional arguments passed to patchwork::wrap_plots, except for guides argument which is already separately specified earlier.\nLikewise, annotation.args argument is calling plot annotation arguments of patchwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09B/Hands-on_Ex09B.html#visualising-correlation-matrix-using-corrplot-package",
    "href": "Hands-on_Ex/Hands-on_Ex09B/Hands-on_Ex09B.html#visualising-correlation-matrix-using-corrplot-package",
    "title": "Hands-on Exercise 09B",
    "section": "",
    "text": "In this hands-on exercise, we will focus on corrplot. However, you are encouraged to explore the other two packages too.\nBefore getting started, you are required to read An Introduction to corrplot Package in order to gain basic understanding of corrplot package.\n\n\nBefore we can plot a corrgram using corrplot(), we need to compute the correlation matrix of wine data frame.\nIn the code chunk below, cor() of R Stats is used to compute the correlation matrix of wine data frame.\n\nwine.cor &lt;- cor(wine[, 1:11])\n\nNext, corrplot() is used to plot the corrgram by using all the default setting as shown in the code chunk below.\n\ncorrplot(wine.cor)\n\n\n\n\n\n\n\n\nNotice that the default visual object used to plot the corrgram is circle. The default layout of the corrgram is a symmetric matrix. The default colour scheme is diverging blue-red. Blue colours are used to represent pair variables with positive correlation coefficients and red colours are used to represent pair variables with negative correlation coefficients. The intensity of the colour or also know as saturation is used to represent the strength of the correlation coefficient. Darker colours indicate relatively stronger linear relationship between the paired variables. On the other hand, lighter colours indicates relatively weaker linear relationship.\n\n\n\nIn corrplot package, there are seven visual geometrics (parameter method) can be used to encode the attribute values. They are: circle, square, ellipse, number, shade, color and pie. The default is circle. As shown in the previous section, the default visual geometric of corrplot matrix is circle. However, this default setting can be changed by using the method argument as shown in the code chunk below.\n\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\n\n\n\n\nFeel free to change the method argument to other supported visual geometrics.\n\n\n\ncorrplor() supports three layout types, namely: “full”, “upper” or “lower”. The default is “full” which display full matrix. The default setting can be changed by using the type argument of corrplot().\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\")\n\n\n\n\n\n\n\n\nThe default layout of the corrgram can be further customised. For example, arguments diag and tl.col are used to turn off the diagonal cells and to change the axis text label colour to black colour respectively as shown in the code chunk and figure below.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\n\n\nPlease feel free to experiment with other layout design argument such as tl.pos, tl.cex, tl.offset, cl.pos, cl.cex and cl.offset, just to mention a few of them.\n\n\n\nWith corrplot package, it is possible to design corrgram with mixed visual matrix of one half and numerical matrix on the other half. In order to create a coorgram with mixed layout, the corrplot.mixed(), a wrapped function for mixed visualisation style will be used.\nFigure below shows a mixed layout corrgram plotted using wine quality data.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\nThe code chunk used to plot the corrgram are shown below.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\nNotice that argument lower and upper are used to define the visualisation method used. In this case ellipse is used to map the lower half of the corrgram and numerical matrix (i.e. number) is used to map the upper half of the corrgram. The argument tl.pos, on the other, is used to specify the placement of the axis label. Lastly, the diag argument is used to specify the glyph on the principal diagonal of the corrgram.\n\n\n\nIn statistical analysis, we are also interested to know which pair of variables their correlation coefficients are statistically significant.\nFigure below shows a corrgram combined with the significant test. The corrgram reveals that not all correlation pairs are statistically significant. For example the correlation between total sulfur dioxide and free surfur dioxide is statistically significant at significant level of 0.1 but not the pair between total sulfur dioxide and citric acid.\n\nWith corrplot package, we can use the cor.mtest() to compute the p-values and confidence interval for each pair of variables.\n\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\nWe can then use the p.mat argument of corrplot function as shown in the code chunk below.\n\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\n\n\n\n\n\n\n\nMatrix reorder is very important for mining the hiden structure and pattern in a corrgram. By default, the order of attributes of a corrgram is sorted according to the correlation matrix (i.e. “original”). The default setting can be over-write by using the order argument of corrplot(). Currently, corrplot package support four sorting methods, they are:\n\n“AOE” is for the angular order of the eigenvectors. See Michael Friendly (2002) for details.\n“FPC” for the first principal component order.\n“hclust” for hierarchical clustering order, and “hclust.method” for the agglomeration method to be used.\n\n“hclust.method” should be one of “ward”, “single”, “complete”, “average”, “mcquitty”, “median” or “centroid”.\n\n“alphabet” for alphabetical order.\n\n“AOE”, “FPC”, “hclust”, “alphabet”. More algorithms can be found in seriation package.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\nIf using hclust, corrplot() can draw rectangles around the corrgram based on the results of hierarchical clustering.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09B/Hands-on_Ex09B.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex09B/Hands-on_Ex09B.html#reference",
    "title": "Hands-on Exercise 09B",
    "section": "",
    "text": "Michael Friendly (2002). “Corrgrams: Exploratory displays for correlation matrices”. The American Statistician, 56, 316–324.\nD.J. Murdoch, E.D. Chow (1996). “A graphical display of large correlation matrices”. The American Statistician, 50, 178–180.\n\n\n\nggcormat() of ggstatsplot package\nggscatmat and ggpairs of GGally.\ncorrplot. A graphical display of a correlation matrix or general matrix. It also contains some algorithms to do matrix reordering. In addition, corrplot is good at details, including choosing color, text labels, color labels, layout, etc.\ncorrgram calculates correlation of variables and displays the results graphically. Included panel functions can display points, shading, ellipses, and correlation values with confidence intervals."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09C/Hands-on_Ex09C.html",
    "href": "Hands-on_Ex/Hands-on_Ex09C/Hands-on_Ex09C.html",
    "title": "Hands-on Exercise 09C",
    "section": "",
    "text": "Heatmaps visualise data through variations in colouring. When applied to a tabular format, heatmaps are useful for cross-examining multivariate data, through placing variables in the columns and observation (or records) in rowa and colouring the cells within the table. Heatmaps are good for showing variance across multiple variables, revealing any patterns, displaying whether any variables are similar to each other, and for detecting if any correlations exist in-between them.\nIn this hands-on exercise, you will gain hands-on experience on using R to plot static and interactive heatmap for visualising and analysing multivariate data.\n\n\n\nBefore you get started, you are required to open a new Quarto document. Keep the default html as the authoring format.\nNext, you will use the code chunk below to install and launch seriation, heatmaply, dendextend and tidyverse in RStudio.\n\npacman::p_load(seriation, dendextend, heatmaply, tidyverse)\n\n\n\n\nIn this hands-on exercise, the data of World Happines 2018 report will be used. The data set is downloaded from here. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\n\n\nIn the code chunk below, read_csv() of readr is used to import WHData-2018.csv into R and parsed it into tibble R data frame format.\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\nThe output tibbled data frame is called wh.\n\n\n\nNext, we need to change the rows by country name instead of row number by using the code chunk below\n\nrow.names(wh) &lt;- wh$Country\n\nNotice that the row number has been replaced into the country name.\n\n\n\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform wh data frame into a data matrix.\n\nwh1 &lt;- dplyr::select(wh, c(3, 7:12))\nwh_matrix &lt;- data.matrix(wh)\n\nNotice that wh_matrix is in R matrix format.\n\n\n\n\nThere are many R packages and functions can be used to drawing static heatmaps, they are:\n\nheatmap()of R stats package. It draws a simple heatmap.\nheatmap.2() of gplots R package. It draws an enhanced heatmap compared to the R base function.\npheatmap() of pheatmap R package. pheatmap package also known as Pretty Heatmap. The package provides functions to draws pretty heatmaps and provides more control to change the appearance of heatmaps.\nComplexHeatmap package of R/Bioconductor package. The package draws, annotates and arranges complex heatmaps (very useful for genomic data analysis). The full reference guide of the package is available here.\nsuperheat package: A Graphical Tool for Exploring Complex Datasets Using Heatmaps. A system for generating extendable and customizable heatmaps for exploring complex datasets, including big data and data with multiple data types. The full reference guide of the package is available here.\n\nIn this section, you will learn how to plot static heatmaps by using heatmap() of R Stats package.\n\n\nIn this sub-section, we will plot a heatmap by using heatmap() of Base Stats. The code chunk is given below.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)\n\n\n\n\n\n\n\n\nNote:\n\nBy default, heatmap() plots a cluster heatmap. The arguments Rowv=NA and Colv=NA are used to switch off the option of plotting the row and column dendrograms.\n\nTo plot a cluster heatmap, we just have to use the default as shown in the code chunk below.\n\nwh_heatmap &lt;- heatmap(wh_matrix)\n\n\n\n\n\n\n\n\nNote:\n\nThe order of both rows and columns is different compare to the native wh_matrix. This is because heatmap do a reordering using clusterisation: it calculates the distance between each pair of rows and columns and try to order them by similarity. Moreover, the corresponding dendrogram are provided beside the heatmap.\n\nHere, red cells denotes small values, and red small ones. This heatmap is not really informative. Indeed, the Happiness Score variable have relatively higher values, what makes that the other variables with small values all look the same. Thus, we need to normalize this matrix. This is done using the scale argument. It can be applied to rows or to columns following your needs.\nThe code chunk below normalises the matrix column-wise.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\n\n\n\n\nNotice that the values are scaled now. Also note that margins argument is used to ensure that the entire x-axis labels are displayed completely and, cexRow and cexCol arguments are used to define the font size used for y-axis and x-axis labels respectively.\n\n\n\n\nheatmaply is an R package for building interactive cluster heatmap that can be shared online as a stand-alone HTML file. It is designed and maintained by Tal Galili.\nBefore we get started, you should review the Introduction to Heatmaply to have an overall understanding of the features and functions of Heatmaply package. You are also required to have the user manualof the package handy with you for reference purposes.\nIn this section, you will gain hands-on experience on using heatmaply to design an interactive cluster heatmap. We will still use the wh_matrix as the input data.\n\n\n\nheatmaply(mtcars)\n\n\n\n\n\nThe code chunk below shows the basic syntax needed to create n interactive heatmap by using heatmaply package.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)])\n\n\n\n\n\nNote that:\n\nDifferent from heatmap(), for heatmaply() the default horizontal dendrogram is placed on the left hand side of the heatmap.\nThe text label of each raw, on the other hand, is placed on the right hand side of the heat map.\nWhen the x-axis marker labels are too long, they will be rotated by 135 degree from the north.\n\n\n\n\nWhen analysing multivariate data set, it is very common that the variables in the data sets includes values that reflect different types of measurement. In general, these variables’ values have their own range. In order to ensure that all the variables have comparable values, data transformation are commonly used before clustering.\nThree main data transformation methods are supported by heatmaply(), namely: scale, normalise and percentilse.\n\n\n\nWhen all variables are came from or assumed to come from some normal distribution, then scaling (i.e.: subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution.\nIn such a case, each value would reflect the distance from the mean in units of standard deviation.\nThe scale argument in heatmaply() supports column and row scaling.\n\nThe code chunk below is used to scale variable values columewise.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\n\n\n\nWhen variables in the data comes from possibly different (and non-normal) distributions, the normalize function can be used to bring data to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations. This preserves the shape of each variable’s distribution while making them easily comparable on the same “scale”. Different from Scaling, the normalise method is performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\nThis is similar to ranking the variables, but instead of keeping the rank values, divide them by the maximal rank. This is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile. The benefit of the percentize function is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it. Similar to Normalize method, the Percentize method is also performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\nheatmaply supports a variety of hierarchical clustering algorithm. The main arguments provided are:\n\ndistfun: function used to compute the distance (dissimilarity) between both rows and columns. Defaults to dist. The options “pearson”, “spearman” and “kendall” can be used to use correlation-based clustering, which uses as.dist(1 - cor(t(x))) as the distance metric (using the specified correlation method).\nhclustfun: function used to compute the hierarchical clustering when Rowv or Colv are not dendrograms. Defaults to hclust.\ndist_method default is NULL, which results in “euclidean” to be used. It can accept alternative character strings indicating the method to be passed to distfun. By default distfun is “dist”” hence this can be one of “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski”.\nhclust_method default is NULL, which results in “complete” method to be used. It can accept alternative character strings indicating the method to be passed to hclustfun. By default hclustfun is hclust hence this can be one of “ward.D”, “ward.D2”, “single”, “complete”, “average” (= UPGMA), “mcquitty” (= WPGMA), “median” (= WPGMC) or “centroid” (= UPGMC).\n\nIn general, a clustering model can be calibrated either manually or statistically.\n\n\n\nIn the code chunk below, the heatmap is plotted by using hierachical clustering algorithm with “Euclidean distance” and “ward.D” method.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\n\n\nIn order to determine the best clustering method and number of cluster the dend_expend() and find_k() functions of dendextend package will be used.\nFirst, the dend_expend() will be used to determine the recommended clustering method to be used.\n\nwh_d &lt;- dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nThe output table shows that “average” method should be used because it gave the high optimum value.\nNext, find_k() is used to determine the optimal number of cluster.\n\nwh_clust &lt;- hclust(wh_d, method = \"average\")\nnum_k &lt;- find_k(wh_clust)\nplot(num_k)\n\n\n\n\n\n\n\n\nFigure above shows that k=3 would be good.\nWith reference to the statistical analysis results, we can prepare the code chunk as shown below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)\n\n\n\n\n\n\n\n\nOne of the problems with hierarchical clustering is that it doesn’t actually place the rows in a definite order, it merely constrains the space of possible orderings. Take three items A, B and C. If you ignore reflections, there are three possible orderings: ABC, ACB, BAC. If clustering them gives you ((A+B)+C) as a tree, you know that C can’t end up between A and B, but it doesn’t tell you which way to flip the A+B cluster. It doesn’t tell you if the ABC ordering will lead to a clearer-looking heatmap than the BAC ordering.\nheatmaply uses the seriation package to find an optimal ordering of rows and columns. Optimal means to optimize the Hamiltonian path length that is restricted by the dendrogram structure. This, in other words, means to rotate the branches so that the sum of distances between each adjacent leaf (label) will be minimized. This is related to a restricted version of the travelling salesman problem.\nHere we meet our first seriation algorithm: Optimal Leaf Ordering (OLO). This algorithm starts with the output of an agglomerative clustering algorithm and produces a unique ordering, one that flips the various branches of the dendrogram around so as to minimize the sum of dissimilarities between adjacent leaves. Here is the result of applying Optimal Leaf Ordering to the same clustering result as the heatmap above.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\nThe default options is “OLO” (Optimal leaf ordering) which optimizes the above criterion (in O(n^4)). Another option is “GW” (Gruvaeus and Wainer) which aims for the same goal but uses a potentially faster heuristic.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\n\n\n\n\nThe option “mean” gives the output we would get by default from heatmap functions in other packages such as gplots::heatmap.2.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\n\nThe option “none” gives us the dendrograms without any rotation that is based on the data matrix.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")\n\n\n\n\n\n\n\n\nThe default colour palette uses by heatmaply is viridis. heatmaply users, however, can use other colour palettes in order to improve the aestheticness and visual friendliness of the heatmap.\nIn the code chunk below, the Blues colour palette of rColorBrewer is used\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\n\n\n\n\n\n\n\nBeside providing a wide collection of arguments for meeting the statistical analysis needs, heatmaply also provides many plotting features to ensure cartographic quality heatmap can be produced.\nIn the code chunk below the following arguments are used:\n\nk_row is used to produce 5 groups.\nmargins is used to change the top margin to 60 and row margin to 200.\nfontsizw_row and fontsize_col are used to change the font size for row and column labels to 4.\nmain is used to write the main title of the plot.\nxlab and ylab are used to write the x-axis and y-axis labels respectively.\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09C/Hands-on_Ex09C.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex09C/Hands-on_Ex09C.html#overview",
    "title": "Hands-on Exercise 09C",
    "section": "",
    "text": "Heatmaps visualise data through variations in colouring. When applied to a tabular format, heatmaps are useful for cross-examining multivariate data, through placing variables in the columns and observation (or records) in rowa and colouring the cells within the table. Heatmaps are good for showing variance across multiple variables, revealing any patterns, displaying whether any variables are similar to each other, and for detecting if any correlations exist in-between them.\nIn this hands-on exercise, you will gain hands-on experience on using R to plot static and interactive heatmap for visualising and analysing multivariate data."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09C/Hands-on_Ex09C.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex09C/Hands-on_Ex09C.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 09C",
    "section": "",
    "text": "Before you get started, you are required to open a new Quarto document. Keep the default html as the authoring format.\nNext, you will use the code chunk below to install and launch seriation, heatmaply, dendextend and tidyverse in RStudio.\n\npacman::p_load(seriation, dendextend, heatmaply, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09C/Hands-on_Ex09C.html#importing-and-preparing-the-data-set",
    "href": "Hands-on_Ex/Hands-on_Ex09C/Hands-on_Ex09C.html#importing-and-preparing-the-data-set",
    "title": "Hands-on Exercise 09C",
    "section": "",
    "text": "In this hands-on exercise, the data of World Happines 2018 report will be used. The data set is downloaded from here. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\n\n\nIn the code chunk below, read_csv() of readr is used to import WHData-2018.csv into R and parsed it into tibble R data frame format.\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\nThe output tibbled data frame is called wh.\n\n\n\nNext, we need to change the rows by country name instead of row number by using the code chunk below\n\nrow.names(wh) &lt;- wh$Country\n\nNotice that the row number has been replaced into the country name.\n\n\n\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform wh data frame into a data matrix.\n\nwh1 &lt;- dplyr::select(wh, c(3, 7:12))\nwh_matrix &lt;- data.matrix(wh)\n\nNotice that wh_matrix is in R matrix format."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09C/Hands-on_Ex09C.html#static-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex09C/Hands-on_Ex09C.html#static-heatmap",
    "title": "Hands-on Exercise 09C",
    "section": "",
    "text": "There are many R packages and functions can be used to drawing static heatmaps, they are:\n\nheatmap()of R stats package. It draws a simple heatmap.\nheatmap.2() of gplots R package. It draws an enhanced heatmap compared to the R base function.\npheatmap() of pheatmap R package. pheatmap package also known as Pretty Heatmap. The package provides functions to draws pretty heatmaps and provides more control to change the appearance of heatmaps.\nComplexHeatmap package of R/Bioconductor package. The package draws, annotates and arranges complex heatmaps (very useful for genomic data analysis). The full reference guide of the package is available here.\nsuperheat package: A Graphical Tool for Exploring Complex Datasets Using Heatmaps. A system for generating extendable and customizable heatmaps for exploring complex datasets, including big data and data with multiple data types. The full reference guide of the package is available here.\n\nIn this section, you will learn how to plot static heatmaps by using heatmap() of R Stats package.\n\n\nIn this sub-section, we will plot a heatmap by using heatmap() of Base Stats. The code chunk is given below.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)\n\n\n\n\n\n\n\n\nNote:\n\nBy default, heatmap() plots a cluster heatmap. The arguments Rowv=NA and Colv=NA are used to switch off the option of plotting the row and column dendrograms.\n\nTo plot a cluster heatmap, we just have to use the default as shown in the code chunk below.\n\nwh_heatmap &lt;- heatmap(wh_matrix)\n\n\n\n\n\n\n\n\nNote:\n\nThe order of both rows and columns is different compare to the native wh_matrix. This is because heatmap do a reordering using clusterisation: it calculates the distance between each pair of rows and columns and try to order them by similarity. Moreover, the corresponding dendrogram are provided beside the heatmap.\n\nHere, red cells denotes small values, and red small ones. This heatmap is not really informative. Indeed, the Happiness Score variable have relatively higher values, what makes that the other variables with small values all look the same. Thus, we need to normalize this matrix. This is done using the scale argument. It can be applied to rows or to columns following your needs.\nThe code chunk below normalises the matrix column-wise.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\n\n\n\n\nNotice that the values are scaled now. Also note that margins argument is used to ensure that the entire x-axis labels are displayed completely and, cexRow and cexCol arguments are used to define the font size used for y-axis and x-axis labels respectively."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09C/Hands-on_Ex09C.html#creating-interactive-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex09C/Hands-on_Ex09C.html#creating-interactive-heatmap",
    "title": "Hands-on Exercise 09C",
    "section": "",
    "text": "heatmaply is an R package for building interactive cluster heatmap that can be shared online as a stand-alone HTML file. It is designed and maintained by Tal Galili.\nBefore we get started, you should review the Introduction to Heatmaply to have an overall understanding of the features and functions of Heatmaply package. You are also required to have the user manualof the package handy with you for reference purposes.\nIn this section, you will gain hands-on experience on using heatmaply to design an interactive cluster heatmap. We will still use the wh_matrix as the input data.\n\n\n\nheatmaply(mtcars)\n\n\n\n\n\nThe code chunk below shows the basic syntax needed to create n interactive heatmap by using heatmaply package.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)])\n\n\n\n\n\nNote that:\n\nDifferent from heatmap(), for heatmaply() the default horizontal dendrogram is placed on the left hand side of the heatmap.\nThe text label of each raw, on the other hand, is placed on the right hand side of the heat map.\nWhen the x-axis marker labels are too long, they will be rotated by 135 degree from the north.\n\n\n\n\nWhen analysing multivariate data set, it is very common that the variables in the data sets includes values that reflect different types of measurement. In general, these variables’ values have their own range. In order to ensure that all the variables have comparable values, data transformation are commonly used before clustering.\nThree main data transformation methods are supported by heatmaply(), namely: scale, normalise and percentilse.\n\n\n\nWhen all variables are came from or assumed to come from some normal distribution, then scaling (i.e.: subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution.\nIn such a case, each value would reflect the distance from the mean in units of standard deviation.\nThe scale argument in heatmaply() supports column and row scaling.\n\nThe code chunk below is used to scale variable values columewise.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\n\n\n\nWhen variables in the data comes from possibly different (and non-normal) distributions, the normalize function can be used to bring data to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations. This preserves the shape of each variable’s distribution while making them easily comparable on the same “scale”. Different from Scaling, the normalise method is performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\nThis is similar to ranking the variables, but instead of keeping the rank values, divide them by the maximal rank. This is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile. The benefit of the percentize function is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it. Similar to Normalize method, the Percentize method is also performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\nheatmaply supports a variety of hierarchical clustering algorithm. The main arguments provided are:\n\ndistfun: function used to compute the distance (dissimilarity) between both rows and columns. Defaults to dist. The options “pearson”, “spearman” and “kendall” can be used to use correlation-based clustering, which uses as.dist(1 - cor(t(x))) as the distance metric (using the specified correlation method).\nhclustfun: function used to compute the hierarchical clustering when Rowv or Colv are not dendrograms. Defaults to hclust.\ndist_method default is NULL, which results in “euclidean” to be used. It can accept alternative character strings indicating the method to be passed to distfun. By default distfun is “dist”” hence this can be one of “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski”.\nhclust_method default is NULL, which results in “complete” method to be used. It can accept alternative character strings indicating the method to be passed to hclustfun. By default hclustfun is hclust hence this can be one of “ward.D”, “ward.D2”, “single”, “complete”, “average” (= UPGMA), “mcquitty” (= WPGMA), “median” (= WPGMC) or “centroid” (= UPGMC).\n\nIn general, a clustering model can be calibrated either manually or statistically.\n\n\n\nIn the code chunk below, the heatmap is plotted by using hierachical clustering algorithm with “Euclidean distance” and “ward.D” method.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\n\n\nIn order to determine the best clustering method and number of cluster the dend_expend() and find_k() functions of dendextend package will be used.\nFirst, the dend_expend() will be used to determine the recommended clustering method to be used.\n\nwh_d &lt;- dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nThe output table shows that “average” method should be used because it gave the high optimum value.\nNext, find_k() is used to determine the optimal number of cluster.\n\nwh_clust &lt;- hclust(wh_d, method = \"average\")\nnum_k &lt;- find_k(wh_clust)\nplot(num_k)\n\n\n\n\n\n\n\n\nFigure above shows that k=3 would be good.\nWith reference to the statistical analysis results, we can prepare the code chunk as shown below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)\n\n\n\n\n\n\n\n\nOne of the problems with hierarchical clustering is that it doesn’t actually place the rows in a definite order, it merely constrains the space of possible orderings. Take three items A, B and C. If you ignore reflections, there are three possible orderings: ABC, ACB, BAC. If clustering them gives you ((A+B)+C) as a tree, you know that C can’t end up between A and B, but it doesn’t tell you which way to flip the A+B cluster. It doesn’t tell you if the ABC ordering will lead to a clearer-looking heatmap than the BAC ordering.\nheatmaply uses the seriation package to find an optimal ordering of rows and columns. Optimal means to optimize the Hamiltonian path length that is restricted by the dendrogram structure. This, in other words, means to rotate the branches so that the sum of distances between each adjacent leaf (label) will be minimized. This is related to a restricted version of the travelling salesman problem.\nHere we meet our first seriation algorithm: Optimal Leaf Ordering (OLO). This algorithm starts with the output of an agglomerative clustering algorithm and produces a unique ordering, one that flips the various branches of the dendrogram around so as to minimize the sum of dissimilarities between adjacent leaves. Here is the result of applying Optimal Leaf Ordering to the same clustering result as the heatmap above.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\nThe default options is “OLO” (Optimal leaf ordering) which optimizes the above criterion (in O(n^4)). Another option is “GW” (Gruvaeus and Wainer) which aims for the same goal but uses a potentially faster heuristic.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\n\n\n\n\nThe option “mean” gives the output we would get by default from heatmap functions in other packages such as gplots::heatmap.2.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\n\nThe option “none” gives us the dendrograms without any rotation that is based on the data matrix.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")\n\n\n\n\n\n\n\n\nThe default colour palette uses by heatmaply is viridis. heatmaply users, however, can use other colour palettes in order to improve the aestheticness and visual friendliness of the heatmap.\nIn the code chunk below, the Blues colour palette of rColorBrewer is used\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\n\n\n\n\n\n\n\nBeside providing a wide collection of arguments for meeting the statistical analysis needs, heatmaply also provides many plotting features to ensure cartographic quality heatmap can be produced.\nIn the code chunk below the following arguments are used:\n\nk_row is used to produce 5 groups.\nmargins is used to change the top margin to 60 and row margin to 200.\nfontsizw_row and fontsize_col are used to change the font size for row and column labels to 4.\nmain is used to write the main title of the plot.\nxlab and ylab are used to write the x-axis and y-axis labels respectively.\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09D/Hands-on_Ex09D.html",
    "href": "Hands-on_Ex/Hands-on_Ex09D/Hands-on_Ex09D.html",
    "title": "Hands-on Exercise 09D",
    "section": "",
    "text": "Parallel coordinates plot is a data visualisation specially designed for visualising and analysing multivariate, numerical data. It is ideal for comparing multiple variables together and seeing the relationships between them. For example, the variables contribute to Happiness Index. Parallel coordinates was invented by Alfred Inselberg in the 1970s as a way to visualize high-dimensional data. This data visualisation technique is more often found in academic and scientific communities than in business and consumer data visualizations. As pointed out by Stephen Few(2006), “This certainly isn’t a chart that you would present to the board of directors or place on your Web site for the general public. In fact, the strength of parallel coordinates isn’t in their ability to communicate some truth in the data to others, but rather in their ability to bring meaningful multivariate patterns and comparisons to light when used interactively for analysis.” For example, parallel coordinates plot can be used to characterise clusters detected during customer segmentation.\nBy the end of this hands-on exercise, you will gain hands-on experience on:\n\nplotting statistic parallel coordinates plots by using ggparcoord() of GGally package, and\nplotting interactive parallel coordinates plots by using parallelPlot package.\n\n\n\n\nFor this exercise, the GGally, parallelPlot and tidyverse packages will be used.\nThe code chunks below are used to install and load the packages in R.\n\npacman::p_load(GGally, parallelPlot, tidyverse)\n\n\n\n\nIn this hands-on exercise, the World Happinees 2018 (http://worldhappiness.report/ed/2018/) data will be used. The data set is download at https://s3.amazonaws.com/happiness-report/2018/WHR2018Chapter2OnlineData.xls. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\nIn the code chunk below, read_csv() of readr package is used to import WHData-2018.csv into R and save it into a tibble data frame object called wh.\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\n\n\n\nIn this section, we will learn how to plot static parallel coordinates plot by using ggparcoord() of GGally package. Before getting started, it is a good practice to read the function description in detail.\n\n\nCode chunk below shows a typical syntax used to plot a basic static parallel coordinates plot by using ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12))\n\n\n\n\n\n\n\n\nNotice that only two argument namely data and columns is used. Data argument is used to map the data object (i.e. wh) and columns is used to select the columns for preparing the parallel coordinates plot.\n\n\n\nThe basic parallel coordinates failed to reveal any meaning understanding of the World Happiness measures. In this section, you will learn how to makeover the plot by using a collection of arguments provided by ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above.\n\ngroupColumn argument is used to group the observations (i.e. parallel lines) by using a single variable (i.e. Region) and colour the parallel coordinates lines by region name.\nscale argument is used to scale the variables in the parallel coordinate plot by using uniminmax method. The method univariately scale each variable so the minimum of the variable is zero and the maximum is one.\nalphaLines argument is used to reduce the intensity of the line colour to 0.2. The permissible value range is between 0 to 1.\nboxplot argument is used to turn on the boxplot by using logical TRUE. The default is FALSE.\ntitle argument is used to provide the parallel coordinates plot a title.\n\n\n\n\nSince ggparcoord() is developed by extending ggplot2 package, we can combination use some of the ggplot2 function when plotting a parallel coordinates plot.\nIn the code chunk below, facet_wrap() of ggplot2 is used to plot 10 small multiple parallel coordinates plots. Each plot represent one geographical region such as East Asia.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\n\n\n\n\n\n\nOne of the aesthetic defect of the current design is that some of the variable names overlap on x-axis.\n\n\n\nTo make the x-axis text label easy to read, let us rotate the labels by 30 degrees. We can rotate axis text labels using theme() function in ggplot2 as shown in the code chunk below\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above:\n\nTo rotate x-axis text labels, we use axis.text.x as argument to theme() function. And we specify element_text(angle = 30) to rotate the x-axis text by an angle 30 degree.\n\n\n\n\nRotating x-axis text labels to 30 degrees makes the label overlap with the plot and we can avoid this by adjusting the text location using hjust argument to theme’s text element with element_text(). We use axis.text.x as we want to change the look of x-axis text.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30, hjust=1))\n\n\n\n\n\n\n\n\n\n\n\n\nparallelPlot is an R package specially designed to plot a parallel coordinates plot by using ‘htmlwidgets’ package and d3.js. In this section, you will learn how to use functions provided in parallelPlot package to build interactive parallel coordinates plot.\n\n\nThe code chunk below plot an interactive parallel coordinates plot by using parallelPlot().\n\nwh &lt;- wh %&gt;%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\nNotice that some of the axis labels are too long. You will learn how to overcome this problem in the next step.\n\n\n\nIn the code chunk below, rotateTitle argument is used to avoid overlapping axis labels.\n\nparallelPlot(wh,\n             rotateTitle = TRUE)\n\n\n\n\n\nOne of the useful interactive feature of parallelPlot is we can click on a variable of interest, for example Happiness score, the monotonous blue colour (default) will change a blues with different intensity colour scheme will be used.\n\n\n\nWe can change the default blue colour scheme by using continousCS argument as shown in the code chunl below.\n\nparallelPlot(wh,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n\nIn the code chunk below, histoVisibility argument is used to plot histogram along the axis of each variables.\n\nhistoVisibility &lt;- rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)\n\n\n\n\n\n\n\n\n\n\nggparcoord() of GGally package\nparallelPlot"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09D/Hands-on_Ex09D.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex09D/Hands-on_Ex09D.html#overview",
    "title": "Hands-on Exercise 09D",
    "section": "",
    "text": "Parallel coordinates plot is a data visualisation specially designed for visualising and analysing multivariate, numerical data. It is ideal for comparing multiple variables together and seeing the relationships between them. For example, the variables contribute to Happiness Index. Parallel coordinates was invented by Alfred Inselberg in the 1970s as a way to visualize high-dimensional data. This data visualisation technique is more often found in academic and scientific communities than in business and consumer data visualizations. As pointed out by Stephen Few(2006), “This certainly isn’t a chart that you would present to the board of directors or place on your Web site for the general public. In fact, the strength of parallel coordinates isn’t in their ability to communicate some truth in the data to others, but rather in their ability to bring meaningful multivariate patterns and comparisons to light when used interactively for analysis.” For example, parallel coordinates plot can be used to characterise clusters detected during customer segmentation.\nBy the end of this hands-on exercise, you will gain hands-on experience on:\n\nplotting statistic parallel coordinates plots by using ggparcoord() of GGally package, and\nplotting interactive parallel coordinates plots by using parallelPlot package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09D/Hands-on_Ex09D.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex09D/Hands-on_Ex09D.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 09D",
    "section": "",
    "text": "For this exercise, the GGally, parallelPlot and tidyverse packages will be used.\nThe code chunks below are used to install and load the packages in R.\n\npacman::p_load(GGally, parallelPlot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09D/Hands-on_Ex09D.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex09D/Hands-on_Ex09D.html#data-preparation",
    "title": "Hands-on Exercise 09D",
    "section": "",
    "text": "In this hands-on exercise, the World Happinees 2018 (http://worldhappiness.report/ed/2018/) data will be used. The data set is download at https://s3.amazonaws.com/happiness-report/2018/WHR2018Chapter2OnlineData.xls. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\nIn the code chunk below, read_csv() of readr package is used to import WHData-2018.csv into R and save it into a tibble data frame object called wh.\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09D/Hands-on_Ex09D.html#plotting-static-parallel-coordinates-plot",
    "href": "Hands-on_Ex/Hands-on_Ex09D/Hands-on_Ex09D.html#plotting-static-parallel-coordinates-plot",
    "title": "Hands-on Exercise 09D",
    "section": "",
    "text": "In this section, we will learn how to plot static parallel coordinates plot by using ggparcoord() of GGally package. Before getting started, it is a good practice to read the function description in detail.\n\n\nCode chunk below shows a typical syntax used to plot a basic static parallel coordinates plot by using ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12))\n\n\n\n\n\n\n\n\nNotice that only two argument namely data and columns is used. Data argument is used to map the data object (i.e. wh) and columns is used to select the columns for preparing the parallel coordinates plot.\n\n\n\nThe basic parallel coordinates failed to reveal any meaning understanding of the World Happiness measures. In this section, you will learn how to makeover the plot by using a collection of arguments provided by ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above.\n\ngroupColumn argument is used to group the observations (i.e. parallel lines) by using a single variable (i.e. Region) and colour the parallel coordinates lines by region name.\nscale argument is used to scale the variables in the parallel coordinate plot by using uniminmax method. The method univariately scale each variable so the minimum of the variable is zero and the maximum is one.\nalphaLines argument is used to reduce the intensity of the line colour to 0.2. The permissible value range is between 0 to 1.\nboxplot argument is used to turn on the boxplot by using logical TRUE. The default is FALSE.\ntitle argument is used to provide the parallel coordinates plot a title.\n\n\n\n\nSince ggparcoord() is developed by extending ggplot2 package, we can combination use some of the ggplot2 function when plotting a parallel coordinates plot.\nIn the code chunk below, facet_wrap() of ggplot2 is used to plot 10 small multiple parallel coordinates plots. Each plot represent one geographical region such as East Asia.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\n\n\n\n\n\n\nOne of the aesthetic defect of the current design is that some of the variable names overlap on x-axis.\n\n\n\nTo make the x-axis text label easy to read, let us rotate the labels by 30 degrees. We can rotate axis text labels using theme() function in ggplot2 as shown in the code chunk below\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above:\n\nTo rotate x-axis text labels, we use axis.text.x as argument to theme() function. And we specify element_text(angle = 30) to rotate the x-axis text by an angle 30 degree.\n\n\n\n\nRotating x-axis text labels to 30 degrees makes the label overlap with the plot and we can avoid this by adjusting the text location using hjust argument to theme’s text element with element_text(). We use axis.text.x as we want to change the look of x-axis text.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30, hjust=1))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09D/Hands-on_Ex09D.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "href": "Hands-on_Ex/Hands-on_Ex09D/Hands-on_Ex09D.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "title": "Hands-on Exercise 09D",
    "section": "",
    "text": "parallelPlot is an R package specially designed to plot a parallel coordinates plot by using ‘htmlwidgets’ package and d3.js. In this section, you will learn how to use functions provided in parallelPlot package to build interactive parallel coordinates plot.\n\n\nThe code chunk below plot an interactive parallel coordinates plot by using parallelPlot().\n\nwh &lt;- wh %&gt;%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\nNotice that some of the axis labels are too long. You will learn how to overcome this problem in the next step.\n\n\n\nIn the code chunk below, rotateTitle argument is used to avoid overlapping axis labels.\n\nparallelPlot(wh,\n             rotateTitle = TRUE)\n\n\n\n\n\nOne of the useful interactive feature of parallelPlot is we can click on a variable of interest, for example Happiness score, the monotonous blue colour (default) will change a blues with different intensity colour scheme will be used.\n\n\n\nWe can change the default blue colour scheme by using continousCS argument as shown in the code chunl below.\n\nparallelPlot(wh,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n\nIn the code chunk below, histoVisibility argument is used to plot histogram along the axis of each variables.\n\nhistoVisibility &lt;- rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09D/Hands-on_Ex09D.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex09D/Hands-on_Ex09D.html#references",
    "title": "Hands-on Exercise 09D",
    "section": "",
    "text": "ggparcoord() of GGally package\nparallelPlot"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09E/Hands-on_Ex09E.html",
    "href": "Hands-on_Ex/Hands-on_Ex09E/Hands-on_Ex09E.html",
    "title": "Hands-on Exercise 09E",
    "section": "",
    "text": "In this hands-on exercise, you will gain hands-on experiences on designing treemap using appropriate R packages. The hands-on exercise consists of three main section. First, you will learn how to manipulate transaction data into a treemap strcuture by using selected functions provided in dplyr package. Then, you will learn how to plot static treemap by using treemap package. In the third section, you will learn how to design interactive treemap by using d3treeR package.\n\n\n\nBefore we get started, you are required to check if treemap and tidyverse pacakges have been installed in you R.\n\npacman::p_load(treemap, treemapify, tidyverse) \n\n\n\n\nIn this exercise, REALIS2018.csv data will be used. This dataset provides information of private property transaction records in 2018. The dataset is extracted from REALIS portal (https://spring.ura.gov.sg/lad/ore/login/index.cfm) of Urban Redevelopment Authority (URA).\n\n\nIn the code chunk below, read_csv() of readr is used to import realis2018.csv into R and parsed it into tibble R data.frame format.\n\nrealis2018 &lt;- read_csv(\"data/realis2018.csv\")\n\nThe output tibble data.frame is called realis2018.\n\n\n\nThe data.frame realis2018 is in trasaction record form, which is highly disaggregated and not appropriate to be used to plot a treemap. In this section, we will perform the following steps to manipulate and prepare a data.frtame that is appropriate for treemap visualisation:\n\ngroup transaction records by Project Name, Planning Region, Planning Area, Property Type and Type of Sale, and\ncompute Total Unit Sold, Total Area, Median Unit Price and Median Transacted Price by applying appropriate summary statistics on No. of Units, Area (sqm), Unit Price ($ psm) and Transacted Price ($) respectively.\n\nTwo key verbs of dplyr package, namely: group_by() and summarize() will be used to perform these steps.\ngroup_by() breaks down a data.frame into specified groups of rows. When you then apply the verbs above on the resulting object they’ll be automatically applied “by group”.\nGrouping affects the verbs as follows:\n\ngrouped select() is the same as ungrouped select(), except that grouping variables are always retained.\ngrouped arrange() is the same as ungrouped; unless you set .by_group = TRUE, in which case it orders first by the grouping variables.\nmutate() and filter() are most useful in conjunction with window functions (like rank(), or min(x) == x). They are described in detail in vignette(“window-functions”).\nsample_n() and sample_frac() sample the specified number/fraction of rows in each group.\nsummarise() computes the summary for each group.\n\nIn our case, group_by() will used together with summarise() to derive the summarised data.frame.\n\n\n\n\n\n\nRecommendation:\n\n\n\nIf you are new to dplyr methods should consult Introduction to dplyr before moving on to the next section.\n\n\n\n\n\nThe code chank below shows a typical two lines code approach to perform the steps.\n\nrealis2018_grouped &lt;- group_by(realis2018, `Project Name`,\n                               `Planning Region`, `Planning Area`, \n                               `Property Type`, `Type of Sale`)\nrealis2018_summarised &lt;- summarise(realis2018_grouped, \n                          `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n                          `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n                          `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n                          `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n\n\n\n\n\n\nNote:\n\n\n\nAggregation functions such as sum() and meadian() obey the usual rule of missing values: if there’s any missing value in the input, the output will be a missing value. The argument na.rm = TRUE removes the missing values prior to computation.\n\n\nThe code chunk above is not very efficient because we have to give each intermediate data.frame a name, even though we don’t have to care about it.\n\n\n\nThe code chunk below shows a more efficient way to tackle the same processes by using the pipe, %&gt;%:\n\n\n\n\n\n\nRecommendation:\n\n\n\nTo learn more about pipe, visit this excellent article: Pipes in R Tutorial For Beginners.\n\n\n\nrealis2018_summarised &lt;- realis2018 %&gt;% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %&gt;%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n\n\n\n\ntreemap package is a R package specially designed to offer great flexibility in drawing treemaps. The core function, namely: treemap() offers at least 43 arguments. In this section, we will only explore the major arguments for designing elegent and yet truthful treemaps.\n\n\nIn this section, treemap() of Treemap package is used to plot a treemap showing the distribution of median unit prices and total unit sold of resale condominium by geographic hierarchy in 2017.\nFirst, we will select records belongs to resale condominium property type from realis2018_selected data frame.\n\nrealis2018_selected &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\n\n\n\nThe code chunk below designed a treemap by using three core arguments of treemap(), namely: index, vSize and vColor.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThings to learn from the three arguments used:\n\nindex\n\nThe index vector must consist of at least two column names or else no hierarchy treemap will be plotted.\nIf multiple column names are provided, such as the code chunk above, the first name is the highest aggregation level, the second name the second highest aggregation level, and so on.\n\nvSize\n\nThe column must not contain negative values. This is because it’s vaues will be used to map the sizes of the rectangles of the treemaps.\n\n\nWarning:\nThe treemap above was wrongly coloured. For a correctly designed treemap, the colours of the rectagles should be in different intensity showing, in our case, median unit prices.\nFor treemap(), vColor is used in combination with the argument type to determines the colours of the rectangles. Without defining type, like the code chunk above, treemap() assumes type = index, in our case, the hierarchy of planning areas.\n\n\n\nIn the code chunk below, type argument is define as value.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThinking to learn from the conde chunk above.\n\nThe rectangles are coloured with different intensity of green, reflecting their respective median unit prices.\nThe legend reveals that the values are binned into ten bins, i.e. 0-5000, 5000-10000, etc. with an equal interval of 5000.\n\n\n\n\nThere are two arguments that determine the mapping to color palettes: mapping and palette. The only difference between “value” and “manual” is the default value for mapping. The “value” treemap considers palette to be a diverging color palette (say ColorBrewer’s “RdYlBu”), and maps it in such a way that 0 corresponds to the middle color (typically white or yellow), -max(abs(values)) to the left-end color, and max(abs(values)), to the right-end color. The “manual” treemap simply maps min(values) to the left-end color, max(values) to the right-end color, and mean(range(values)) to the middle color.\n\n\n\nThe code chunk below shows a value type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above:\n\nalthough the colour palette used is RdYlBu but there are no red rectangles in the treemap above. This is because all the median unit prices are positive.\nThe reason why we see only 5000 to 45000 in the legend is because the range argument is by default c(min(values, max(values)) with some pretty rounding.\n\n\n\n\nThe “manual” type does not interpret the values as the “value” type does. Instead, the value range is mapped linearly to the colour palette.\nThe code chunk below shows a manual type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\nThe colour scheme used is very confusing. This is because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative\n\nTo overcome this problem, a single colour palette such as Blues should be used.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\ntreemap() supports two popular treemap layouts, namely: “squarified” and “pivotSize”. The default is “pivotSize”.\nThe squarified treemap algorithm (Bruls et al., 2000) produces good aspect ratios, but ignores the sorting order of the rectangles (sortID). The ordered treemap, pivot-by-size, algorithm (Bederson et al., 2002) takes the sorting order (sortID) into account while aspect ratios are still acceptable.\n\n\n\nThe code chunk below plots a squarified treemap by changing the algorithm argument.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\nWhen “pivotSize” algorithm is used, sortID argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\n\ntreemapify is a R package specially developed to draw treemaps in ggplot2. In this section, you will learn how to designing treemps closely resemble treemaps designing in previous section by using treemapify. Before you getting started, you should read Introduction to “treemapify” its user guide.\n\n\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\nGroup by Planning Region\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\n\n\n\n\nGroup by Planning Area\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\n\n\n\n\n\n\nAdding boundary line\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\",\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis slide shows you how to install a R package which is not available in cran.\n\nIf this is the first time you install a package from github, you should install devtools package by using the code below or else you can skip this step.\n\n\ninstall.packages(\"devtools\", repos = \"https://cloud.r-project.org\")\n\npackage 'devtools' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\ASUS\\AppData\\Local\\Temp\\RtmpCo0f42\\downloaded_packages\n\n\n\nNext, you will load the devtools library and install the package found in github by using the codes below.\n\n\nlibrary(devtools)\ninstall_github(\"timelyportfolio/d3treeR\")\n\n\nNow you are ready to launch d3treeR package\n\n\nlibrary(d3treeR)\n\n\n\n\nThe codes below perform two processes.\n\ntreemap() is used to build a treemap by using selected variables in condominium data.frame. The treemap created is save as object called tm.\n\n\ntm &lt;- treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Planning Area\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        title=\"Private Residential Property Sold, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\nThen d3tree() is used to build an interactive treemap.\n\n\nd3tree(tm,rootname = \"Singapore\" )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09E/Hands-on_Ex09E.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex09E/Hands-on_Ex09E.html#overview",
    "title": "Hands-on Exercise 09E",
    "section": "",
    "text": "In this hands-on exercise, you will gain hands-on experiences on designing treemap using appropriate R packages. The hands-on exercise consists of three main section. First, you will learn how to manipulate transaction data into a treemap strcuture by using selected functions provided in dplyr package. Then, you will learn how to plot static treemap by using treemap package. In the third section, you will learn how to design interactive treemap by using d3treeR package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09E/Hands-on_Ex09E.html#installing-and-launching-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex09E/Hands-on_Ex09E.html#installing-and-launching-r-packages",
    "title": "Hands-on Exercise 09E",
    "section": "",
    "text": "Before we get started, you are required to check if treemap and tidyverse pacakges have been installed in you R.\n\npacman::p_load(treemap, treemapify, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09E/Hands-on_Ex09E.html#data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex09E/Hands-on_Ex09E.html#data-wrangling",
    "title": "Hands-on Exercise 09E",
    "section": "",
    "text": "In this exercise, REALIS2018.csv data will be used. This dataset provides information of private property transaction records in 2018. The dataset is extracted from REALIS portal (https://spring.ura.gov.sg/lad/ore/login/index.cfm) of Urban Redevelopment Authority (URA).\n\n\nIn the code chunk below, read_csv() of readr is used to import realis2018.csv into R and parsed it into tibble R data.frame format.\n\nrealis2018 &lt;- read_csv(\"data/realis2018.csv\")\n\nThe output tibble data.frame is called realis2018.\n\n\n\nThe data.frame realis2018 is in trasaction record form, which is highly disaggregated and not appropriate to be used to plot a treemap. In this section, we will perform the following steps to manipulate and prepare a data.frtame that is appropriate for treemap visualisation:\n\ngroup transaction records by Project Name, Planning Region, Planning Area, Property Type and Type of Sale, and\ncompute Total Unit Sold, Total Area, Median Unit Price and Median Transacted Price by applying appropriate summary statistics on No. of Units, Area (sqm), Unit Price ($ psm) and Transacted Price ($) respectively.\n\nTwo key verbs of dplyr package, namely: group_by() and summarize() will be used to perform these steps.\ngroup_by() breaks down a data.frame into specified groups of rows. When you then apply the verbs above on the resulting object they’ll be automatically applied “by group”.\nGrouping affects the verbs as follows:\n\ngrouped select() is the same as ungrouped select(), except that grouping variables are always retained.\ngrouped arrange() is the same as ungrouped; unless you set .by_group = TRUE, in which case it orders first by the grouping variables.\nmutate() and filter() are most useful in conjunction with window functions (like rank(), or min(x) == x). They are described in detail in vignette(“window-functions”).\nsample_n() and sample_frac() sample the specified number/fraction of rows in each group.\nsummarise() computes the summary for each group.\n\nIn our case, group_by() will used together with summarise() to derive the summarised data.frame.\n\n\n\n\n\n\nRecommendation:\n\n\n\nIf you are new to dplyr methods should consult Introduction to dplyr before moving on to the next section.\n\n\n\n\n\nThe code chank below shows a typical two lines code approach to perform the steps.\n\nrealis2018_grouped &lt;- group_by(realis2018, `Project Name`,\n                               `Planning Region`, `Planning Area`, \n                               `Property Type`, `Type of Sale`)\nrealis2018_summarised &lt;- summarise(realis2018_grouped, \n                          `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n                          `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n                          `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n                          `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n\n\n\n\n\n\nNote:\n\n\n\nAggregation functions such as sum() and meadian() obey the usual rule of missing values: if there’s any missing value in the input, the output will be a missing value. The argument na.rm = TRUE removes the missing values prior to computation.\n\n\nThe code chunk above is not very efficient because we have to give each intermediate data.frame a name, even though we don’t have to care about it.\n\n\n\nThe code chunk below shows a more efficient way to tackle the same processes by using the pipe, %&gt;%:\n\n\n\n\n\n\nRecommendation:\n\n\n\nTo learn more about pipe, visit this excellent article: Pipes in R Tutorial For Beginners.\n\n\n\nrealis2018_summarised &lt;- realis2018 %&gt;% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %&gt;%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09E/Hands-on_Ex09E.html#designing-treemap-with-treemap-package",
    "href": "Hands-on_Ex/Hands-on_Ex09E/Hands-on_Ex09E.html#designing-treemap-with-treemap-package",
    "title": "Hands-on Exercise 09E",
    "section": "",
    "text": "treemap package is a R package specially designed to offer great flexibility in drawing treemaps. The core function, namely: treemap() offers at least 43 arguments. In this section, we will only explore the major arguments for designing elegent and yet truthful treemaps.\n\n\nIn this section, treemap() of Treemap package is used to plot a treemap showing the distribution of median unit prices and total unit sold of resale condominium by geographic hierarchy in 2017.\nFirst, we will select records belongs to resale condominium property type from realis2018_selected data frame.\n\nrealis2018_selected &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\n\n\n\nThe code chunk below designed a treemap by using three core arguments of treemap(), namely: index, vSize and vColor.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThings to learn from the three arguments used:\n\nindex\n\nThe index vector must consist of at least two column names or else no hierarchy treemap will be plotted.\nIf multiple column names are provided, such as the code chunk above, the first name is the highest aggregation level, the second name the second highest aggregation level, and so on.\n\nvSize\n\nThe column must not contain negative values. This is because it’s vaues will be used to map the sizes of the rectangles of the treemaps.\n\n\nWarning:\nThe treemap above was wrongly coloured. For a correctly designed treemap, the colours of the rectagles should be in different intensity showing, in our case, median unit prices.\nFor treemap(), vColor is used in combination with the argument type to determines the colours of the rectangles. Without defining type, like the code chunk above, treemap() assumes type = index, in our case, the hierarchy of planning areas.\n\n\n\nIn the code chunk below, type argument is define as value.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThinking to learn from the conde chunk above.\n\nThe rectangles are coloured with different intensity of green, reflecting their respective median unit prices.\nThe legend reveals that the values are binned into ten bins, i.e. 0-5000, 5000-10000, etc. with an equal interval of 5000.\n\n\n\n\nThere are two arguments that determine the mapping to color palettes: mapping and palette. The only difference between “value” and “manual” is the default value for mapping. The “value” treemap considers palette to be a diverging color palette (say ColorBrewer’s “RdYlBu”), and maps it in such a way that 0 corresponds to the middle color (typically white or yellow), -max(abs(values)) to the left-end color, and max(abs(values)), to the right-end color. The “manual” treemap simply maps min(values) to the left-end color, max(values) to the right-end color, and mean(range(values)) to the middle color.\n\n\n\nThe code chunk below shows a value type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above:\n\nalthough the colour palette used is RdYlBu but there are no red rectangles in the treemap above. This is because all the median unit prices are positive.\nThe reason why we see only 5000 to 45000 in the legend is because the range argument is by default c(min(values, max(values)) with some pretty rounding.\n\n\n\n\nThe “manual” type does not interpret the values as the “value” type does. Instead, the value range is mapped linearly to the colour palette.\nThe code chunk below shows a manual type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\nThe colour scheme used is very confusing. This is because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative\n\nTo overcome this problem, a single colour palette such as Blues should be used.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\ntreemap() supports two popular treemap layouts, namely: “squarified” and “pivotSize”. The default is “pivotSize”.\nThe squarified treemap algorithm (Bruls et al., 2000) produces good aspect ratios, but ignores the sorting order of the rectangles (sortID). The ordered treemap, pivot-by-size, algorithm (Bederson et al., 2002) takes the sorting order (sortID) into account while aspect ratios are still acceptable.\n\n\n\nThe code chunk below plots a squarified treemap by changing the algorithm argument.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\nWhen “pivotSize” algorithm is used, sortID argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09E/Hands-on_Ex09E.html#designing-treemap-using-treemapify-package",
    "href": "Hands-on_Ex/Hands-on_Ex09E/Hands-on_Ex09E.html#designing-treemap-using-treemapify-package",
    "title": "Hands-on Exercise 09E",
    "section": "",
    "text": "treemapify is a R package specially developed to draw treemaps in ggplot2. In this section, you will learn how to designing treemps closely resemble treemaps designing in previous section by using treemapify. Before you getting started, you should read Introduction to “treemapify” its user guide.\n\n\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\nGroup by Planning Region\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\n\n\n\n\nGroup by Planning Area\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\n\n\n\n\n\n\nAdding boundary line\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\",\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09E/Hands-on_Ex09E.html#designing-interactive-treemap-using-d3treer",
    "href": "Hands-on_Ex/Hands-on_Ex09E/Hands-on_Ex09E.html#designing-interactive-treemap-using-d3treer",
    "title": "Hands-on Exercise 09E",
    "section": "",
    "text": "This slide shows you how to install a R package which is not available in cran.\n\nIf this is the first time you install a package from github, you should install devtools package by using the code below or else you can skip this step.\n\n\ninstall.packages(\"devtools\", repos = \"https://cloud.r-project.org\")\n\npackage 'devtools' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\ASUS\\AppData\\Local\\Temp\\RtmpCo0f42\\downloaded_packages\n\n\n\nNext, you will load the devtools library and install the package found in github by using the codes below.\n\n\nlibrary(devtools)\ninstall_github(\"timelyportfolio/d3treeR\")\n\n\nNow you are ready to launch d3treeR package\n\n\nlibrary(d3treeR)\n\n\n\n\nThe codes below perform two processes.\n\ntreemap() is used to build a treemap by using selected variables in condominium data.frame. The treemap created is save as object called tm.\n\n\ntm &lt;- treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Planning Area\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        title=\"Private Residential Property Sold, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\nThen d3tree() is used to build an interactive treemap.\n\n\nd3tree(tm,rootname = \"Singapore\" )"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#storyboard-design",
    "href": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#storyboard-design",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "This storyboard translates our investigative goals into four interactive visual modules. Each module supports a key research question from the VAST Challenge MC2 and is designed for eventual implementation in Shiny.\n\n\n\nObjective:\nGive users an overview of which COOTEFOO board members are covered by each dataset (FILAH, TROUT, journalist), and how positively or negatively they are portrayed.\nRationale:\nThis module addresses bias at a glance by highlighting sentiment polarity and dataset focus across all six COOTEFOO members. It serves as the entry point for deeper exploration.\nUI Design: - selectInput(\"selected_member\", ...) to choose a board member - plotOutput(\"sentiment_heatmap\") showing a heatmap of average sentiment by dataset - Highlight selected member on the heatmap - Optional checkboxes: dataset filter (FILAH, TROUT, journalist)\nExpected Insight:\nUsers will see how frequently each member is mentioned, and whether sentiment aligns across sources or diverges — surfacing possible bias.\n\n\n\n\nObjective:\nAllow users to analyze what each board member says or is said about, and whether the sentiment aligns with specific topics (e.g., tourism or fishing).\nRationale:\nTo assess potential favoritism, we need to examine the nature and tone of interactions linked to specific topics and plans, independent of geography.\nUI Design: - selectInput(\"member\", ...) auto-linked from Module 1 - plotOutput(\"sentiment_by_topic\"): violin or bar plot of sentiment by topic - facet_wrap(~dataset) for comparison across sources - Tooltip to show role (speaker, participant, etc.)\nExpected Insight:\nHelps answer whether a member expresses more support or criticism toward tourism or fishing initiatives.\n\n\n\n\nObjective:\nMap each member’s travel patterns and detect any regional concentration of activity or sentiment.\nRationale:\nSpatial bias can reveal operational preferences or strategic absences. This complements aspatial bias detection with location-based insight.\nUI Design: - leafletOutput(\"travel_map\"): travel lines and place markers - Zone overlay from oceanus_map.geojson - sliderInput(\"year_range\", ...) to filter by time - Heatmap option: average sentiment by region\nExpected Insight:\nDetect whether board members physically support certain zones or avoid contentious ones, and whether sentiment aligns with presence.\n\n\n\n\nObjective:\nShow when and how each board member participated in meetings, discussions, or trips over time.\nRationale:\nTemporal analysis supports detection of shifting biases, campaign timing, or reactions to events.\nUI Design: - plotOutput(\"activity_timeline\"): line or lollipop chart of involvement over time - Color-coded roles: Chair, Coordinator, Attendee - Toggle by plan type or topic\nExpected Insight:\nIdentify patterns in engagement intensity and shifts in behavior, possibly linked to external pressures or lobbying.\n\n\n\n\nEach module is connected via reactiveValues() in Shiny. When a user selects a board member in Module 1, all other modules update dynamically to reflect data relevant to that member."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#prototype-implementation-plan",
    "href": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#prototype-implementation-plan",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "This section outlines how the storyboard modules will be implemented in a Shiny-compatible Quarto prototype. The plan emphasizes open-ended interactivity, modular layout, and traceable logic across views.\n\n\n\nThe prototype is developed using Quarto (HTML format) with integrated Shiny runtime. It leverages the following R packages:\n\nshiny, shinydashboard: UI layout and reactivity\ntidyverse: Data manipulation\nggplot2, plotly: Visualization\nleaflet, sf, tmap: Spatial visualizations\nDT: Interactive tables\nvisNetwork: Ego-network for topic/entity links (optional)\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset\nPurpose\nFormat\n\n\n\n\nFILAH.json\nPro-fishing records\nJSON knowledge graph\n\n\nTROUT.json\nPro-tourism records\nJSON knowledge graph\n\n\njournalist.json\nIndependent journalistic data\nJSON knowledge graph\n\n\noceanus_map.geojson\nZoning map of Oceanus\nGeoJSON\n\n\nroad_map.json\nBoard member travel plans\nJSON\n\n\ndata_export.xlsx\nCleaned sentiment and edges\nExcel (used for heatmap)\n\n\n\nAll data has been pre-processed into sentiment summary tables, trip records, and structured node-edge frames as required by each module.\n\n\n\n\n\n\n\nModule\nBuild Order\nRationale\n\n\n\n\nModule 1\n✅ First\nEstablish entry point and member selection\n\n\nModule 2\nSecond\nReuses selection input from Module 1\n\n\nModule 3\nThird\nRequires geospatial setup, follows EDA\n\n\nModule 4\nLast\nTemporal data filtered by member/topic\n\n\n\nEach module is developed as a standalone tabPanel() or navPane() for flexible rendering in Shiny.\n\n\n\n\n\nMember-centric: All views revolve around COOTEFOO members as investigative targets\nInteractive: Selections update views via reactiveValues and observeEvent\nColor-coded bias: Consistent use of color to indicate source and sentiment polarity\n\n🟦 FILAH, 🟨 TROUT, 🟩 journalist\n\nResponsive layout: Designed for both desktop and tablet-width screens\nInterpretability: Tooltips and legends clarify role, topic, or sentiment meaning\n\n\n\n\n\nPrototype integration will follow this logic: 1. Member selection in Module 1 → updates filters across Modules 2–4 2. Filtered sentiment + topic → determines edge content in Module 2 3. Node-level plan and role filters → control Module 3 and 4 displays 4. All data pipelines use reactive() expressions for efficiency\nThis ensures seamless cross-module storytelling without manual refresh."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#storyboard-implementation-plan",
    "href": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#storyboard-implementation-plan",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "This section outlines the storyboard modules for our investigative prototype, along with how each will be implemented in Quarto and Shiny. The design follows the “overview first, zoom and filter, details on demand” approach from the visual analytics storyboard framework.\n\n\nObjective: Provide an overview of which COOTEFOO board members are discussed in each dataset, and how positively or negatively they are portrayed.\nUI Design: - selectInput(\"selected_member\", ...) to choose a COOTEFOO board member\n- checkboxGroupInput(\"source_filter\", ...) to toggle between FILAH 🟦, TROUT 🟨, and journalist 🟩\n- plotOutput(\"sentiment_heatmap\"): heatmap of average sentiment per member by dataset\n- Tooltip and hover details for interpretation\nExpected Insight: Surface sentiment bias and dataset focus at a glance. Entry point for deeper exploration.\nBuild Priority: ✅ First – establishes member selection logic for all modules\n\n\n\n\nObjective: Examine whether board members favor or oppose specific topics (e.g., tourism, fishing), independent of geography.\nUI Design: - selectInput(\"member\", ...) auto-linked from Module 1\n- plotOutput(\"sentiment_by_topic\"): violin/bar plots of sentiment by topic\n- facet_wrap(~dataset) for side-by-side comparison\n- Tooltip shows role (speaker, participant, etc.)\nExpected Insight: Detect topic-linked favoritism or criticism patterns.\nBuild Priority: Second – extends selection logic from Module 1.\n\n\n\n\nObjective: Map each member’s travel behavior and regional focus.\nUI Design: - leafletOutput(\"travel_map\"): travel lines and markers\n- Oceanus zone overlay from oceanus_map.geojson\n- sliderInput(\"year_range\", ...) for temporal filtering\n- Optional heatmap toggle: average sentiment per region\nExpected Insight: Identify whether board members physically support or avoid certain regions and how presence correlates with sentiment.\nBuild Priority: Third – requires spatial processing setup.\n\n\n\n\nObjective: Visualize board members’ engagement over time, by topic and role.\nUI Design: - plotOutput(\"activity_timeline\"): lollipop or line chart of activity\n- Role-based color codes: Chair, Coordinator, Attendee\n- Filter by plan type or topic\nExpected Insight: Track behavioral shifts, engagement timing, and reactions to key events.\nBuild Priority: Last – depends on cleaned temporal and role-tagged data.\n\n\n\n\nThe prototype is developed in Quarto (HTML format) with Shiny runtime for interactivity. Core packages include:\n\nUI & Reactivity: shiny, shinydashboard, reactiveValues, observeEvent\nData: tidyverse, jsonlite, lubridate\nVisualizations: ggplot2, plotly, leaflet, visNetwork, DT\nSpatial Handling: sf, tmap\n\nEach module will be implemented as a tabPanel() in a Shiny layout. Cross-module interactivity is powered by shared selection state via reactiveValues().\nDesign Principles: - Member-centric: COOTEFOO board members drive all filters\n- Interactive: Selections ripple across all views\n- Consistent color encoding: 🟦 FILAH, 🟨 TROUT, 🟩 journalist\n- Responsive layout: Usable on desktop and tablet\n- Interpretability: Tooltips and legends throughout"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#data-preparation-and-exploratory-narratives",
    "href": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#data-preparation-and-exploratory-narratives",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "To support our storyboard modules, we begin with cleaning and exploring key aspects of the datasets. This includes understanding the structure, extracting relevant geospatial information, identifying role and plan type distributions, and performing basic text analysis.\n\n\nWe start by extracting geotagged nodes from each dataset (FILAH, TROUT, journalist). Only entries with valid latitude and longitude are included for mapping.\n\n# Define helper function to extract geotagged place nodes\nextract_places &lt;- function(data, source_name) {\n  data$nodes %&gt;%\n    filter(!is.na(lat) & !is.na(lon)) %&gt;%\n    mutate(source = source_name)\n}\n\n# Extract and combine geotagged nodes across all datasets\nplaces_filah &lt;- extract_places(filah, \"FILAH\")\nplaces_trout &lt;- extract_places(trout, \"TROUT\")\nplaces_journalist &lt;- extract_places(journalist, \"journalist\")\n\nall_places &lt;- bind_rows(places_filah, places_trout, places_journalist)\n\n# Extract all nodes (not just places with lat/lon)\nnodes_filah &lt;- filah$nodes %&gt;% mutate(source = \"FILAH\")\nnodes_trout &lt;- trout$nodes %&gt;% mutate(source = \"TROUT\")\nnodes_journalist &lt;- journalist$nodes %&gt;% mutate(source = \"journalist\")\n\n# Combine all nodes into a single dataframe\nall_nodes &lt;- bind_rows(nodes_filah, nodes_trout, nodes_journalist)\n\nWe briefly inspect the spatial distribution to ensure values fall within expected boundaries.\n\nsummary(all_places$lat)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -166.0  -165.7  -165.6  -165.2  -164.5  -164.3 \n\nsummary(all_places$lon)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  38.99   39.26   39.42   39.38   39.54   39.67 \n\n\nWe also check whether zone values align with the official zoning names provided in the Oceanus map.\n\nunique(all_nodes$zone)\n\n[1] NA            \"industrial\"  \"tourism\"     \"residential\" \"government\" \n[6] \"commercial\"  \"connector\"  \n\nunique(oceanus_map$Name)\n\n [1] \"Suna Island\"         \"Thalassa Retreat\"    \"Makara Shoal\"       \n [4] \"Silent Sanctuary\"    \"Cod Table\"           \"Ghoti Preserve\"     \n [7] \"Wrasse Beds\"         \"Nemo Reef\"           \"Don Limpet Preserve\"\n[10] \"Tuna Shelf\"          \"Haacklee\"            \"Port Grove\"         \n[13] \"Lomark\"              \"Himark\"              \"Paackland\"          \n[16] \"Centralia\"           \"South Paackland\"     \"Exit West\"          \n[19] \"Nav 3\"               \"Nav D\"               \"Nav B\"              \n[22] \"Nav A\"               \"Nav C\"               \"Nav 2\"              \n[25] \"Nav 1\"               \"Exit East\"           \"Exit South\"         \n[28] \"Exit North\"          \"Nav E\"              \n\n\n\n\n\nTo better understand spatial focus across the datasets, we first count how often each zone is mentioned:\n\nall_nodes %&gt;%\n  filter(!is.na(zone)) %&gt;%\n  count(zone, name = \"place_count\") %&gt;%\n  ggplot(aes(x = fct_reorder(zone, place_count), y = place_count, fill = zone)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  labs(title = \"Place Mentions by Zone Type\",\n       x = \"Zone Type\", y = \"Count\")\n\n\n\n\n\n\n\n\n\n\n\nWe then compare how often each dataset (FILAH, TROUT, journalist) mentions different zones.\n\n# Stacked bar chart: mentions by zone type and dataset source\nall_nodes %&gt;%\n  filter(!is.na(zone)) %&gt;%\n  count(source, zone) %&gt;%\n  ggplot(aes(x = fct_reorder(zone, n, sum), y = n, fill = source)) +\n  geom_col(position = \"stack\") +\n  coord_flip() +\n  labs(title = \"Zone Mentions by Source\",\n       x = \"Zone Type\", y = \"Mentions\", fill = \"Source\")\n\n\n\n\n\n\n\n\n\n\n\nWe examine the distribution of plan types proposed by each source.\n\nall_nodes %&gt;%\n  filter(!is.na(plan_type)) %&gt;%\n  count(source, plan_type) %&gt;%\n  ggplot(aes(x = fct_reorder(plan_type, n, sum), y = n, fill = source)) +\n  geom_col(position = \"stack\") +\n  coord_flip() +\n  labs(title = \"Plan Type Mentions by Source\",\n       x = \"Plan Type\", y = \"Mentions\")\n\n\n\n\n\n\n\n\n\n\n\nWe prepare and tokenize text fields to uncover common language used across sources.\n\nlibrary(tidytext)\nlibrary(stringr)\n\n# Clean plan_type formatting for consistency\nall_nodes_clean &lt;- all_nodes %&gt;%\n  mutate(plan_type = str_to_title(plan_type))\n\n# Combine text fields into one string\nall_text &lt;- all_nodes_clean %&gt;%\n  mutate(text = paste(label, short_title, long_title, short_topic, long_topic, sep = \" \")) %&gt;%\n  select(source, text) %&gt;%\n  filter(!is.na(text))\n\n# Tokenize and remove common stop words and numbers\ntokens &lt;- all_text %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  anti_join(stop_words, by = \"word\") %&gt;%\n  filter(!str_detect(word, \"^\\\\d+$\"))\n\n# Visualize most frequent words by source\ntokens %&gt;%\n  filter(word != \"na\") %&gt;%\n  count(source, word, sort = TRUE) %&gt;%\n  group_by(source) %&gt;%\n  slice_max(n, n = 10) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(x = reorder_within(word, n, source), y = n, fill = source)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~source, scales = \"free_y\") +\n  scale_x_reordered() +\n  coord_flip() +\n  labs(title = \"Top Words in Meeting Records by Source\",\n       x = \"Word\", y = \"Frequency\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#storyboard-ui-design",
    "href": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#storyboard-ui-design",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "We propose 4 interactive modules for the Shiny prototype. Each aligns with the “overview first, zoom and filter, details on demand” framework.\n\n\n\nGoal: Identify which board members are discussed and their associated sentiment\nUI: Member selector, dataset filter, sentiment heatmap\nInsight: Identify prominent members and potential bias\n\n\n\n\n\nGoal: Explore whether members show preference for certain topics\nUI: Violin/bar chart by topic, faceted by source, tooltip shows roles\nInsight: Understand favor/disfavor patterns without geospatial bias\n\n\n\n\n\nGoal: Map where board members have been active\nUI: Interactive leaflet map, zone overlay, slider by year\nInsight: Highlight travel patterns and engagement across regions\n\n\n\n\n\nGoal: Trace member involvement over time by role\nUI: Timeline chart (lollipop or line), color by role, filters by topic/plan\nInsight: Show changes in influence or engagement over time"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#technical-integration",
    "href": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#technical-integration",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "The Shiny app will use reactiveValues() to coordinate filters and selections across modules. Each module will live in a tabPanel(), using these packages:\n\nshiny, shinydashboard for layout and interactivity\nggplot2, plotly, leaflet, visNetwork for visualization\nDT for searchable data tables\nsf, tmap for geospatial overlays\n\nPrinciples:\n\nMember-centric filtering across views\nColor consistency: 🔵 FILAH, 🔹 TROUT, 🟩 journalist\nTooltip support and accessible design for all plots\nShiny-ready modular construction from this Quarto base"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#overview",
    "href": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#overview",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "This prototype continues from Take-home Exercise 2, based on Mini-Challenge 2 (MC2) from the VAST Challenge 2025, which investigates claims of bias within the fictional government committee COOTEFOO, amid tensions between economic development and cultural preservation in Oceanus.\nOceanus faces competing visions from:\n\nFishing is Living and Heritage (FILAH) – preserving traditional fishing zones.\nTourism Raises OceanUs Together (TROUT) – promoting tourism-led growth.\n\nBoth groups have accused COOTEFOO of favoring the other. To investigate, we analyze:\n\n🟦 FILAH.json – records submitted by the pro-fishing group\n🟨 TROUT.json – records from the pro-tourism group\n🟩 journalist.json – independent reports to provide a neutral perspective\n🗺️ oceanus_map.geojson – official zoning map of Oceanus\n🛣️ road_map.json – road network infrastructure\n\n📝 All datasets were provided through the official VAST Challenge MC2 instruction page.\nWe aim to build a modular Shiny app prototype that allows stakeholders to:\n1. View which COOTEFOO members are most frequently discussed\n2. Analyze the topics and sentiment associated with each\n3. Compare bias across datasets\n4. Explore geographic movement patterns\nThis report outlines our Quarto-based storyboard prototype, which will be integrated into a future Shiny app."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#proposed-modules-and-technical-integration",
    "href": "Take-home_Ex/Take-home_Ex_3/Take-home_Ex_3.html#proposed-modules-and-technical-integration",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "To address the investigative goals in our project proposal, we present five interactive modules in a Shiny dashboard. These modules follow Shneiderman’s mantra: “overview first, zoom and filter, then details on demand” and are structured using a consistent member-centric interaction model.\nEach module is built with Shiny-ready functions and coordinated using reactiveValues() to support cross-filtering, seamless user experience, and visual storytelling.\n\n\n\nGoal: Help users understand the structure, scale, and characteristics of the datasets before deeper analysis\nUI Components:\n\n-    Summary tables (e.g., total nodes, node types, source counts) using `DT`\n\n-    Bar chart: Frequency of zone types mentioned\n\n-    Violin or bar plots: Distribution of plan types by source\n\n-    Top word frequencies across datasets (via `tidytext`)\nInsight: Offers transparency and builds user confidence by surfacing base-level descriptive insights.\n\n\n\n\n\nGoal: Reveal structural relationships between COOTEFOO members, their discussion topics, and proposed plans\nUI Components:\n\n-   Interactive `visNetwork` graph\n\n-    Member selector (ego filter)\n\n-    Dataset toggle: FILAH, TROUT, journalist\n\n-    Tooltip: Node type, role, or plan type\n\nInsight: Visualizes influence patterns, overlapping interest areas, and collaborative networks across stakeholders.\n\n\n\n\n\n\nGoal: Understand where members are engaging geographically through travel and planning actions\nUI Components:\n\n-    Interactive `leaflet` map overlaid with Oceanus zones\n\n-   Year/meeting slider\n\nMarker toggles for travel nodes and planned site visits\nInsight: Maps the physical footprint of member activity, highlighting areas of concentrated or neglected engagement.\n\n\n\n\n\n\nGoal: Uncover sentiment trends across members, grouped by topic or industry focus\nUI Components:\n\n-    Violin/bar plots faceted by source\n\n-    Filters: Member, topic, plan type\n\n-    Tooltip: Sentiment score, role, context\nInsight: Highlights potential pro/anti-industry bias and comparative sentiment levels between advocacy groups.\n\n\n\n\n\nGoal: Track member participation and influence over time in a chronological view\nUI Components:\n\n-    Lollipop or line chart by meeting index or date\n\n-    Color-coded by role (e.g., Chair, Member)\n\n-    Filters: Topic, source, plan type\n\n-    Hover: Meeting label, topic discussed, plan proposed\n\nInsight: Shows evolution in member engagement, surfacing key turning points or role transitions over the course of discussions.\n\n\n\n\nAll modules are developed using modular R functions inside a Quarto document and are structured for direct transition to Shiny. Core design considerations include:\n\nFramework: tabsetPanel() with each module in its own tabPanel()\nShared State: Member/topic/source filters coordinated via reactiveValues()\nKey Packages:\n\n-    `shiny`, `shinydashboard`: layout & interaction\n\n-    `ggplot2`, `plotly`: dynamic plot\n\n-    `leaflet`, `sf`, `tmap`: spatial mapping\n\n-    `visNetwork`: network graphs\n\n-    `DT`: tabular summaries\n\n-    `tidytext`: text tokenization and frequency analysis"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html",
    "title": "Hands-on Exercise 10",
    "section": "",
    "text": "By the end of this hands-on exercise, you will be able to:\n\ncreate bullet chart by using ggplot2,\ncreate sparklines by using ggplot2 ,\nbuild industry standard dashboard by using R Shiny.\n\n\n\n\nFor the purpose of this hands-on exercise, the following R packages will be used.\n\n\nShow the code\n\n\npacman::p_load(lubridate, ggthemes, reactable,\nreactablefmtr, gt, gtExtras, tidyverse, svglite)\n\n\n\ntidyverse provides a collection of functions for performing data science task such as importing, tidying, wrangling data and visualising data. It is not a single package but a collection of modern R packages including but not limited to readr, tidyr, dplyr, ggplot, tibble, stringr, forcats and purrr.\nlubridate provides functions to work with dates and times more efficiently.\nggthemes is an extension of ggplot2. It provides additional themes beyond the basic themes of ggplot2.\ngtExtras provides some additional helper functions to assist in creating beautiful tables with gt, an R package specially designed for anyone to make wonderful-looking tables using the R programming language.\nreactable provides functions to create interactive data tables for R, based on the React Table library and made with reactR.\nreactablefmtr provides various features to streamline and enhance the styling of interactive reactable tables with easy-to-use and highly-customizable functions and themes.\n\n\n\n\n\n\nFor the purpose of this study, a personal database in Microsoft Access mdb format called Coffee Chain will be used.\n\n\n\nIn the code chunk below, odbcConnectAccess() of RODBC package is used used to import a database query table into R.\n\n\nShow the code\n\n\nlibrary(RODBC)\ncon &lt;- odbcConnectAccess2007('data/Coffee Chain.mdb')\ncoffeechain &lt;- sqlFetch(con, 'CoffeeChain Query')\nwrite_rds(coffeechain, \"data/CoffeeChain.rds\")\nodbcClose(con)\n\n\nNote: Before running the code chunk, you need to change the R system to 32bit version. This is because the odbcConnectAccess() is based on 32bit and not 64bit\n\n\n\nThe code chunk below is used to import CoffeeChain.rds into R.\n\n\nShow the code\n\n\ncoffeechain &lt;- read_rds(\"data/CoffeeChain.rds\")\n\n\nNote: This step is optional if coffeechain is already available in R.\nThe code chunk below is used to aggregate Sales and Budgeted Sales at the Product level.\n\n\nShow the code\n\n\nproduct &lt;- coffeechain %&gt;%\n  group_by(`Product`) %&gt;%\n  summarise(`target` = sum(`Budget Sales`),\n            `current` = sum(`Sales`)) %&gt;%\n  ungroup()\n\n\n\n\n\nThe code chunk below is used to plot the bullet charts using ggplot2 functions.\n\n\nShow the code\n\n\nggplot(product, aes(Product, current)) + \n  geom_col(aes(Product, max(target) * 1.01),\n           fill=\"grey85\", width=0.85) +\n  geom_col(aes(Product, target * 0.75),\n           fill=\"grey60\", width=0.85) +\n  geom_col(aes(Product, target * 0.5),\n           fill=\"grey50\", width=0.85) +\n  geom_col(aes(Product, current), \n           width=0.35,\n           fill = \"black\") + \n  geom_errorbar(aes(y = target,\n                    x = Product, \n                    ymin = target,\n                    ymax= target), \n                width = .4,\n                colour = \"red\",\n                size = 1) +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, you will learn how to plot sparklines by using ggplot2.\n\n\n\n\nShow the code\n\n\nsales_report &lt;- coffeechain %&gt;%\n  filter(Date &gt;= \"2013-01-01\") %&gt;%\n  mutate(Month = month(Date)) %&gt;%\n  group_by(Month, Product) %&gt;%\n  summarise(Sales = sum(Sales)) %&gt;%\n  ungroup() %&gt;%\n  select(Month, Product, Sales)\n\n\nThe code chunk below is used to compute the minimum, maximum and end othe the month sales.\n\n\nShow the code\n\n\nmins &lt;- group_by(sales_report, Product) %&gt;% \n  slice(which.min(Sales))\nmaxs &lt;- group_by(sales_report, Product) %&gt;% \n  slice(which.max(Sales))\nends &lt;- group_by(sales_report, Product) %&gt;% \n  filter(Month == max(Month))\n\n\nThe code chunk below is used to compute the 25 and 75 quantiles.\n\n\nShow the code\n\n\nquarts &lt;- sales_report %&gt;%\n  group_by(Product) %&gt;%\n  summarise(quart1 = quantile(Sales, \n                              0.25),\n            quart2 = quantile(Sales, \n                              0.75)) %&gt;%\n  right_join(sales_report)\n\n\n\n\n\nThe code chunk used.\n\n\nShow the Code\n\n\nggplot(sales_report, aes(x=Month, y=Sales)) + \n  facet_grid(Product ~ ., scales = \"free_y\") + \n  geom_ribbon(data = quarts, aes(ymin = quart1, max = quart2), \n              fill = 'grey90') +\n  geom_line(size=0.3) +\n  geom_point(data = mins, col = 'red') +\n  geom_point(data = maxs, col = 'blue') +\n  geom_text(data = mins, aes(label = Sales), vjust = -1) +\n  geom_text(data = maxs, aes(label = Sales), vjust = 2.5) +\n  geom_text(data = ends, aes(label = Sales), hjust = 0, nudge_x = 0.5) +\n  geom_text(data = ends, aes(label = Product), hjust = 0, nudge_x = 1.0) +\n  expand_limits(x = max(sales_report$Month) + \n                  (0.25 * (max(sales_report$Month) - min(sales_report$Month)))) +\n  scale_x_continuous(breaks = seq(1, 12, 1)) +\n  scale_y_continuous(expand = c(0.1, 0)) +\n  theme_tufte(base_size = 3, base_family = \"Helvetica\") +\n  theme(axis.title=element_blank(), axis.text.y = element_blank(), \n        axis.ticks = element_blank(), strip.text = element_blank())\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, you will learn how to create static information dashboard by using gt and gtExtras packages. Before getting started, it is highly recommended for you to visit the webpage of these two packages and review all the materials provided on the webpages at least once. You done not have to understand and remember everything provided but at least have an overview of the purposes and functions provided by them.\n\n\nIn this section, you will learn how to prepare a bullet chart report by using functions of gt and gtExtras packages.\n\n\nShow the Code\n\n\nlibrary(gt)\nlibrary(gtExtras)\n\nproduct %&gt;%\n  gt::gt() %&gt;%\n  gt_plt_bullet(column = current, \n              target = target, \n              width = 60,\n              palette = c(\"lightblue\", \n                          \"black\")) %&gt;%\n  gt_theme_538()\n\n\n\n\n\n\n\n\n\n\n\nProduct\ncurrent\n\n\n\n\nAmaretto\n\n\n\n   \n\n\n\nCaffe Latte\n\n\n\n   \n\n\n\nCaffe Mocha\n\n\n\n   \n\n\n\nChamomile\n\n\n\n   \n\n\n\nColombian\n\n\n\n   \n\n\n\nDarjeeling\n\n\n\n   \n\n\n\nDecaf Espresso\n\n\n\n   \n\n\n\nDecaf Irish Cream\n\n\n\n   \n\n\n\nEarl Grey\n\n\n\n   \n\n\n\nGreen Tea\n\n\n\n   \n\n\n\nLemon\n\n\n\n   \n\n\n\nMint\n\n\n\n   \n\n\n\nRegular Espresso\n\n\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\nBefore we can prepare the sales report by product by using gtExtras functions, code chunk below will be used to prepare the data.\n\n\nShow the Code\n\n\nreport &lt;- coffeechain %&gt;%\n  mutate(Year = year(Date)) %&gt;%\n  filter(Year == \"2013\") %&gt;%\n  mutate (Month = month(Date, \n                        label = TRUE, \n                        abbr = TRUE)) %&gt;%\n  group_by(Product, Month) %&gt;%\n  summarise(Sales = sum(Sales)) %&gt;%\n  ungroup()\n\n\nIt is important to note that one of the requirement of gtExtras functions is that almost exclusively they require you to pass data.frame with list columns. In view of this, code chunk below will be used to convert the report data.frame into list columns.\n\n\nshow the Code\n\n\nreport %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\")\n\n# A tibble: 13 × 2\n   Product           `Monthly Sales`\n   &lt;chr&gt;             &lt;list&gt;         \n 1 Amaretto          &lt;dbl [12]&gt;     \n 2 Caffe Latte       &lt;dbl [12]&gt;     \n 3 Caffe Mocha       &lt;dbl [12]&gt;     \n 4 Chamomile         &lt;dbl [12]&gt;     \n 5 Colombian         &lt;dbl [12]&gt;     \n 6 Darjeeling        &lt;dbl [12]&gt;     \n 7 Decaf Espresso    &lt;dbl [12]&gt;     \n 8 Decaf Irish Cream &lt;dbl [12]&gt;     \n 9 Earl Grey         &lt;dbl [12]&gt;     \n10 Green Tea         &lt;dbl [12]&gt;     \n11 Lemon             &lt;dbl [12]&gt;     \n12 Mint              &lt;dbl [12]&gt;     \n13 Regular Espresso  &lt;dbl [12]&gt;     \n\n\n\n\n\n\n\nShow the Code\n\n\nreport %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\") %&gt;%\n   gt() %&gt;%\n   gt_plt_sparkline('Monthly Sales',\n                    same_limit = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nProduct\nMonthly Sales\n\n\n\n\nAmaretto\n\n\n\n   1.2K\n\n\n\nCaffe Latte\n\n\n\n   1.5K\n\n\n\nCaffe Mocha\n\n\n\n   3.7K\n\n\n\nChamomile\n\n\n\n   3.3K\n\n\n\nColombian\n\n\n\n   5.5K\n\n\n\nDarjeeling\n\n\n\n   3.0K\n\n\n\nDecaf Espresso\n\n\n\n   3.2K\n\n\n\nDecaf Irish Cream\n\n\n\n   2.7K\n\n\n\nEarl Grey\n\n\n\n   3.0K\n\n\n\nGreen Tea\n\n\n\n   1.5K\n\n\n\nLemon\n\n\n\n   4.4K\n\n\n\nMint\n\n\n\n   1.5K\n\n\n\nRegular Espresso\n\n\n\n   1.1K\n\n\n\n\n\n\n\n\n\n\n\n\nFirst, calculate summary statistics by using the code chunk below.\n\n\nShow the Code\n\n\nreport %&gt;% \n  group_by(Product) %&gt;% \n  summarise(\"Min\" = min(Sales, na.rm = T),\n            \"Max\" = max(Sales, na.rm = T),\n            \"Average\" = mean(Sales, na.rm = T)\n            ) %&gt;%\n  gt() %&gt;%\n  fmt_number(columns = 4,\n    decimals = 2)\n\n\n\n\n\n\n\nProduct\nMin\nMax\nAverage\n\n\n\n\nAmaretto\n1016\n1210\n1,119.00\n\n\nCaffe Latte\n1398\n1653\n1,528.33\n\n\nCaffe Mocha\n3322\n3828\n3,613.92\n\n\nChamomile\n2967\n3395\n3,217.42\n\n\nColombian\n5132\n5961\n5,457.25\n\n\nDarjeeling\n2926\n3281\n3,112.67\n\n\nDecaf Espresso\n3181\n3493\n3,326.83\n\n\nDecaf Irish Cream\n2463\n2901\n2,648.25\n\n\nEarl Grey\n2730\n3005\n2,841.83\n\n\nGreen Tea\n1339\n1476\n1,398.75\n\n\nLemon\n3851\n4418\n4,080.83\n\n\nMint\n1388\n1669\n1,519.17\n\n\nRegular Espresso\n890\n1218\n1,023.42\n\n\n\n\n\n\n\n\n\n\n\nNext, use the code chunk below to add the statistics on the table.\n\n\nShow the Code\n\n\nspark &lt;- report %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\")\n\n\n\n\nShow the Code\n\n\nsales &lt;- report %&gt;% \n  group_by(Product) %&gt;% \n  summarise(\"Min\" = min(Sales, na.rm = T),\n            \"Max\" = max(Sales, na.rm = T),\n            \"Average\" = mean(Sales, na.rm = T)\n            )\n\n\n\n\nShow the Code\n\n\nsales_data = left_join(sales, spark)\n\n\n\n\n\n\n\nShow the Code\n\n\nsales_data %&gt;%\n  gt() %&gt;%\n  gt_plt_sparkline('Monthly Sales',\n                   same_limit = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProduct\nMin\nMax\nAverage\nMonthly Sales\n\n\n\n\nAmaretto\n1016\n1210\n1119.000\n\n\n\n   1.2K\n\n\n\nCaffe Latte\n1398\n1653\n1528.333\n\n\n\n   1.5K\n\n\n\nCaffe Mocha\n3322\n3828\n3613.917\n\n\n\n   3.7K\n\n\n\nChamomile\n2967\n3395\n3217.417\n\n\n\n   3.3K\n\n\n\nColombian\n5132\n5961\n5457.250\n\n\n\n   5.5K\n\n\n\nDarjeeling\n2926\n3281\n3112.667\n\n\n\n   3.0K\n\n\n\nDecaf Espresso\n3181\n3493\n3326.833\n\n\n\n   3.2K\n\n\n\nDecaf Irish Cream\n2463\n2901\n2648.250\n\n\n\n   2.7K\n\n\n\nEarl Grey\n2730\n3005\n2841.833\n\n\n\n   3.0K\n\n\n\nGreen Tea\n1339\n1476\n1398.750\n\n\n\n   1.5K\n\n\n\nLemon\n3851\n4418\n4080.833\n\n\n\n   4.4K\n\n\n\nMint\n1388\n1669\n1519.167\n\n\n\n   1.5K\n\n\n\nRegular Espresso\n890\n1218\n1023.417\n\n\n\n   1.1K\n\n\n\n\n\n\n\n\n\n\n\n\nSimilarly, we can combining the bullet chart and sparklines using the steps below.\n\n\nShow the Code\n\n\nbullet &lt;- coffeechain %&gt;%\n  filter(Date &gt;= \"2013-01-01\") %&gt;%\n  group_by(Product) %&gt;%\n  summarise(Target = sum(`Budget Sales`),\n            Actual = sum(Sales)) %&gt;%\n  ungroup()\n\n\n\n\nShow the Code\n\n\nsales_data = sales_data %&gt;%\n  left_join(bullet)\n\n\n\n\nShow the Code\n\n\nsales_data %&gt;%\n  gt() %&gt;%\n  gt_plt_sparkline('Monthly Sales') %&gt;%\n  gt_plt_bullet(column = Actual, \n                target = Target, \n                width = 28,\n                palette = c(\"lightblue\", \n                          \"black\")) %&gt;%\n  gt_theme_538()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProduct\nMin\nMax\nAverage\nMonthly Sales\nActual\n\n\n\n\nAmaretto\n1016\n1210\n1119.000\n\n\n\n   1.2K\n\n\n\n\n   \n\n\n\nCaffe Latte\n1398\n1653\n1528.333\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nCaffe Mocha\n3322\n3828\n3613.917\n\n\n\n   3.7K\n\n\n\n\n   \n\n\n\nChamomile\n2967\n3395\n3217.417\n\n\n\n   3.3K\n\n\n\n\n   \n\n\n\nColombian\n5132\n5961\n5457.250\n\n\n\n   5.5K\n\n\n\n\n   \n\n\n\nDarjeeling\n2926\n3281\n3112.667\n\n\n\n   3.0K\n\n\n\n\n   \n\n\n\nDecaf Espresso\n3181\n3493\n3326.833\n\n\n\n   3.2K\n\n\n\n\n   \n\n\n\nDecaf Irish Cream\n2463\n2901\n2648.250\n\n\n\n   2.7K\n\n\n\n\n   \n\n\n\nEarl Grey\n2730\n3005\n2841.833\n\n\n\n   3.0K\n\n\n\n\n   \n\n\n\nGreen Tea\n1339\n1476\n1398.750\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nLemon\n3851\n4418\n4080.833\n\n\n\n   4.4K\n\n\n\n\n   \n\n\n\nMint\n1388\n1669\n1519.167\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nRegular Espresso\n890\n1218\n1023.417\n\n\n\n   1.1K\n\n\n\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, you will learn how to create interactive information dashboard by using reactable and reactablefmtr packages. Before getting started, it is highly recommended for you to visit the webpage of these two packages and review all the materials provided on the webpages at least once. You done not have to understand and remember everything provided but at least have an overview of the purposes and functions provided by them.\nIn order to build an interactive sparklines, we need to install dataui R package by using the code chunk below.\n\n\nShow the Code\n\n\nremotes::install_github(\"timelyportfolio/dataui\")\n\n\nNext, you all need to load the package onto R environment by using the code chunk below.\n\n\nShow the Code\n\n\nlibrary(dataui)\n\n\n\n\nSimilar to gtExtras, to plot an interactive sparklines by using reactablefmtr package we need to prepare the list field by using the code chunk below.\n\n\nShow the Code\n\n\nreport &lt;- report %&gt;%\n  group_by(Product) %&gt;%\n  summarize(`Monthly Sales` = list(Sales))\n\n\nNext, react_sparkline will be to plot the sparklines as shown below.\n\n\nShow the Code\n\n\nreactable(\n  report,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(report)\n    )\n  )\n)\n\n\n\n\n\n\n\n\n\nBy default the pagesize is 10. In the code chunk below, arguments defaultPageSize is used to change the default setting.\n\n\nShow the Code\n\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(report)\n    )\n  )\n)\n\n\n\n\n\n\n\n\n\nIn the code chunk below highlight_points argument is used to show the minimum and maximum values points and label argument is used to label first and last values.\n\n\nShow the Code\n\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        labels = c(\"first\", \"last\")\n        )\n    )\n  )\n)\n\n\n\n\n\n\n\n\n\nIn the code chunk below statline argument is used to show the mean line.\n\n\nShow the Code\n\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        statline = \"mean\"\n        )\n    )\n  )\n)\n\n\n\n\n\n\n\n\n\nInstead adding reference line, bandline can be added by using the bandline argument.\n\n\nShow the Code\n\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        line_width = 1,\n        bandline = \"innerquartiles\",\n        bandline_color = \"green\"\n        )\n    )\n  )\n)\n\n\n\n\n\n\n\n\n\nInstead of displaying the values as sparklines, we can display them as sparkbars as shiwn below.\n\n\nShow the Code\n\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkbar(\n        report,\n        highlight_bars = highlight_bars(\n          min = \"red\", max = \"blue\"),\n        bandline = \"innerquartiles\",\n        statline = \"mean\")\n    )\n  )\n)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#overview",
    "title": "Hands-on Exercise 10",
    "section": "",
    "text": "By the end of this hands-on exercise, you will be able to:\n\ncreate bullet chart by using ggplot2,\ncreate sparklines by using ggplot2 ,\nbuild industry standard dashboard by using R Shiny."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#getting-started",
    "title": "Hands-on Exercise 10",
    "section": "",
    "text": "For the purpose of this hands-on exercise, the following R packages will be used.\n\n\nShow the code\n\n\npacman::p_load(lubridate, ggthemes, reactable,\nreactablefmtr, gt, gtExtras, tidyverse, svglite)\n\n\n\ntidyverse provides a collection of functions for performing data science task such as importing, tidying, wrangling data and visualising data. It is not a single package but a collection of modern R packages including but not limited to readr, tidyr, dplyr, ggplot, tibble, stringr, forcats and purrr.\nlubridate provides functions to work with dates and times more efficiently.\nggthemes is an extension of ggplot2. It provides additional themes beyond the basic themes of ggplot2.\ngtExtras provides some additional helper functions to assist in creating beautiful tables with gt, an R package specially designed for anyone to make wonderful-looking tables using the R programming language.\nreactable provides functions to create interactive data tables for R, based on the React Table library and made with reactR.\nreactablefmtr provides various features to streamline and enhance the styling of interactive reactable tables with easy-to-use and highly-customizable functions and themes."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#importing-microsoft-access-database",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#importing-microsoft-access-database",
    "title": "Hands-on Exercise 10",
    "section": "",
    "text": "For the purpose of this study, a personal database in Microsoft Access mdb format called Coffee Chain will be used.\n\n\n\nIn the code chunk below, odbcConnectAccess() of RODBC package is used used to import a database query table into R.\n\n\nShow the code\n\n\nlibrary(RODBC)\ncon &lt;- odbcConnectAccess2007('data/Coffee Chain.mdb')\ncoffeechain &lt;- sqlFetch(con, 'CoffeeChain Query')\nwrite_rds(coffeechain, \"data/CoffeeChain.rds\")\nodbcClose(con)\n\n\nNote: Before running the code chunk, you need to change the R system to 32bit version. This is because the odbcConnectAccess() is based on 32bit and not 64bit\n\n\n\nThe code chunk below is used to import CoffeeChain.rds into R.\n\n\nShow the code\n\n\ncoffeechain &lt;- read_rds(\"data/CoffeeChain.rds\")\n\n\nNote: This step is optional if coffeechain is already available in R.\nThe code chunk below is used to aggregate Sales and Budgeted Sales at the Product level.\n\n\nShow the code\n\n\nproduct &lt;- coffeechain %&gt;%\n  group_by(`Product`) %&gt;%\n  summarise(`target` = sum(`Budget Sales`),\n            `current` = sum(`Sales`)) %&gt;%\n  ungroup()\n\n\n\n\n\nThe code chunk below is used to plot the bullet charts using ggplot2 functions.\n\n\nShow the code\n\n\nggplot(product, aes(Product, current)) + \n  geom_col(aes(Product, max(target) * 1.01),\n           fill=\"grey85\", width=0.85) +\n  geom_col(aes(Product, target * 0.75),\n           fill=\"grey60\", width=0.85) +\n  geom_col(aes(Product, target * 0.5),\n           fill=\"grey50\", width=0.85) +\n  geom_col(aes(Product, current), \n           width=0.35,\n           fill = \"black\") + \n  geom_errorbar(aes(y = target,\n                    x = Product, \n                    ymin = target,\n                    ymax= target), \n                width = .4,\n                colour = \"red\",\n                size = 1) +\n  coord_flip()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#plotting-sparklines-using-ggplot2",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#plotting-sparklines-using-ggplot2",
    "title": "Hands-on Exercise 10",
    "section": "",
    "text": "In this section, you will learn how to plot sparklines by using ggplot2.\n\n\n\n\nShow the code\n\n\nsales_report &lt;- coffeechain %&gt;%\n  filter(Date &gt;= \"2013-01-01\") %&gt;%\n  mutate(Month = month(Date)) %&gt;%\n  group_by(Month, Product) %&gt;%\n  summarise(Sales = sum(Sales)) %&gt;%\n  ungroup() %&gt;%\n  select(Month, Product, Sales)\n\n\nThe code chunk below is used to compute the minimum, maximum and end othe the month sales.\n\n\nShow the code\n\n\nmins &lt;- group_by(sales_report, Product) %&gt;% \n  slice(which.min(Sales))\nmaxs &lt;- group_by(sales_report, Product) %&gt;% \n  slice(which.max(Sales))\nends &lt;- group_by(sales_report, Product) %&gt;% \n  filter(Month == max(Month))\n\n\nThe code chunk below is used to compute the 25 and 75 quantiles.\n\n\nShow the code\n\n\nquarts &lt;- sales_report %&gt;%\n  group_by(Product) %&gt;%\n  summarise(quart1 = quantile(Sales, \n                              0.25),\n            quart2 = quantile(Sales, \n                              0.75)) %&gt;%\n  right_join(sales_report)\n\n\n\n\n\nThe code chunk used.\n\n\nShow the Code\n\n\nggplot(sales_report, aes(x=Month, y=Sales)) + \n  facet_grid(Product ~ ., scales = \"free_y\") + \n  geom_ribbon(data = quarts, aes(ymin = quart1, max = quart2), \n              fill = 'grey90') +\n  geom_line(size=0.3) +\n  geom_point(data = mins, col = 'red') +\n  geom_point(data = maxs, col = 'blue') +\n  geom_text(data = mins, aes(label = Sales), vjust = -1) +\n  geom_text(data = maxs, aes(label = Sales), vjust = 2.5) +\n  geom_text(data = ends, aes(label = Sales), hjust = 0, nudge_x = 0.5) +\n  geom_text(data = ends, aes(label = Product), hjust = 0, nudge_x = 1.0) +\n  expand_limits(x = max(sales_report$Month) + \n                  (0.25 * (max(sales_report$Month) - min(sales_report$Month)))) +\n  scale_x_continuous(breaks = seq(1, 12, 1)) +\n  scale_y_continuous(expand = c(0.1, 0)) +\n  theme_tufte(base_size = 3, base_family = \"Helvetica\") +\n  theme(axis.title=element_blank(), axis.text.y = element_blank(), \n        axis.ticks = element_blank(), strip.text = element_blank())"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#static-information-dashboard-design-gt-and-gtextras-methods",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#static-information-dashboard-design-gt-and-gtextras-methods",
    "title": "Hands-on Exercise 10",
    "section": "",
    "text": "In this section, you will learn how to create static information dashboard by using gt and gtExtras packages. Before getting started, it is highly recommended for you to visit the webpage of these two packages and review all the materials provided on the webpages at least once. You done not have to understand and remember everything provided but at least have an overview of the purposes and functions provided by them.\n\n\nIn this section, you will learn how to prepare a bullet chart report by using functions of gt and gtExtras packages.\n\n\nShow the Code\n\n\nlibrary(gt)\nlibrary(gtExtras)\n\nproduct %&gt;%\n  gt::gt() %&gt;%\n  gt_plt_bullet(column = current, \n              target = target, \n              width = 60,\n              palette = c(\"lightblue\", \n                          \"black\")) %&gt;%\n  gt_theme_538()\n\n\n\n\n\n\n\n\n\n\n\nProduct\ncurrent\n\n\n\n\nAmaretto\n\n\n\n   \n\n\n\nCaffe Latte\n\n\n\n   \n\n\n\nCaffe Mocha\n\n\n\n   \n\n\n\nChamomile\n\n\n\n   \n\n\n\nColombian\n\n\n\n   \n\n\n\nDarjeeling\n\n\n\n   \n\n\n\nDecaf Espresso\n\n\n\n   \n\n\n\nDecaf Irish Cream\n\n\n\n   \n\n\n\nEarl Grey\n\n\n\n   \n\n\n\nGreen Tea\n\n\n\n   \n\n\n\nLemon\n\n\n\n   \n\n\n\nMint\n\n\n\n   \n\n\n\nRegular Espresso"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#sparklines-gtextras-method",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#sparklines-gtextras-method",
    "title": "Hands-on Exercise 10",
    "section": "",
    "text": "Before we can prepare the sales report by product by using gtExtras functions, code chunk below will be used to prepare the data.\n\n\nShow the Code\n\n\nreport &lt;- coffeechain %&gt;%\n  mutate(Year = year(Date)) %&gt;%\n  filter(Year == \"2013\") %&gt;%\n  mutate (Month = month(Date, \n                        label = TRUE, \n                        abbr = TRUE)) %&gt;%\n  group_by(Product, Month) %&gt;%\n  summarise(Sales = sum(Sales)) %&gt;%\n  ungroup()\n\n\nIt is important to note that one of the requirement of gtExtras functions is that almost exclusively they require you to pass data.frame with list columns. In view of this, code chunk below will be used to convert the report data.frame into list columns.\n\n\nshow the Code\n\n\nreport %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\")\n\n# A tibble: 13 × 2\n   Product           `Monthly Sales`\n   &lt;chr&gt;             &lt;list&gt;         \n 1 Amaretto          &lt;dbl [12]&gt;     \n 2 Caffe Latte       &lt;dbl [12]&gt;     \n 3 Caffe Mocha       &lt;dbl [12]&gt;     \n 4 Chamomile         &lt;dbl [12]&gt;     \n 5 Colombian         &lt;dbl [12]&gt;     \n 6 Darjeeling        &lt;dbl [12]&gt;     \n 7 Decaf Espresso    &lt;dbl [12]&gt;     \n 8 Decaf Irish Cream &lt;dbl [12]&gt;     \n 9 Earl Grey         &lt;dbl [12]&gt;     \n10 Green Tea         &lt;dbl [12]&gt;     \n11 Lemon             &lt;dbl [12]&gt;     \n12 Mint              &lt;dbl [12]&gt;     \n13 Regular Espresso  &lt;dbl [12]&gt;     \n\n\n\n\n\n\n\nShow the Code\n\n\nreport %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\") %&gt;%\n   gt() %&gt;%\n   gt_plt_sparkline('Monthly Sales',\n                    same_limit = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nProduct\nMonthly Sales\n\n\n\n\nAmaretto\n\n\n\n   1.2K\n\n\n\nCaffe Latte\n\n\n\n   1.5K\n\n\n\nCaffe Mocha\n\n\n\n   3.7K\n\n\n\nChamomile\n\n\n\n   3.3K\n\n\n\nColombian\n\n\n\n   5.5K\n\n\n\nDarjeeling\n\n\n\n   3.0K\n\n\n\nDecaf Espresso\n\n\n\n   3.2K\n\n\n\nDecaf Irish Cream\n\n\n\n   2.7K\n\n\n\nEarl Grey\n\n\n\n   3.0K\n\n\n\nGreen Tea\n\n\n\n   1.5K\n\n\n\nLemon\n\n\n\n   4.4K\n\n\n\nMint\n\n\n\n   1.5K\n\n\n\nRegular Espresso\n\n\n\n   1.1K\n\n\n\n\n\n\n\n\n\n\n\n\nFirst, calculate summary statistics by using the code chunk below.\n\n\nShow the Code\n\n\nreport %&gt;% \n  group_by(Product) %&gt;% \n  summarise(\"Min\" = min(Sales, na.rm = T),\n            \"Max\" = max(Sales, na.rm = T),\n            \"Average\" = mean(Sales, na.rm = T)\n            ) %&gt;%\n  gt() %&gt;%\n  fmt_number(columns = 4,\n    decimals = 2)\n\n\n\n\n\n\n\nProduct\nMin\nMax\nAverage\n\n\n\n\nAmaretto\n1016\n1210\n1,119.00\n\n\nCaffe Latte\n1398\n1653\n1,528.33\n\n\nCaffe Mocha\n3322\n3828\n3,613.92\n\n\nChamomile\n2967\n3395\n3,217.42\n\n\nColombian\n5132\n5961\n5,457.25\n\n\nDarjeeling\n2926\n3281\n3,112.67\n\n\nDecaf Espresso\n3181\n3493\n3,326.83\n\n\nDecaf Irish Cream\n2463\n2901\n2,648.25\n\n\nEarl Grey\n2730\n3005\n2,841.83\n\n\nGreen Tea\n1339\n1476\n1,398.75\n\n\nLemon\n3851\n4418\n4,080.83\n\n\nMint\n1388\n1669\n1,519.17\n\n\nRegular Espresso\n890\n1218\n1,023.42\n\n\n\n\n\n\n\n\n\n\n\nNext, use the code chunk below to add the statistics on the table.\n\n\nShow the Code\n\n\nspark &lt;- report %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\")\n\n\n\n\nShow the Code\n\n\nsales &lt;- report %&gt;% \n  group_by(Product) %&gt;% \n  summarise(\"Min\" = min(Sales, na.rm = T),\n            \"Max\" = max(Sales, na.rm = T),\n            \"Average\" = mean(Sales, na.rm = T)\n            )\n\n\n\n\nShow the Code\n\n\nsales_data = left_join(sales, spark)\n\n\n\n\n\n\n\nShow the Code\n\n\nsales_data %&gt;%\n  gt() %&gt;%\n  gt_plt_sparkline('Monthly Sales',\n                   same_limit = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProduct\nMin\nMax\nAverage\nMonthly Sales\n\n\n\n\nAmaretto\n1016\n1210\n1119.000\n\n\n\n   1.2K\n\n\n\nCaffe Latte\n1398\n1653\n1528.333\n\n\n\n   1.5K\n\n\n\nCaffe Mocha\n3322\n3828\n3613.917\n\n\n\n   3.7K\n\n\n\nChamomile\n2967\n3395\n3217.417\n\n\n\n   3.3K\n\n\n\nColombian\n5132\n5961\n5457.250\n\n\n\n   5.5K\n\n\n\nDarjeeling\n2926\n3281\n3112.667\n\n\n\n   3.0K\n\n\n\nDecaf Espresso\n3181\n3493\n3326.833\n\n\n\n   3.2K\n\n\n\nDecaf Irish Cream\n2463\n2901\n2648.250\n\n\n\n   2.7K\n\n\n\nEarl Grey\n2730\n3005\n2841.833\n\n\n\n   3.0K\n\n\n\nGreen Tea\n1339\n1476\n1398.750\n\n\n\n   1.5K\n\n\n\nLemon\n3851\n4418\n4080.833\n\n\n\n   4.4K\n\n\n\nMint\n1388\n1669\n1519.167\n\n\n\n   1.5K\n\n\n\nRegular Espresso\n890\n1218\n1023.417\n\n\n\n   1.1K\n\n\n\n\n\n\n\n\n\n\n\n\nSimilarly, we can combining the bullet chart and sparklines using the steps below.\n\n\nShow the Code\n\n\nbullet &lt;- coffeechain %&gt;%\n  filter(Date &gt;= \"2013-01-01\") %&gt;%\n  group_by(Product) %&gt;%\n  summarise(Target = sum(`Budget Sales`),\n            Actual = sum(Sales)) %&gt;%\n  ungroup()\n\n\n\n\nShow the Code\n\n\nsales_data = sales_data %&gt;%\n  left_join(bullet)\n\n\n\n\nShow the Code\n\n\nsales_data %&gt;%\n  gt() %&gt;%\n  gt_plt_sparkline('Monthly Sales') %&gt;%\n  gt_plt_bullet(column = Actual, \n                target = Target, \n                width = 28,\n                palette = c(\"lightblue\", \n                          \"black\")) %&gt;%\n  gt_theme_538()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProduct\nMin\nMax\nAverage\nMonthly Sales\nActual\n\n\n\n\nAmaretto\n1016\n1210\n1119.000\n\n\n\n   1.2K\n\n\n\n\n   \n\n\n\nCaffe Latte\n1398\n1653\n1528.333\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nCaffe Mocha\n3322\n3828\n3613.917\n\n\n\n   3.7K\n\n\n\n\n   \n\n\n\nChamomile\n2967\n3395\n3217.417\n\n\n\n   3.3K\n\n\n\n\n   \n\n\n\nColombian\n5132\n5961\n5457.250\n\n\n\n   5.5K\n\n\n\n\n   \n\n\n\nDarjeeling\n2926\n3281\n3112.667\n\n\n\n   3.0K\n\n\n\n\n   \n\n\n\nDecaf Espresso\n3181\n3493\n3326.833\n\n\n\n   3.2K\n\n\n\n\n   \n\n\n\nDecaf Irish Cream\n2463\n2901\n2648.250\n\n\n\n   2.7K\n\n\n\n\n   \n\n\n\nEarl Grey\n2730\n3005\n2841.833\n\n\n\n   3.0K\n\n\n\n\n   \n\n\n\nGreen Tea\n1339\n1476\n1398.750\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nLemon\n3851\n4418\n4080.833\n\n\n\n   4.4K\n\n\n\n\n   \n\n\n\nMint\n1388\n1669\n1519.167\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nRegular Espresso\n890\n1218\n1023.417\n\n\n\n   1.1K"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#interactive-information-dashboard-design-reactable-and-reactablefmtr-methods",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#interactive-information-dashboard-design-reactable-and-reactablefmtr-methods",
    "title": "Hands-on Exercise 10",
    "section": "",
    "text": "In this section, you will learn how to create interactive information dashboard by using reactable and reactablefmtr packages. Before getting started, it is highly recommended for you to visit the webpage of these two packages and review all the materials provided on the webpages at least once. You done not have to understand and remember everything provided but at least have an overview of the purposes and functions provided by them.\nIn order to build an interactive sparklines, we need to install dataui R package by using the code chunk below.\n\n\nShow the Code\n\n\nremotes::install_github(\"timelyportfolio/dataui\")\n\n\nNext, you all need to load the package onto R environment by using the code chunk below.\n\n\nShow the Code\n\n\nlibrary(dataui)\n\n\n\n\nSimilar to gtExtras, to plot an interactive sparklines by using reactablefmtr package we need to prepare the list field by using the code chunk below.\n\n\nShow the Code\n\n\nreport &lt;- report %&gt;%\n  group_by(Product) %&gt;%\n  summarize(`Monthly Sales` = list(Sales))\n\n\nNext, react_sparkline will be to plot the sparklines as shown below.\n\n\nShow the Code\n\n\nreactable(\n  report,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(report)\n    )\n  )\n)\n\n\n\n\n\n\n\n\n\nBy default the pagesize is 10. In the code chunk below, arguments defaultPageSize is used to change the default setting.\n\n\nShow the Code\n\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(report)\n    )\n  )\n)\n\n\n\n\n\n\n\n\n\nIn the code chunk below highlight_points argument is used to show the minimum and maximum values points and label argument is used to label first and last values.\n\n\nShow the Code\n\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        labels = c(\"first\", \"last\")\n        )\n    )\n  )\n)\n\n\n\n\n\n\n\n\n\nIn the code chunk below statline argument is used to show the mean line.\n\n\nShow the Code\n\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        statline = \"mean\"\n        )\n    )\n  )\n)\n\n\n\n\n\n\n\n\n\nInstead adding reference line, bandline can be added by using the bandline argument.\n\n\nShow the Code\n\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        line_width = 1,\n        bandline = \"innerquartiles\",\n        bandline_color = \"green\"\n        )\n    )\n  )\n)\n\n\n\n\n\n\n\n\n\nInstead of displaying the values as sparklines, we can display them as sparkbars as shiwn below.\n\n\nShow the Code\n\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkbar(\n        report,\n        highlight_bars = highlight_bars(\n          min = \"red\", max = \"blue\"),\n        bandline = \"innerquartiles\",\n        statline = \"mean\")\n    )\n  )\n)"
  }
]